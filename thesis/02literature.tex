\chapter{Background}\label{chap:back}

In this chapter, we will review prior literature relating to authorship attribution. We start with a review of work that relates to the more general and related fields of text classification and language modelling, and follow this by looking at work which is more directly related to ours.

Note that many of the articles that we make reference to are arXiv\footnote{https://arxiv.org/} preprints and not all of them have been published in peer-reviewed journals. Due to the fast-moving nature of the field, it is necessary to consider the newest ideas, datasets, and methods, even before they have proven themselves though peer review. Most of the preprint papers we cite in this work are authored by well-known established researchers. Many of the papers have already been accepted at established conferences and will appear shortly in peer reviewed journals.

\citet{stamatatos2009survey} has already created a comprehensive survey, summarizing and comparing various techniques that have been used for authorship attribution, including feature engineering and classification methods. In order to avoid repeating \citeauthor{stamatatos2009survey}'s work, we therefore focus here on newer research (published after his survey), and research that is closely connected to our own (that is, work which uses similar features, classification techniques, and datasets).

Authorship Attribution (AA) is a broad and completely well-defined field. We focus on two subtasks, namely Authorship Identification (AID) and Authorship Verification (AV). The first is the most well-known task, and it involves identifying the author of a disputed work from a set of candidate authors. For Authorship Verification, we attempt to predict whether or not a specific person was the author of a disputed text. While the first task is more common, it has been argued (for example by \citet{koppel2004authorship}) that the second has more real-world applications. We discuss each of these in this section, and also mention the related AA tasks of Author Profiling and Native Language Identification.


\section{Text Classification}
Authorship Attribution has seen research from different fields, including forensic studies (who are largely concerned with practical applications), linguistics (who are largely concerned with stylometry and language use, and often look at historically disputed texts manually or with computer assistance) and computer scientists (who are largely concerned with seeing how well machines can understand something as complicated as writing style). Our work is closest to the last category, and we see AA in essence as a text classification task, in which we attempt to build a system or systems that can take texts as input and produce the names or identities of specific authors as output.

We further aim to create systems that are \textit{automatic} and \textit{portable}. Therefore, we largely avoid approaches that would require manual annotation or be overly specific to a single dataset (for example, a system that only works on English texts, or only on poetry). 

In this section, we provide an overview of some common text classification methods which form part of the AA systems that we describe later. Specifically, several of our systems rely on Support Vector Machines (SVMs), which have been used successfully for many text classification tasks. We also experiment with Neural Networks, which have outperformed Support Vector Machines in many natural language processing tasks in the last few years, but which are not yet been extensively applied to AA tasks. We therefore begin with a brief overview of these classifiers, and then look more specifically at how they relate to AA tasks.

\subsection{Support Vector Machines}

A Support Vector Machine is a linear classifier. It tries to find a separating hyperplane that maximises the distance between two classes. Support Vector Machines have been widely used over the last two decades since \citet{joachims1998text} showed that they provide a robust and well-performing method for many text classification tasks. They have proven to work well in tasks ranging from Named Entity Recognition \cite{Kravalova:2009:CNE:1699705.1699748}, Authorship Profiling, such as personality and gender prediction \cite{verhoeven2016twisty,busgeropvollenbroeck:2016}, and authorship attribution tasks \cite{hurlimann2015glad,diederich2003authorship,koppel2003exploiting}

\citet{koppel2009computational} found that SVM approaches were state of the art for Authorship Attribution tasks. They state:
\begin{quote}
Comparative studies on machine learning methods for topic-based text categorisation problems (Dumais et al. 1998; Yang 1999) have shown that in general, support vector machine (SVM) learning is at least as good for text categorisation as any other learning method and the same has been found for authorship attribution (Abbasi \& Chen 2005; Zheng et al. 2006).
\end{quote}

\citet{houvardas2006ngram} used Support Vector Machines extensively to experiment with Authorship Attribution, using different n-gram ranges and feature selection methods. He found that SVMs were particularly well suited to Authorship Attribution tasks and achieved good results for an Authorship Identification task on the C50 dataset, which we describe in more detail in Chapter \ref{chap:data}. 

\subsection{Neural Networks}
Recently, Neural Networks have received a lot of attention and have outperformed previous state-of-the-art approaches in many natural language processing tasks. \citet{lai2015recurrent} showed that neural network classifiers could outperform other methods (including SVMs) in various text classification tasks. More generally, Neural Networks, and specifically Recurrent Neural Networks -- a Neural Network which can model sequences -- have been used to achieve promising results in many areas of Natural Language Processing. \citet{goldberg2016primer} states:

\begin{quote}
Recurrent models have been shown to produce very strong results for language modeling, as well as for sequence tagging, machine translation, dependency parsing, sentiment analysis, noisy text normalization, dialog state tracking, response generation, and modeling the relation between character sequences and part-of-speech tags.    
\end{quote}

However, neural networks often rely on larger datasets in order to outperform other classifiers, such as SVMs, and this is arguably one of the reasons why they are not yet widely used in AA tasks (we discuss some notable exceptions in Section \ref{background:nn-aid}). We see two promising ways that Neural Networks can be used to tackle AA tasks in spite of the often limited data. For Authorship Identification, neural language models can be used to distinguish between different authors, as in the work of \citet{bagnall2015author,bagnall2016authorship}. We discuss this further in Section \ref{background:lm} and Section \ref{background:nn-aid}. For Authorship Verification, so-called Siamese Neural Networks conceptually fit the task very well, but there is very little empirical research that they work well for this in practice. We introduce Siamese Neural Networks in Section \ref{background:siamese}. 


\subsection{Siamese Neural Networks}
\label{background:siamese}
Siamese neural networks are a customized neural network architecture in which two sub-networks share weights which are simultaneously updated during training. A joining neuron learns a distance function between the two networks, and outputs a single similarity measure. Thus the network as a whole learns a custom distance function between pairs of inputs. Siamese neural networks are designed for verification tasks, in which we want to classify the relation between pairs of inputs, and they therefore conceptually fit the Authorship Verification task, in which we examine pairs of texts, very well. They have been used mainly for image verification tasks, as discussed below, but they are now gaining in popularity for text classification tasks such as question answering in which the question and candidate answer are taken as the input pair \citet{yin2016abcnn}.

Because Siamese Neural Networks were first used for image verification tasks, and most of the recent literature relating to Siamese Networks is still in that domain, we first present a broader introduction to Siamese Networks within image processing. This is followed by a review of the much smaller body of recent work which experiments with using similar ideas for text classification tasks.

\subsubsection{Image Similarity Tasks}
The concept of a Siamese Neural Network was first introduced by \citet{bromley1993signature}, who used it for Signature Verification. With signature verification, the task is to predict whether two signatures are signed by the same hand, or if one of the them is a forgery.  The authors compare the two identical sub-networks, or ``legs'', of the siamese network to feature extraction, as these learn which features are important, while the joining neuron measures the similarity between the two legs. If the measured distance between a known signature and an unknown one is greater than some threshold, then the unknown signature is rejected as being a forgery. More recently, similar architectures are still being actively used and researched for signature verification \cite{tiflin2012lstm}.

Since then, Siamese Networks have been used for similar verification tasks, such as face verification. This is similar to signature verification, but it is used, for example, for access control. A new picture of a person is taken and compared to one that is stored on record (for example, in an ID card or passport). If the system predicts that the new picture is the same as the stored one, the person is `verified' and allowed access. \citet{chopra2005learning} describe how a siamese neural network can be used to solve this task as follows:

\begin{quote}
We present a method for training a similarity metric from data. The method can be used for recognition or verification applications where the number of categories is very large and not known during training, and where the number of training samples for a single category is very small. The idea is to learn a function that maps input patterns into a target space such that the $L_2$ norm in the target space approximates the “semantic” distance in the input space. The method is applied to a face verification task. The learning process minimizes a discriminative loss function that drives the similarity metric to be small for pairs of faces from the same person, and large for pairs from different persons.
\end{quote}

Similarly \citet{zhu2017deep} achieved good results using siamese neural networks for face verification (called person reidentification in their work). A more complicated extension of image verification is automatic scene detection for films. Siamese networks have shown promising results for this task as well by doing multiple frame-wise verification sub-tasks, deciding which frames belong to the same scene, and which to different scenes in order to detect when the scene changes \cite{baraldi2015deep}.

The face verification task, in which real-world settings often mean that ta large or unknown number of classes needs to be taken into account and the training data per class is highly limited, is highly similar to the AV task that we already described. Although images are used for the face verification task, while we need to look at text for the AV task, because neural networks have shown to be proficient at classifying both images and text, and because the siamese architecture has been shown to be successful for the face verification task, it is likely that a similar architecture would work well for the AV task.

Verification tasks are associated with what is sometimes referred to as `one-shot' learning. One shot learning describes teaching a classifier to recognise examples of a specific class by training it on a single example from that class, as opposed to normal classification tasks in which the classifier is given thousands or millions of training examples for each class. \citet{koch2015siamese} shows how Siamese Neural Networks can be used to solve one-shot learning tasks in image classification by comparing each test example to each training example, and seeing which one is predicted most strongly as being \textit{same-class}. Bouma\footnote{https://sorenbouma.github.io/blog/oneshot/} presents a higher-level introduction to the same idea.

Siamese Neural Networks have also being used for pronunciation similarity in speech recognition tasks \cite{naaman2017learning}, for Optical Character Recognition \cite{hosseini2015similarity}, and for several types of text classification task, which we discuss next.

\subsubsection{Text Similarity Tasks}
\label{siamese-networks-for-nlp}
More recently, Siamese Neural Networks are being used for text classification tasks. \citet{yin2016abcnn} use a model based on Siamese Neural Networks for three text classification tasks. These are answer selection, in which the goal is to select the correct answer for a given question; paraphrase identification, in which pairs of sentences must be identified as either being a paraphrase of each other or not; and textual entailment, in which they attempt to predict whether or not one sentence entails another. The find that their architecture performs adequately on all three tasks.

\citet{neculoiu2016learning} use a Siamese Network to match job titles that refer to the same (or similar) positions, but which are lexically different. For example, in the technology industry,  ``software architectural technician Java/J2EE'' may describe the same job as ``Java Engineer''. When matching job applications to job applicants, it is useful to be able to automatically map all titles to the same concept, and the authors show that this can be done with a Siamese Neural Network architecture.

Similarly, \citet{mueller2016siamese} use Siamese Networks to achieve high results in a sentence similarity task, in which the goal is to predict how semantically similar two sentences are.

Notably, almost all of the text similarity tasks described above involve very short texts, usually sentences or phrases. While the Authorship Verification task, in which we have to decide whether to texts are by the same author, conceptually fits the siamese model very well, there is almost no empirical evidence that this architecture performs well for learning a similarity function over longer text pairs.

\section{Text Generation}
\label{background:lm}
Automatically generating text in a specific style is a more difficult task than discriminating between different styles of human-generated text. If we can generate text in the style of a specific author then we can also discriminate between authorship styles. One of the main methods we experiment with, described in Section \ref{background:nn-aid}, uses generative language models for discriminating between authorship styles. Therefore, we introduce the concept of text generation and review prior work in this area, focusing on work that uses generative character-based neural language modelling.

\subsection{Neural Language Modelling}\label{neural-networks}
A language model is a function that assigns a probability to a sequence of words or characters. A common word sequence, such as ``Yesterday, I bought an apple'', would be assigned a much higher probability than an uncommon sequence such as ``My mattocks are inaureoled''\footnote{Which loosely could mean ``my double-ended battle hoe is surrounded by a halo''.}.

Most language models have historically been so-called word-level language models \cite{mikolov2012subword}. That is, these models take individual words as the smallest unit, and calculate the probability of a specific word given the sequence of words that come before it. 

More recently, character-level language models have become more popular, and are useful for their ability to model unseen or ``OOV'' (out-of-vocabulary) words. It is impossible for a language model to account for every word in existence, and word-based language models cannot deal with words that they have not been trained on. Character-level language models work in the same way as word-based models, but take an atomic unit to be the character, and calculate the probability of a specific character given the sequence of characters that come before it. These models deal much better with unseen or OOV words, as even unseen words usually follow specific character patterns (for example, a character-level model might be able to predict a `-tion' suffix for a word, even if it hasn't seen the complete word during training). 

\citet{sutskever2011generating} showed that Recurrent Neural Networks (RNNs), a neural network model designed for sequence modelling, could be used for character-level language modelling and that these language models could generate surprisingly coherent sounding text. This concept was popularised in a blog post titled \textit{The Unreasonable Effectiveness of Recurrent Neural Networks}\footnote{http://karpathy.github.io/2015/05/21/rnn-effectiveness/}, where Andrej Karpathy showed how well character-level recurrent neural networks were able to generate text in a specific style. For example, his network was able to automatically generate text that closely resembles that written by Shakespeare, an excerpt of which is given below:

\begin{quote}
KING LEAR:
O, if you were a feeble sight, the courtesy of your law,
Your sight and several breath, will wear the gods
With his heads, and my hands are wonder'd at the deeds,
So drop upon your lordship's head, and your opinion
Shall be against your honour.
\end{quote}

\citet{mikolov2012subword} discuss the trade-offs between word- and character-based language modelling and show how Neural Networks can be used for a variety of language modelling tasks. They introduce so-called \textit{subword} models, which share the advantages of word- and character-based models.

More recently \citet{gatt2017survey} present a comprehensive modern survey on Natural Language Generation. Although their work is generally broader than ours, it is interesting to us because they mention the idea of generating text in the style of a specific author, stating 

\begin{quote}
A second question is whether stylistic variation could be modelled in a more specific fashion, for example, by tailoring style to a specific author, rather than to generic dimensions related to ‘formality’,  ‘involvement’ and so on.  
\end{quote}

Any work that could achieve this fine-grained style control could certainly be useful in discriminating between authorship styles. Unfortunately, as noted by \citeauthor{gatt2017survey}, this is a very new area of research and no one has yet successfully achieved this.

There are broader notions of style, which we discuss in more details in Section \ref{background:style}, and there is some work in generating text in these broader styles. \citet{ficler2017controlling} experiment with generating movie reviews in specific styles. For example, by setting a ``professional'' parameter in their model, they get a sample generation of ``This film has a certain sense of imagination and a sobering look at the clandestine indictment.'', which is in the style of a professional critic. By turning off this setting, they get examples such as ``I know it’s a little bit too long, but it’s a great movie to watch !!!'', which is more in the style of an internet review. They use a conditioned Long Short-Term Memory Recurrent Neural Network (LSTM-RNN), conditioning the generated texts based on several parameters, such as sentiment, formality, and register.

\section{Authorship Attribution}\label{authorship-attribution}

In this section we present a review of prior work that is closely related to our research on Authorship Attribution (AA). The two main sub-tasks of AA, as discussed before, are Authorship Identification (AID) and Authorship Verification (AV). We discuss each of these in turn. By some definitions, related tasks such as age profiling, gender profiling, personality profiling, and native language identification, also fall under the AA umbrella. We discuss these tasks briefly later, but for our work we will regard AA as consisting of AID and AV.

\subsection{Authorship Identification}
\label{authorship-identification}
Authorship Identification is the most well-known AA task. For the AID task, the goal is to predict which of a closed set of candidate authors is the author of an \textit{unknown} text, where the unknown text is of disputed authorship. \citet{stamatatos2009survey} describes this task as follows:

\begin{quote}
In the typical authorship attribution problem, a text of unknown authorship is assigned to one candidate author, given a set of candidate authors for whom text samples of undisputed authorship are available. From a machine learning point-of-view, this can be viewed as a multi-class single-label text categorization task.
\end{quote}

Many approaches to authorship attribution have been attempted over the last three decades, including supervised and unsupervised approaches. For supervised approaches, the features used have varied greatly. \citet{stamatatos2009survey} lists 20 commonly used features, which he breaks down into the categories of \textit{lexical} (for example, word n-grams), \textit{character} (for example, character n-grams), \textit{syntactic} (for example, parts of speech tags), \textit{semantic} (for example, synonyms), and \textit{application specific} (for example, language specific dictionary). 

Intuitively, when attempting to identify the author of a text, it seems that very unusual words and phrases used by that author would be helpful.  In practice, however, the unique way that an author uses very common words and phrases is far more useful. Looking only at very common function words (for example, `the', `and', `he', etc), can in many cases be enough to reliably distinguish one author from another. \citet{kestemont2014function} talks about why function words are important for authorship attribution. He gives four main points regarding function words, namely that all people who write in the same language will use the same function words; that function words appear with high frequency in almost all texts; that function words are not effected by the \textit{topic} of the text; and that function words seem``seems less under an author’s conscious control''.

\citet{koppel2009computational} also support the idea that function words are good for Authorship Attribution. They state:

\begin{quote}
The reason for using FWs [function words] in preference to others is that we do not expect their frequencies to vary greatly with the topic of the text, and hence, we may hope to recognize texts by the same author on different topics. It also is unlikely that the frequency of FW use can be consciously controlled, so one may hope that use of FWs for attribution will minimize the risk of being deceived    
\end{quote}

However, \citet{kestemont2014function} also argues that function words are more useful in English settings than for other languages, because English, as a language that does not make heavy use of inflections, relies more on function words than other languages do. He believes that character n-grams can often provide an adequate representation of function words in a text, while also being ``sensitive to the internal morphemic structure of words''. That is, it is likely that models which rely on character n-grams as features will be more language independent than word-based models, as character n-grams can capture distinctive function word usage, while also capturing distinctive inflection usage.

Feature engineering is not always desirable as it requires a topic specialist to construct task-specific features. It can also make classification tasks less efficient (for example, if a parts-of-speech tagger is needed to extract a specific feature then classification performance will be drastically reduced), and less generalizable (a model that relies on parts-of-speech tag is less likely to perform well cross-lingually, especially in the case of languages for which good taggers are not available). Furthermore, recent research has shown that manual feature engineering can result in worse accuracy than in cases where the classifier is relied upon to infer features from rawer input. For example, we were part of a team that achieved the top ranking in an Authorship Profiling shared task, using only word and character n-grams as features. Our research showed that adding additional features only hurt perfomance \cite{basile2017ngram}. Similarly, \citet{braud2017writing} showed that a simple unigram model outperformed a model with many complex features in a task that used writing to style to identify scientific fraud.

Recently, authorship attribution models which rely on almost no manual feature engineering have proven to perform well. For example, \citet{bagnall2015author} achieved the top score at an Authorship Attribution shared task, using only the characters of each text as features. We discuss his approach in more detail in Section \ref{background:nn-aid}.

With the above in mind, we focus on approaches that do not require heavy feature engineering, and which are therefore more applicable across different domains and languages.


Much of the prior work related to AID uses settings that are not common in real-world authorship attribution tasks. \citet{luyckx2011scalability} talks about the scalability issues relating to authorship identification and criticises prior research on two main points. First, earlier work often uses a very small number of candidate authors for AI tasks (sometimes only two). Second, this work often uses very long texts (often each text is a full-length book). By contrast, in practical AA tasks there is often only a short fragment of text available, and a large number of candidate authors. \citeauthor{luyckx2011scalability} states:

\begin{quote}authorship attribution `in the wild' may entail thousands of candidate authors with often small sets of data or only very short texts, in substantially more topics, genres, and registers\end{quote}

It is therefore desirable to experiment with AID tasks that are closer to the settings found ``in the wild''. That is, we want our methods to perform well in cases where there are many candidate authors and limited text available for each. More modern papers often acknowledge and address the issues raised by \citeauthor{luyckx2011scalability}, but it is not unusual to still see studies that ignore these issues. For example, \citet{akimushkin2017role} provide an extensive analysis on using text networks for AA, but present results on a dataset consisting of only eight authors, and using ten full-length books for each author.

\citet{abbasi2008writeprints} present research that has closer ties to practical AA problems. They focus on attempting to identify authors in ``cyberspace'', and use datasets consisting of emails and online forums. They use the Enron dataset \cite{klimt2004enron}, which is built from a large collection of real-world emails. Our work contrasts with theirs in that they use many of the features described above, while in most of our models, we attempt to solve the task using as few features as possible.


\subsection{Authorship Verification}
\label{authorship-verification}
Authorship Verification is an AA task in which the goal is to predict whether two texts are written by the same author. It is sometimes formulated as deciding whether or not a \textit{specific} author wrote a disputed text. Therefore, it can be thought of in two ways. First, it can be modelled as a one-class classification problem, in which we attempt to distinguish a single class (or a single author) from all other possible texts.

\citet{koppel2004authorship} takes this approach and in explaining why AV is a more interesting and realistic task than AID, they state:

\begin{quote}
If, for example, all we wished to do is to determine if a text was written by Shakespeare or Marlowe, it would be sufficient to use their respective known writings, to construct a model distinguishing them, and to test the unknown text against the model. If, on the other hand, we need to determine if a text was written by Shakespeare or not, it is very difficult – if not impossible – to assemble an exhaustive, or even representative, sample of not-Shakespeare.     
\end{quote}

To keep the advantages of AV over AID, but to also solve the problem of representing ``not-Shakespeare'', \citet{koppel2009computational} later argued that it might be better to think of AV as a two-class problem. He states:
\begin{quote}
Verification can be thought of as a one-class classification problem (Manevitz \& Yousef, 2001; Scholkopf, Platt, Shawe- Taylor, Smola, \& Williamson, 2001; Tax, 2001). But perhaps a better way to think about authorship verification is that we are given two example sets and are asked whether these sets were generated by the same process (i.e., author) or by two different processes.    
\end{quote}


Koppel has pushed the idea of AV being a more important and interesting task than AID more strongly over the years. He has stated that ``a solution to [authorship verification] can serve as a building block for solving almost any conceivable authorship attribution problem'' \cite{koppel2012fundamental} and has called it the ``fundamental problem of Authorship Attribution'' \cite{koppel2012fundamental,koppel2012authorship}. He has also said that ``Almost any conceivable authorship attribution problem can be reduced to one fundamental problem: whether a pair of (possibly short) documents were written by the same author'' \cite{koppel2014determining}

\citet{luyckx2008authorship} are also proponents of Authorship Verification. They state:

\begin{quote}
Most studies also use sizes of training data that are unrealistic for situations in which stylometry is applied (e.g., forensics), and thereby overestimate the accuracy of their approach in these situations. A more realistic interpretation of the task is as an authorship verification problem[...].
\end{quote}

However not everyone uses the same definitions or terminology for Authorship Verification tasks.  Another way to distinguish between AV and AID is by distinguishing between `closed-world' and `open-world' tasks. In AID, the world is `closed' because we can enumerate all the potential authors for a given text. In AV, the world is `open', because either the two texts are written by the same author, or they are not. In the latter case, we do not attempt to define who is the actual author of the unknown text. This distinction (and terminology) is discussed by \citet{stolerman2011classify} who aim to reconcile theoretical AA work with practical issues. They present research on ``Breaking the Closed-World Assumption in Stylometric Authorship Attribution'', and argue that much theoretical research is impractical as it assumes a closed-world setting.

They introduce the \textit{classify-verify} method for AA tasks, which adds a second step to a traditional classification approach in which the classifier is taught to ``abstain'' in certain cases, and argue that this is a good compromise between closed- and open-world tasks.

\citet{stolerman2015authorship} presents a comprehensive review of Authorship Verification methods. He distinguishes between two classes of authorship tasks, namely \textit{one-class} problems and \textit{two-class} problems. The former refers to tasks in which we attempt to distinguish a single class (for example, a single author) against all other classes (for example, all other possible authors). Two-class problems refer to tasks in which we attempt to assign one of two labels (for example, \textit{same-author} or \textit{different-author} to each instance). \citeauthor{stolerman2015authorship} notes that there is no need to discuss the more general n-class classification as any n-class problem can be reduced to multiple one-class or two-class tasks.

\citeauthor{stolerman2015authorship} discusses at length the need for authorship attribution research to focus more on practical tasks, and to move away from the traditional closed-task setups that have dominated previous work. He states

\begin{quote}
The standard closed-world authorship attribution domain, however, is abundant with datasets that can be trivially formulated to test verification. If more datasets are to be used and tested, it can assert the usability of current and future verification methodologies, with emphasis on which techniques are suited to what problems. Verification methods should be tested on datasets that challenge with a high number of potential authors, taking pure one-sided learning approaches, limited amounts of training data, texts with real-world characteristics and the like.   
\end{quote}

Central to the verification task is the idea of similarity. Because we have two texts, a known and an unknown, we want to be able to tell how stylistically similar these two texts are. This has led researchers to propose many different ways of representing similarity, both in terms of features used as well as custom distance measures. \citet{halvani2016authorship} propose a distance function which is a modified version of Manhattan Distance, and show a novel similarity-based authorship verification that generalises well over different genres and languages. \citeauthor{halvani2016authorship}'s work is mainly of interest to us because they show results for many different PAN datasets, which we also use. 

A less usual approach to modelling similarity is by using compression models. Data compression exploits patterns in text to efficiently reduce the storage space required to store a representation of that text. If a compression model `trained' on a \textit{known} text is also able to efficiently compress an \textit{unknown} text, then at least some patterns in the known text are also present in the unknown text. \citet{stamatatos2009survey} discusses some older work that uses compression models for AA, and more recently \citet{halvani2017authorship} has investigated these methods again. Although we do not use compression models in our current work, this research is related to ours because character-based neural language models, which we do use and describe in detail in Section \ref{method:nn-aid} and Section \ref{models:nn-aid}, rely on a similar idea: that a system trained to find low-level patterns in a known text can be used to evaluate an unknown text.

\subsection{Neural Networks for Authorship Attribution}
\label{background:nn-aid}
As we mentioned before, there has been very little work in using Neural Networks for Authorship Attribution. This is partly because in AA tasks there is often very limited training data available. Neural Networks have proven to be very powerful in many text classification tasks, but they often rely on huge amounts of training data to achieve good results. Here, we discuss some existing attempts that use neural approaches to Authorship Attribution.

\citet{bagnall2015author,bagnall2016authorship} has achieved the top rank at recent PAN Authorship Attribution shared tasks. He competed in and won two shared tasks over two years. The first task was an AV task, while the second was an ``authorship clustering'' task, which is a more complicated task but which can still be remodelled as AV. \citeauthor{bagnall2016authorship}'s approach is interesting for three reasons. First, he uses a neural network approach which outperformed competitors' SVM models. As far as we know, this is the only indication in prior literature that neural networks can improve upon existing AA methods. Second, \citeauthor{bagnall2015author} uses almost no manual feature engineering, relying only on character sequences. Third, instead of training a discriminative classifier to discriminate between authors, as is more common for text classification tasks, he trains separate generative models for each candidate author. He assumes that a model trained on a known text from a specific author will better fit unknown texts from that same author. He therefore classifies each unknown text by seeing how well each author-specific model models that text.

Although in both 2015 and 2016 the AA tasks presented by PAN were closer to AV tasks than AID ones, \citeauthor{bagnall2015author}'s approach better fits the AID task. He approximates the AV task as follows. After evaluating each unknown text against each author-specific model, if the model from the known text models the unknown text better than half of all author-specific models, then he predicts that the two texts are written by the same author.

There has been some indication that Convolutional Neural Networks (CNNs) can be used for Authorship Attribution. \citet{shrestha2017convolutional} use CNNs to identify the authors of short texts (Tweets) and \citet{ruder2016character} use a similar model for larger scale AA tasks, using datasets from Twitter, Reddit, Blogs, The Internet Movie Database (IMDb), and The Enron Email dataset.

\subsubsection{Generative and Discriminative models}
Unlike \citeauthor{bagnall2015author}'s approach discussed above, which relies on generative models, the approaches by both \citet{shrestha2017convolutional} and \citet{ruder2016character} use discriminative models. \citet{ng2002discriminative} compare discriminative and generative models for classification (specifically logistic regression and naive Bayes), and show that, depending on the size of the training data, generative models can outperform discriminative ones. Similarly and more recently \citet{yogatama2017generative} build on the word by \citeauthor{ng2002discriminative} and show empirically that the same is true for LSTM models in text classification tasks. They state:

\begin{quote}
    However we also find that generative models approach their asymptotic error rate more rapidly than their discriminative counterparts—the same pattern that Ng \& Jordan (2001) proved holds for linear classification models that make more naıve conditional independence assumptions.  Building on this finding, we hypothesize that RNN-based generative classification models will be more robust to shifts in the data distribution.  This hypothesis is confirmed in a series of experiments in zero-shot and continual learning settings that show that generative models substantially out-perform discriminative models.
\end{quote}

\citet{deng2015deep} also compare generative and discriminative models, mainly in the domain of speech processing, and similarly find that generative models can outperform discriminative models in certain cases, especially when training data is limited.


\subsection{Transfer Learning}\label{background:style}
\label{transfer-learning-and-style-transfer}
As mentioned above, a major obstacle against using neural networks for AA tasks is that of \textit{data scarcity}. Often we simply do not have enough training data available per author for a neural network model to learn a generic representation of each author. Previously, Transfer Learning has been used to surmount data-scarcity obstacles. 

Transfer learning is the practice of training a neural network on one task and then using the learned weights to initialise a new network for a different task. \citet{riemer2017representation} describe this by stating: ``The very popular strategy of fine-tuning a neural network involves first training a neural network on a source task and then using the model to simply initialize the weights of a target task network up to the highest allowable common representation layer''. This is often used for tasks where data-scarcity is an issue. A large network is trained on a more general task, such as image recognition, on a large dataset, and this knowledge is then ``transferred'' to a more specific network to be used in a task with less data available. Although this is most well-known for its use in image classification tasks, it is also used in text-based tasks. For example, \citet{zoph2016transfer} use Transfer Learning to improve Neural  Machine Translation for low-resource language pairs.

Although the terms \textit{transfer learning} and \textit{fine-tuning} are often used interchangeably, \citet{lalor2017improving} make a distinction between the two, reserving transfer learning for situations in which data from a different task is used in the second training phase, and using \textit{fine-tuning} for situations in which more data for the same task is used.

Of special interest to Authorship Attribution is the idea of fine-tuning a language model to be personalised to a specific author. \citet{yoon2017efficient} show that it's possible to ``personalize'' language models in this way. They train generative LSTM language models and then fine-tune them to represent a specific author, even though they have only limited data per author. In this work, the goal is to predict user-specific replies and sentence-completion on mobile phones, but the idea is not dissimilar to that of \citeauthor{bagnall2015author}'s that we discussed previously.

\subsection{Style}

We have discussed how it is possible to attribute a work to a specific author based on specific stylistic features. However, it is important to note that the concept of authorship style is not well defined nor well understood, and different studies often refer to very different concepts under the same label of ``writing style''.  \citet{gatt2017survey}, in the survey on text generation that we previously mentioned state:

\begin{quote}
What does the term `linguistic style' refer to? Most work on what we
shall refer to as `stylistic nlg' shies away from a rigorous definition,
preferring to operationalise the notion in the terms most relevant to
the problem at hand.
\end{quote}

For authorship attribution, style is closely associated with, for example, which parts-of-speech tags authors choose to use (a specific author might use a lot of adjectives), how they choose to punctuate (some authors are proud of never having used a semi-colon in their lives; others use them frequently), or how long they typically make their sentences. By contrast, ``style'' often refers to a more general concept which includes the register, formality, or tone of a specific text. For example, \citet{jhamtani2017shakespearizing} show how it is possible to automatically ``translate'' between writing styles by taking modern text as input and producing text in the style of Shakespeare as output, while keeping the meaning of the text the same. In order to achieve this, we need to be able to discriminate between \textit{content} and \textit{style}. If we could do this reliably, authorship attribution would be a much easier task, as we could simply compare various writing styles needing to deal with content. However, \citet{jhamtani2017shakespearizing} needed parallel corpora to achieve the results that they did. These were available as all of Shakespeare's works have been translated into ``modern'' English manually. There is ongoing work on automatic style transfer. \citet{kabbara2016stylistic} propose the opposite of \citet{jhamtani2017shakespearizing}, that is, they present a proposal in which they aim to build a Recurrent Neural Network system that, trained on Shakespeare and Simple-English Wikipedia, could translate works written in the style of Shakespeare into a more modern and easier to read style. We discuss this problem further in Section \ref{method:svm-av}.

While we have two different versions of Shakespeare's work (the original version and the modernized version), this is uncommon. In nearly all cases, when we examine text written in different styles, the content differs as well. One exception is the book \textit{Exercises in Style} \cite{queneau1981exercises}, in which the same short story is re-written 99 times in different styles. The book was written in French, but has since been translated into over 30 other languages. While this work is of interest to any studies that relate to writing style, the book is too short to be of use in teaching machine learning models to discriminate between styles more generally, and moreover although the author consciously varies his style for the 99 variations, they are all still written by the same author (or translator). Therefore much of the subconscious writing style variations (such as the use of function words discussed above) would be lost.

However we define style, it is clear that the separation of content from style is necessary for many AA tasks. Recently, work has been done to separate content from artistic style in photographs and paintings. For example \citet{gatys2016image} show that it is possible to automatically alter modern photographs to make them resemble the distinctive artistic style of van Gogh. Central to this idea is that CNNs seem to learn different levels of abstraction at different layers. Lower layers often learn to recognise more fundamental features, such as shapes, while higher layers learn more abstract features, such as those we associate with style. This is related to the work by \citet{raghu2016expressive} who discuss the relation between the structure of a neural network and the functions that it is able to compute. They state that lower layers are more important and are more sensitive to noise and optimisations, while higher layers can model exponentially more complex functions. Unfortunately while image classification is often done with deeply stacked CNNs, thus allowing for different layers to learn content- and style-related features, this is not the same for text tasks, where sequences are more important than hierarchies. Therefore there is no work showing that textual style can be separate from content in the same way. 

\subsection{Related tasks}
\label{related-tasks}
By some definitions, other author profiling tasks also form part of general AA. These include, Age Profiling, Gender Profiling, Personality Profiling, and Native Language Identification. For age profiling the task is usually to predict which of several age ranges an author belongs to, looking only at text written by that author. For gender profiling, we attempt to discriminate between male and female authors. Personality profiling is less common, and usually attempts to predict Myers-Briggs personality types \cite{myers1962myers}. Native Language Identification is the task of predicting the authors first language from text produced in a second language (usually English). This has practical links to forensic linguistics, in that even if we cannot identify the exact author of an unknown text, it is often still useful to identify their nationality by using their native language as a proxy, and its ties to Authorship Attribution are discussed by \citet{stolerman2015authorship}. 

We do not investigate these tasks closely in the current work, but studies that focus on these areas is related to ours in that the systems that work well for these tasks also work for AID and AV, due to the fact that the tasks all fall under a general assumption of classifying authors based on their writing style. Specifically, the annual PAN AA shared task, which is one of our primary sources for AID and AV prior work, runs parallel shared task on Authorship Profiling, with a focus on age and gender tasks. Methods which use Support Vector Machine classifiers have shown to outperform other methods in all of these tasks. We were part of the top-performing team in the PAN 2017 shared task, and we again found SVMs to outperform other methods \cite{basile2017ngram}. Similarly \citet{verhoeven2016twisty} use SVMs and TF-IDF vectors for gender and personality profiling.
