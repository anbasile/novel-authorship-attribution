\chapter{Introduction}
\label{chap:intro}

Authorship Attribution, or identifying an author by analysing their writing style, has fascinated researchers for centuries. Historically, such analyses were carried about manually, with experts counting word and character occurrences in the combined works of a specific author in order to try build a ``fingerprint'' for that author. A disputed work could then be analysed to see if the frequencies of specific words and characters was similar to the corpus of \textit{known} works by the alleged author. 

More recently, computers have proven adept at recognising author's unique writing styles. This is not only because they are better at calculating frequencies across large amounts of text, but also because they can take into account millions of patterns. Where a human might focus on the number of times a specific author used semicolons, or on adjective frequency, or on sentence length, a computer can simultaneously pay attention to every conceivable pattern. We can use computers to instantly calculate statistical summaries over large bodies of text, and by extension we can easily compare a disputed text to the known texts of the alleged author.

Perhaps the most famous examples of Authorship Attribution is the case of \textit{The Federalist Papers}, a collection of essays written by Alexander Hamilton, James Madison, and John Jay. In 1964, \citet{mosteller2007inference} showed how word frequencies and statistical analysis could be used for Authorship Attribution by using these methods to provide strong evidence for which of the three authors had written which essays in the collection.

A more recent example of computers assisting in Authorship Attribution is that of J. K. Rowling, who attempted to publish a crime novel under the pseudonym Robert Galbraith after becoming world famous for the \textit{Harry Potter} series. She was interested in seeing if her books only sold well because she was famous, and described writing under a pseudonym by saying ``It has been wonderful to publish without hype or expectation and pure pleasure to get feedback under a different name''\footnote{http://www.telegraph.co.uk/culture/books/10178344/JK-Rowling-unmasked-as-author-of-acclaimed-detective-novel.html}. Her pseudonym did not last long however, and computers helped in supporting the hypothesis that Robert Galbraith was in fact J. K. Rowling. This series of events brought forensic linguistics and stylometery closer to public interest, after it was reported in major media outlets. \textit{Smithsonian}\footnote{http://www.smithsonianmag.com/science-nature/how-did-computers-uncover-jk-rowlings-pseudonym-180949824/} described it as follows:

\begin{quote}
Consider the recent outing of Harry Potter author J.K. Rowling as the writer of The Cuckoo’s Calling, a crime novel she published under the pen name Robert Galbraith. England’s Sunday Times, responding to an anonymous tip that Rowling was the book’s real author, hired Duquesne University’s Patrick Juola to analyze the text of Cuckoo, using software that he had spent over a decade refining. One of Juola’s tests examined sequences of adjacent words, while another zoomed in on sequences of characters; a third test tallied the most common words, while a fourth examined the author’s preference for long or short words. Juola wound up with a linguistic fingerprint -- hard data on the author’s stylistic quirks.

He then ran the same tests on four other books: The Casual Vacancy, Rowling’s first post-Harry Potter novel, plus three stylistically similar crime novels by other female writers. Juola concluded that Rowling was the most likely author of The Cuckoo’s Calling, since she was the only one whose writing style showed up as the closest or second-closest match in each of the tests. After consulting an Oxford linguist and receiving a concurring opinion, the newspaper confronted Rowling, who confessed.
\end{quote}

Getting computers to help with statistical analyses is only the first step, however. While we as humans can create some useful approximations for what defines a specific author's style (for example, vocabulary, punctuation, sentence length, and various other features) and can use computers to quickly calculate these statistics over many texts, we still need to instruct the computers about each of these rules and then decide how to interpret the comparative results. In the description above, Joula noticed that the style of Galbraith's writing was \textit{more} similar to J. K. Rowling than three other authors, when comparing various statistical analyses that he thought important, but this required an expert to create the software and analyse the results -- a step that was only taken after a tip-off that suggested Rowling might be the true author. Using recent advances in Machine Learning, and Natural Language Processing, it is possible to have a computer learn these rules on its own, from existing examples. Using a Machine Learning approach, we remove to some extent the need for an expert, and we add the possibility that our Machine Learning algorithms can learn counter-intuitive rules that an expert may not have thought of.

Our work experiments with various, often novel, approaches to the Authorship Attribution problem. We use various models, including unsupervised approaches that can assist with human-based analysis, and Machine Learning approaches that learn rules about what constitutes authorship style from large amounts of data. For the automated approaches, we use Support Vector Machines and Neural Networks -- both of which have proven to be capable of learning meaningful rules from textual data with minimal human interference. 

Although the focus of Authorship Attribution has often been on literature, it has many modern practical applications in forensic linguistics as well. For example, \citet{afroz2014doppelganger} show how authorship attribution can be used for deanonymizing criminals in underground internet forums, using their writing style alone. Similar techniques have been used to identify the author of computer code, for cases involving malicious software or ``malware'' investigations. Programmers, like writers, also have their own specific ``style'' and this can be used to identify them even when they have taken steps to never reveal their true identity when selling their code to criminals.  

This work is organized as follows:

\begin{itemize}
    \item In Chapter 2 we provide an overview of prior research. We focus on work which is closely connected with the methods that we use here.
    \item in Chapter 3 we provide a brief overview of the different methods with which we experiment. This is intended to be accessible to non-experts.
    \item in Chapter 4, we provide an overview of the datasets that we use. We hope that this will help in any attempts to reproduce or extend our work.
    \item in Chapter 5, we provide a detailed description of each of our experimental models. In contrast to Chapter 3, this chapter does not attempt to be accessible to non-experts, and contains sufficient detail to reproduce our algorithms and models.
    \item in Chapter 6, we provide the results we achieved, comparing to previous results on the same datasets where applicable, and discuss what these results mean.
    \item in Chapter 7, we summarize our entire work and provide some suggestions for future research.
\end{itemize}

