{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from spacy.en import English\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# config\n",
    "pan15train = \"/data/pan15-authorship-verification-training-dataset-english-2015-04-19/\"\n",
    "pan15test = \"/data/pan15-authorship-verification-test-dataset2-english-2015-04-19/\"\n",
    "pan14train = \"/data/pan14-author-verification-training-corpus-english-essays-2014-04-22/\"\n",
    "pan14test = \"/data/pan14-author-verification-test-corpus2-english-essays-2014-04-22/\"\n",
    "char_embeddings_file = \"glove.840B.300d-char.txt\"  # full path to the character embeddings file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def read_file(filepath):\n",
    "    with open(filepath) as f:\n",
    "        s = f.read()\n",
    "    return s\n",
    "\n",
    "def load_pan_data(directory, prefix=\"E\"):\n",
    "    \"\"\"Load known and unknown texts in the PAN data format\"\"\"\n",
    "    # FIXME: assumes one known file per author, which is fine for English datasets only\n",
    "    authors = sorted([x for x in os.listdir(directory) if x.startswith(prefix)])\n",
    "    known_texts = []\n",
    "    unknown_texts = []\n",
    "    for author in authors:\n",
    "        kf = os.path.join(directory, author, \"known01.txt\")\n",
    "        uf = os.path.join(directory, author, \"unknown.txt\")\n",
    "        known_texts.append(read_file(kf))\n",
    "        unknown_texts.append(read_file(uf))\n",
    "        \n",
    "    truthfile = os.path.join(directory, \"truth.txt\")\n",
    "    with open(truthfile) as f:\n",
    "        lines = f.read().strip().split(\"\\n\")\n",
    "    y = [1 if line.split()[1] == \"Y\" else 0 for line in lines]\n",
    "    y = np.array(y)\n",
    "    return known_texts, unknown_texts, y\n",
    "\n",
    "def create_pairs(knownX, unknownX):\n",
    "    print(len(knownX), len(unknownX))\n",
    "    \"\"\"Creates pairs of known and unknown texts\"\"\"\n",
    "    pairs = []\n",
    "    for i in range(len(knownX)):\n",
    "        pairs += [[knownX[i], unknownX[i]]]\n",
    "    pairs = np.array(pairs)\n",
    "    print(pairs.shape)\n",
    "    return pairs\n",
    "\n",
    "def vectorize(text, nlp):\n",
    "    \"\"\"Convert text (string) to embeddings (numpy array)\n",
    "    nlp should be an initialised Spacy pipeline with loaded embeddings\"\"\"\n",
    "    return nlp(text, entity=False, tag=False, parse=False).vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "import random\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Lambda, Embedding, LSTM, Dropout, Masking, Conv1D, MaxPooling1D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))\n",
    "\n",
    "def cosine_distance(vests):\n",
    "    x, y = vests\n",
    "    x = K.l2_normalize(x, axis=-1)\n",
    "    y = K.l2_normalize(y, axis=-1)\n",
    "    return -K.mean(x * y, axis=-1, keepdims=True)\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    margin = 1\n",
    "    return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))\n",
    "\n",
    "def create_base_network(input_dim):\n",
    "    '''Base network to be shared (eq. to feature extraction).\n",
    "    '''\n",
    "    seq = Sequential()\n",
    "    seq.add(Dense(64, input_shape=(input_dim,), activation='relu'))\n",
    "    seq.add(Dense(64, activation='relu'))      \n",
    "    return seq\n",
    "\n",
    "def compute_accuracy(predictions, labels):\n",
    "    return np.mean(np.equal(predictions.ravel() < 0.5, labels))\n",
    "\n",
    "def train(tr_pairs, tr_y, epochs=20):\n",
    "    input_dim = tr_pairs.shape[-1]\n",
    "\n",
    "    # network definition\n",
    "    base_network = create_base_network(input_dim)\n",
    "\n",
    "    input_a = Input(shape=(input_dim,))\n",
    "    input_b = Input(shape=(input_dim,))\n",
    "\n",
    "    # because we re-use the same instance `base_network`,\n",
    "    # the weights of the network\n",
    "    # will be shared across the two branches\n",
    "    processed_a = base_network(input_a)\n",
    "    processed_b = base_network(input_b)\n",
    "    distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n",
    "    model = Model(inputs=[input_a, input_b], outputs=distance)\n",
    "    \n",
    "    rms = RMSprop()\n",
    "    model.compile(loss=contrastive_loss, optimizer=rms)\n",
    "    model.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,\n",
    "              validation_split=0.1,\n",
    "              batch_size=20,\n",
    "              epochs=epochs)\n",
    "    return model\n",
    "       \n",
    "def evaluate(model, tr_pairs, tr_y, te_pairs, te_y):\n",
    "    # compute final accuracy on training and test sets\n",
    "    pred = model.predict([tr_pairs[:, 0], tr_pairs[:, 1]])\n",
    "    tr_acc = compute_accuracy(pred, tr_y)\n",
    "    pred = model.predict([te_pairs[:, 0], te_pairs[:, 1]])\n",
    "    te_acc = compute_accuracy(pred, te_y)\n",
    "    print('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))\n",
    "    print('* Accuracy on test set: %0.2f%%' % (100 * te_acc))\n",
    "    \n",
    "def combine_vectors(v1s, v2s):\n",
    "    return [np.hstack([v1s[i], v2s[i]]) for i in range(len(v1s))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "known_train, unknown_train, y_train = load_pan_data(pan15train)\n",
    "known_test, unknown_test, y_test = load_pan_data(pan15test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 1, 1, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/tmp/pg/output/cwnb.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-765c967127be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/tmp/pg/output/cwnb.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/externals/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    565\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_read_fileobject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/tmp/pg/output/cwnb.pickle'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "classifier = joblib.load(\"/tmp/pg/output/cwnb.pickle\")\n",
    "\n",
    "\n",
    "with open(\"/tmp/pg/output/cwvec.pickle\", \"rb\") as f:\n",
    "    vectorizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_relation_vector(text, classifier, vectorizer):\n",
    "    \"\"\"Find out how similar a piece of text is to our corpus, sentence by sentence\"\"\"\n",
    "    sentences = text.replace(\"\\n\", \" \").split(\".\")\n",
    "    sentences = [x.strip() for x in sentences]\n",
    "    print(\".\")\n",
    "    textvec = vectorizer.transform(sentences)\n",
    "    print(\"vec\")\n",
    "    predictions = classifier.predict_proba(textvec)\n",
    "    print(\"cls\")\n",
    "    return predictions\n",
    "    \n",
    "    # sum the probability distribution and normalize by number of sentences\n",
    "    # relation_vector = np.mean(predictions, axis=0)\n",
    "    # relation_vector /= len(predictions)\n",
    "    return relation_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 1, 1, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-24c57fcbe78d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mk1preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_relation_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mknown_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mu1preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_relation_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munknown_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classifier' is not defined"
     ]
    }
   ],
   "source": [
    "k1preds = get_relation_vector(known_train[1], classifier, vectorizer)\n",
    "u1preds = get_relation_vector(unknown_train[1], classifier, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-304fe45f0b12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mk0preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_relation_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mknown_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mu0preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_relation_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munknown_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classifier' is not defined"
     ]
    }
   ],
   "source": [
    "k0preds = get_relation_vector(known_train[0], classifier, vectorizer)\n",
    "u0preds = get_relation_vector(unknown_train[0], classifier, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "k4preds = get_relation_vector(known_train[4], classifier, vectorizer)\n",
    "u4preds = get_relation_vector(unknown_train[4], classifier, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "[np.argmax(s) for s in k0preds if max(s) > 0.04 and np.argmax(s) != 98]\n",
    "[np.argmax(s) for s in u0preds if max(s) > 0.04 and np.argmax(s) != 98]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "s1 = set(np.array([np.argpartition(k1preds[i], -5)[-5:] for i in range(len(k1preds))]).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "s2 = set(np.array([np.argpartition(u1preds[i], -5)[-5:] for i in range(len(u1preds))]).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "s1.intersection(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "known_rels = [get_relation_vector(t, classifier, vectorizer) for t in known_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "unknown_rels = [get_relation_vector(t, classifier, vectorizer) for t in unknown_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean, cosine\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "def minmax(x, y):\n",
    "    mins, maxs = 0.0, 0.0\n",
    "    for i in range(x.shape[0]):\n",
    "        a, b = x[i], y[i]\n",
    "        if a >= b:\n",
    "            maxs += a\n",
    "            mins += b\n",
    "        else:\n",
    "            maxs += b\n",
    "            mins += a\n",
    "    if maxs > 0.0:\n",
    "        return 1.0 - (mins / maxs)\n",
    "    return 0.0\n",
    "\n",
    "distances = [euclidean(*x) for x in zip(known_rels, unknown_rels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sdists = [known_rels[i] - unknown_rels[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = SVC()\n",
    "\n",
    "cross_val_score(clf, distances, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "same = [x for i, x in enumerate(distances) if y_train[i]]\n",
    "diff = [x for i, x in enumerate(distances) if not y_train[i]]\n",
    "print(len(same), len(diff))\n",
    "plt.scatter(range(len(same)), same)\n",
    "plt.scatter(range(len(diff)), diff)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "distances[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nb = MultinomialNB()\n",
    "vec = TfidfVectorizer()\n",
    "\n",
    "vec.fit(known_train + unknown_train)\n",
    "\n",
    "ktrvecs = vec.transform(known_train)\n",
    "utrvecs = vec.transform(unknown_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fsentences = unknown_train[0].split(\".\")\n",
    "fvecs = vec.transform(fsentences)\n",
    "print(nb.predict(fvecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# del known_train[20]\n",
    "ktrvecs = vec.transform(known_train)\n",
    "labels = list(range(100))\n",
    "del labels[20]\n",
    "nb.fit(ktrvecs, labels)\n",
    "print(nb.predict(fvecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "known_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "unknown_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_NB_WORDS = 5000\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "\n",
    "known_train, unknown_train, y_train = load_pan_data(pan15train)\n",
    "known_test, unknown_test, y_test = load_pan_data(pan15test)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(known_train + unknown_train + known_test + unknown_test)\n",
    "\n",
    "known_seqs_tr = pad_sequences(tokenizer.texts_to_sequences(known_train), MAX_SEQUENCE_LENGTH)\n",
    "unknown_seqs_tr = pad_sequences(tokenizer.texts_to_sequences(unknown_train), MAX_SEQUENCE_LENGTH)\n",
    "known_seqs_te = pad_sequences(tokenizer.texts_to_sequences(known_test), MAX_SEQUENCE_LENGTH)\n",
    "unknown_seqs_te = pad_sequences(tokenizer.texts_to_sequences(unknown_test), MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "embeddings_index = {}\n",
    "with open(\"/data/glove/glove.6B.100d.txt\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# prepare embedding matrix\n",
    "EMBEDDING_DIM = 100\n",
    "num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "print(num_words)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "known_seqs_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tr_pairs = create_pairs(known_seqs_tr, unknown_seqs_tr)\n",
    "te_pairs = create_pairs(known_seqs_te, unknown_seqs_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "known_train, unknown_train, y_train = load_pan_data(pan15train)\n",
    "known_test, unknown_test, y_test = load_pan_data(pan15test)\n",
    "print(\"word vec...\")\n",
    "# word vectors\n",
    "w_known_train_vecs = [vectorize(t, nlp_word) for t in known_train]\n",
    "w_unknown_train_vecs = [vectorize(t, nlp_word) for t in unknown_train]\n",
    "w_known_test_vecs = [vectorize(t, nlp_word) for t in known_test]\n",
    "w_unknown_test_vecs = [vectorize(t, nlp_word) for t in unknown_test]\n",
    "# print(\"char vec...\")\n",
    "# character vectors\n",
    "c_known_train_vecs = [vectorize(t, nlp_char) for t in known_train]\n",
    "c_unknown_train_vecs = [vectorize(t, nlp_char) for t in unknown_train]\n",
    "c_known_test_vecs = [vectorize(t, nlp_char) for t in known_test]\n",
    "c_unknown_test_vecs = [vectorize(t, nlp_char) for t in unknown_test]\n",
    "print(\"combining...\")\n",
    "# word + character vectors\n",
    "wc_known_train_vecs = combine_vectors(w_known_train_vecs, c_known_train_vecs)\n",
    "wc_unknown_train_vecs = combine_vectors(w_unknown_train_vecs, c_unknown_train_vecs)\n",
    "wc_known_test_vecs = combine_vectors(w_known_test_vecs, c_known_test_vecs)\n",
    "wc_unknown_test_vecs = combine_vectors(w_unknown_test_vecs, c_unknown_test_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def run_experiment(known_train, unknown_train, known_test, unknown_test, y_train, y_test):\n",
    "    tr_pairs = create_pairs(known_train, unknown_train)\n",
    "    te_pairs = create_pairs(known_test, unknown_test)\n",
    "    \n",
    "    model = train(tr_pairs, y_train)\n",
    "    evaluate(model, tr_pairs, y_train, te_pairs, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = train(te_pairs, y_test, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "evaluate(model, tr_pairs, y_train, te_pairs, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# known_train, unknown_train, y_train = load_pan_data(pan14train)\n",
    "# known_test, unknown_test, y_test = load_pan_data(pan14test)\n",
    "run_experiment(known_seqs_te, unknown_seqs_te, known_seqs_tr, unknown_seqs_tr, y_test, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "datadir = \"/Users/g/Downloads/Gutenberg/txt/\"\n",
    "files = [x for x in os.listdir(datadir) if x.endswith(\".txt\")]\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "for fname in files:\n",
    "    with open(os.path.join(datadir,fname), encoding=\"ISO-8859-1\") as f:\n",
    "        text = f.read()\n",
    "    texts.append(text)\n",
    "    labels.append(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "w3vec = TfidfVectorizer(ngram_range=(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "t1 = datetime.now()\n",
    "w3vecs = w3vec.fit(texts)\n",
    "print(datetime.now() - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "w3vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "cvec = TfidfVectorizer(analyzer='char', ngram_range=(2,4), max_df=4)\n",
    "wvec = TfidfVectorizer(ngram_range=(1,2), max_df=4)\n",
    "\n",
    "# rcvec = TfidfVectorizer(analyzer='char', ngram_range=(2,4), max_df=0.01)\n",
    "# rwvec = TfidfVectorizer(ngram_range=(1,2), max_df=0.01)\n",
    "vec = FeatureUnion([\n",
    "    ('cvec', cvec),\n",
    "    ('wvec', wvec)\n",
    "])\n",
    "\n",
    "cls = MultinomialNB(alpha=0.001)\n",
    "\n",
    "datadir = \"/Users/g/Downloads/Gutenberg/txt/sample/\"\n",
    "files = [os.path.join(datadir,fn) for fn in next(os.walk(datadir))[2]]\n",
    "texts = []\n",
    "labels = []\n",
    "for fname in files:\n",
    "    with open(fname) as f:\n",
    "        s = f.read()\n",
    "    texts.append(s)\n",
    "    labels.append(fname.split(\"/\")[-1])\n",
    "    \n",
    "\n",
    "    \n",
    "# keep = [1,2,5,8,14,17]\n",
    "\n",
    "# texts = [text for i,text in enumerate(texts) if i in keep]\n",
    "# labels = [label for i,label in enumerate(labels) if i in keep]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vecs = vec.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cls.fit(vecs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(\"/Users/g/Downloads/Gutenberg/txt/Mark Twain___A Tramp Abroad.txt\") as f:\n",
    "    s = f.read()\n",
    "    \n",
    "nvecs = vec.transform([s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "probs = cls.predict_proba(nvecs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alfred Russel Wallace___Island Life.txt',\n",
       " 'Charles Darwin___The Descent of Man and Selection in Relation to Sex Volume II (1st Edition).txt',\n",
       " 'Charlotte Bronte___Jane Eyre.txt',\n",
       " 'Charlotte Mary Yonge___The Trial.txt',\n",
       " 'D H Lawrence___The Rainbow.txt',\n",
       " \"Harriet Elizabeth Beecher Stowe___Uncle Tom's Cabin.txt\",\n",
       " 'Henry Rider Haggard___Dawn.txt',\n",
       " 'James Fenimore Cooper___Mercedes of Castile.txt',\n",
       " 'Lord Byron___The Works of Lord Byron: Letters and Journals. Vol. 2.txt',\n",
       " 'Louisa May Alcott___Little Women.txt',\n",
       " 'Mark Twain___Following the Equator, Complete.txt',\n",
       " 'Robert Louis Stevenson___The Works of Robert Louis Stevenson - Swanston Edition, Volume 25.txt',\n",
       " 'Sir Walter Scott___Guy Mannering.txt',\n",
       " 'Thomas Henry Huxley___Essays Upon Some Controverted Questions.txt',\n",
       " 'Wilkie Collins___The Moonstone.txt',\n",
       " 'William Dean Howells___Literature and Life.txt',\n",
       " 'William Makepeace Thackeray___The History of Henry Esmond, Esq.txt',\n",
       " 'Winston Churchill___Richard Carvel, Complete.txt']"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.99999999199053491, 10),\n",
       " (5.3710585658899181e-09, 9),\n",
       " (7.5889394292526667e-10, 5),\n",
       " (4.7750333688270632e-10, 6),\n",
       " (3.2176000164137913e-10, 15),\n",
       " (3.0454990246221039e-10, 16),\n",
       " (2.1852132071818747e-10, 17),\n",
       " (1.6120581024159878e-10, 14),\n",
       " (9.6294829881725523e-11, 12),\n",
       " (9.3106340646671123e-11, 4),\n",
       " (6.5480246425681415e-11, 3),\n",
       " (6.281149778646114e-11, 1),\n",
       " (4.698160923084154e-11, 0),\n",
       " (1.3432248403596974e-11, 2),\n",
       " (9.7028625161261614e-12, 11),\n",
       " (6.999781416183236e-12, 7),\n",
       " (1.1529132404225449e-12, 8),\n",
       " (1.9621617448346977e-13, 13)]"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(x,i) for i, x in enumerate(probs)], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_probs(text, classifier, vectorizer):\n",
    "    vecs = vectorizer.transform([text])\n",
    "    probs = classifier.predict_proba(vecs)\n",
    "    return probs[0]\n",
    "    return sorted([(x,i) for i, x in enumerate(probs)], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, spearmanr\n",
    "from scipy.spatial.distance import euclidean\n",
    "corrs = []\n",
    "pairs = []\n",
    "for i in range(len(known_train)):\n",
    "    probs_k = get_probs(known_train[i], cls, vec)\n",
    "    probs_u = get_probs(unknown_train[i], cls, vec)\n",
    "    correlation = pearsonr(probs_k, probs_u)[0]\n",
    "    # correlation = euclidean(probs_k, probs_u)\n",
    "    corrs.append(correlation)\n",
    "    pairs.append((probs_k[:], probs_u[:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.19815314915191615,\n",
       " 0.6946779903948187,\n",
       " 0.31151294280240333,\n",
       " 0.067101358003576278,\n",
       " 0.75778955012574212,\n",
       " 0.75230073206835435,\n",
       " 0.72714035783737352,\n",
       " -0.24569773997361741,\n",
       " 0.088672215506988186,\n",
       " -0.19869364184115995]"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 0, 0, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿ English, my English!  \n",
      "Assesing ones strengths and weaknesses in any situation is a hard task. \n",
      "Usually we are not so good at recognizing our strengths, instead we spend our time being critical of ourselves, thus we tend to present a much more detailed side of our flaws. \n",
      "I will try to give you a reasonable picture of my good and not quite as good sides, as I see them, in reference to the four skills that were mentioned in the guide-lines handed out earlier in regard of this essay.   y biggest weakness by far is reading in English. \n",
      "I most of the time find it quite boring, it probably has to do with the fact that I'm a very outgoing person and therefore I like meeting people and I prefer to converse with them. \n",
      "A contributing factor to my feelings about reading also has to do with the fact that I'm just not used to it, which makes reading a bit uncomfortable. \n",
      "It takes time to read and it is not by far as fun as actually talking with someone.\n",
      "You miss out on facial expressions and the dialogue for example. \n",
      "It's not that I can't manage, I just don't find it all that amusing, at times. \n",
      "This leads too the fact that I more or less skim through anything written in English, which ultimately results in my misinterpretations. \n",
      "But I am indeed working on that partly since it is required in order to be able to teach but also for my own benefit. \n",
      "I have been known to read books in English on occasion though. \n",
      "Preferably novels taking place during the nineteenth century when in my personal opinion the English language was at its best. \n",
      "I think it's terrible that we've lost so much of the chivalry, the sence of class and style that one can encounter in for example Jane Austens' \"Pride & Prejudice\", or the vocabulary used in Shakespeares works.  \n",
      "This brings us to the speaking, which I believe is one of my strongest features, not only when it comes to speaking English, but speaking in general. \n",
      "I admit that it sometimes does get out of hand. \n",
      "You see, in my previous English classes I've always been the one to speak my opinion and discuss things that are important to me. \n",
      "This might and actually has lead to the fact that others who are not as forward as I am just sit quiet and on top of that they're doing absolutely nothing to change or take any kind of initiative to speak. \n",
      "This annoyes me, should I keep holding back to suit them or should I allow myself the oportunity to improve and broaden my horizons!? \n",
      "In this class though it doesn't seem to be a thing that even needs to be discussed, since everybody is fairly good in English, and not at all shy.  \n",
      "I do know how and when to use a very formal vocabulary, but I tend to disregard that at times when I get \"too comfortable\" with the person I'm addressing, it's not a lack of respect or anything of that nature, I just get too \"buddy-ish\". \n",
      "I think that speaking a lot helps me improve all the aspects of English, writing, reading, listening but most importantly to communicate in any given situation. \n",
      "My desire to communicate enables people in my everyday life to correct me when I'm wrong and vice versa. \n",
      "I believe there are two kinds of English students. \n",
      "The ones that learn the language by using grammar and dictionaries and school-English, not that there is anything wrong with that. \n",
      "In fact I think it's a good start, a solid base to begin at. \n",
      "And then the others, much like myself, who learn by actually speaking, conversing and therefore picking up a rich vocabulary. \n",
      "Both have advantages and disadvantages. \n",
      "I've learned my English by speaking, practising and perhaps mostely by listening to others.  \n",
      "Listening is also one of my strengths. \n",
      "For this is where I learn and adapt. \n",
      "I like watching news, documentaries or whatever is on in English. \n",
      "If I'm lucky I might even catch one of David Attenboroughs' \"wild-life-documentaries\". \n",
      "He has an elegant vocabulary and intonation. \n",
      "Although I must say that if it were possible to choose I'd pick one of the people behind the series of \"The young ones\", preferably Rick Mayallik. \n",
      "But I'm not apposed to listening to sports either, as long as it is English.  \n",
      "I've come to the last of the four skills, namely writing. \n",
      "I think I'm fairly good at that. \n",
      "It takes a lot of hard work though. \n",
      "When I took \"Business English\" last fall we got to do a lot of inquiries, and business-letters which I enjoyed very much. \n",
      "At times it is easier to write when you have a given assignment, than to just come up with an idea.\n",
      "This forces me to try not to drift away, as I have a tendency to do just that. \n",
      "I've never had an English pen pal, so I've never really written on a regular basis other than when I've been in school. \n",
      "My knowledge of gramatical terms is terrible which might be detected from time to time. \n",
      "Although I seem to be able to communicate without any larger difficulties.   \n",
      "In conclusion: It seems I've got my work cut out for me for at least the next two years ahead. \n",
      "Hopefully I'll learn to master things and tackle my weak spots quickly. \n",
      "After that there are other challenges that need to be attended to.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(known_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿ In the name of Religion, you can get away with anything!  \n",
      "With this argumentative piece I've tried to shed a light on the consequences of religion. \n",
      "What goes on, apart from all of the good deeds man does in the name of religion. \n",
      "I've devided them in five headings, all regarding different aspects of the topic. \n",
      "I myself am not a believer. \n",
      "I'm a naturally born critic or sceptic, if you will. \n",
      "I firmly believe that it should be up to every man/woman to inividually process information and facts that are given or thrown upon us. \n",
      "Just because something has been going on for a long period of time doesn't necessarily mean it's alright.  \n",
      "1) Religion is prohibiting.  \n",
      "It compromises the person. \n",
      "Religion prevents individual thinking and persuades mankind to accept and \"buy the whole concept\" of its ideas in the scriptures. \n",
      "The scriptures compromise peoples prerogative to think for themselves, they roothlesly rob people of their natural ability to evaluate and act on their own intuition. \n",
      "Religion makes the individual fall in line and not to question anything it says. \n",
      "Is that a good thing, I ask? \n",
      "Is compromising yourself necessary, in the name of religion?  \n",
      "2) Religion causes conflicts.  \n",
      "Lets look at religion in a passed-present-future perspective.  \n",
      "I don't think I'm going out on a limb if I point out that more or less all wars, some excluded originates from religious grounds and believes. \n",
      "The discontent of people who don't agree with you or don't share your believes causes conflicts in the name of religion.  \n",
      "3) Religion prevents individual freedom.  \n",
      "In the end of 1980, an Irish-catholic girl was brutaly raped and got pregnant. \n",
      "She suffered in agony for she wanted to have the foetus removed but her religion forbids abortion. \n",
      "Her case was globaly acknowledged in the media and many, myself included felt for her and witnessed the madness, in the name of religion. \n",
      "The bible, an ancient book of moral clauses and devises for a pure way of life, said she couldn't have an abortion. \n",
      "Luckely she could get help in England and have the abortion done there. \n",
      "Imagine if she hadn't, what life would that child have led!?  \n",
      "4) Religion discriminates.  \n",
      "Sure, it unifies certain people who share the same ideology. \n",
      "On the other hand it condems different genres even within the same religion and thus the people who don't agree with them. \n",
      "People who don't share their point of view or religious believes. \n",
      "Is it righteous to commit cruel actions as long as it is in the name of religion?   \n",
      "5) Religious downpayment!? \n",
      "Even in the early days you were able to go to your church and seek repentance for your sins. \n",
      "You mearly went to a priest and confessed and he would gladly give it to you. \n",
      "He might suggest that you ought to do a few Hail Mary's and suddenly in an instance you had been forgiven. \n",
      "Back then you could even assure your place in heaven when buying a letter of indulgence. \n",
      "Buy your way into the Lords glory, heaven, kingdom or what ever you preferred. \n",
      "Today's version of that same phenomena is called Televangelism. \n",
      "The 80's way to repentance. \n",
      "Televised fundraisings, usually held by a spiritual leader of some kind. \n",
      "He would very convincingly encourage the viewers to support his \"cause\", by donating obnoxiously large sums of money to him and the \"cause\".  \n",
      "The cause might be o an honest nature, to build evangelist churches or help the poor. \n",
      "But in some cases they were spend for personal gain by the so called prophet. \n",
      "Outrageous news of spiritual leaders spending their money on prostitutes and extramarital affairs. \n",
      "These people who are supposed to set an example to their followers. \n",
      "I guess their only human, but it can't feel all that to be good old Mrs Lewis in the south of California, who recently donated a large sum to \"reverend Chastity\", and learn that he has spent \"her\" money on questionable things, can it!? \n",
      "It's a strange world we live in where some people profits from other peoples misery.   \n",
      "Conclusion: Imagine a world without prejudice of religion, race, colour or believes! \n",
      "Almost impossible to picture I know. \n",
      "Religion should be more humble and \"human\", it should serve as a medium that brings people together, not to discriminate. \n",
      "I'm not saying that it should be removed or terminated, because many people need to be able to put their trust in a supreme being. \n",
      "It needs to be altered/changed or modified, since everything else has been in order to fit in in today's society. \n",
      "Many vicious things have been done, is being done and will be done in the name of religion. \n",
      "It is not acceptable! \n",
      "TAKE ACTION!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pairs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.3350353902965163, 15),\n",
       " (0.20131051845460246, 4),\n",
       " (0.13004367334862393, 9),\n",
       " (0.068448876772215714, 5),\n",
       " (0.052951978216860505, 2)]"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_n(pairs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.26263545481161771, 13),\n",
       " (0.17495152089484001, 9),\n",
       " (0.12145389340008755, 4),\n",
       " (0.090082522812767213, 10),\n",
       " (0.080364906706658829, 5)]"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_n(pairs[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "diffs = [(abs(pairs[i][0] - pairs[i][1])) for i in range(len(pairs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.52941176,  0.38235294,  0.5       ])"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "dtc = LinearSVC()\n",
    "\n",
    "cross_val_score(dtc, diffs, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 1, 1, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "probs_k = get_probs(known_train[0], cls, vec)\n",
    "probs_u = get_probs(unknown_train[0], cls, vec)\n",
    "correlation = pearsonr(probs_k, probs_u)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.26246228675188588, 15),\n",
       " (0.18054003355657608, 11),\n",
       " (0.13850120787692094, 8),\n",
       " (0.094010621134931582, 2),\n",
       " (0.087282149361291902, 6)]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(x,i) for i,x in enumerate(probs_k)], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.21970686997968397, 8),\n",
       " (0.17872084440275832, 5),\n",
       " (0.11205463908917677, 6),\n",
       " (0.097153017650052964, 2),\n",
       " (0.069663401737269259, 15)]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(x,i) for i,x in enumerate(probs_u)], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "preds = [1 if c > 0.3 else 0 for c in corrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.46      0.49        50\n",
      "          1       0.53      0.60      0.56        50\n",
      "\n",
      "avg / total       0.53      0.53      0.53       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 43, 1: 57})"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 1, 0, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'Alfred Russel Wallace___Island Life.txt'),\n",
       " (1,\n",
       "  'Charles Darwin___The Descent of Man and Selection in Relation to Sex Volume II (1st Edition).txt'),\n",
       " (2, 'Charlotte Bronte___Jane Eyre.txt'),\n",
       " (3, 'Charlotte Mary Yonge___The Trial.txt'),\n",
       " (4, 'D H Lawrence___The Rainbow.txt'),\n",
       " (5, \"Harriet Elizabeth Beecher Stowe___Uncle Tom's Cabin.txt\"),\n",
       " (6, 'Henry Rider Haggard___Dawn.txt'),\n",
       " (7, 'James Fenimore Cooper___Mercedes of Castile.txt'),\n",
       " (8, 'Lord Byron___The Works of Lord Byron: Letters and Journals. Vol. 2.txt'),\n",
       " (9, 'Louisa May Alcott___Little Women.txt'),\n",
       " (10, 'Mark Twain___Following the Equator, Complete.txt'),\n",
       " (11,\n",
       "  'Robert Louis Stevenson___The Works of Robert Louis Stevenson - Swanston Edition, Volume 25.txt'),\n",
       " (12, 'Sir Walter Scott___Guy Mannering.txt'),\n",
       " (13, 'Thomas Henry Huxley___Essays Upon Some Controverted Questions.txt'),\n",
       " (14, 'Wilkie Collins___The Moonstone.txt'),\n",
       " (15, 'William Dean Howells___Literature and Life.txt'),\n",
       " (16, 'William Makepeace Thackeray___The History of Henry Esmond, Esq.txt'),\n",
       " (17, 'Winston Churchill___Richard Carvel, Complete.txt')]"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(i,x) for i,x in enumerate(cls.classes_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Oh, he's coming, he's\\ncoming. I'm going to get to see Santa Claus! Is it not wonderful? I'm\\ngoing to see him. Let me look.  Oh, it's getting\\nbigger and _bigger_ and BIGGER!\\n\\nHurray! daddy's coming! daddy's coming!\\n\\nNow I can hear the bells. Oh, it's coming closer and _closer_\\nand CLOSER. Look out, it's going to hit the boat! \\n\\nHe flew right by us.\\n\\nMaybe he didn't see the boat. Oh, now he isn't coming at all.\\n\\nYes, he is. He's landed right over there. Here he comes; here he comes!\\n\\nHere we are, Santa Claus. This is the place. Come in. Merry Christmas, Santa Claus, merry Christmas!\\n\\n\\n Hello, there--where are you? It's so dark I can't see a single thing.\\n\\n Hello, daddy; merry Christmas.\\n\\n Hello yourself. Merry Christmas to you, too. Are you all ready for me?\\n\\nYes, it's all ready. The magical tree is just waiting for\\nyour touch to turn into a real Christmas tree.\\n\\n Oh, we're going to have a real Christmas tree.\\n\\nHello, who's this young person?\\n\\nThis is Anita.\\n\\nAnd why isn't she sound asleep like the rest of the\\nchildren?\\n\\nShe's such a good little girl that I told her she could\\nstay up with me and wait until you came.\\n\\nOh, ho; so you've made a hit with my boy, Jack\\nFrost, have you? Well, if that's the case, I guess you can stay.\\n\\nBut all of the children would like to see you, Santa Claus.\\nSee, they've prepared the candle and the wreath of holly and the star\\nof Bethlehem all for you. There's Sergius and Tomasso and Hulda and\\nMeeny and Hans and Yakob and Neelda and Ah Goo and Sano San and Mieze\\nand the leetla Dutch twins, Klinker and Schwillie Willie Winkum.\\nThey've all been awfully good children. And Biddy Mary and Paddy Mike\\nthey brought the candle. They're good, too.\\n\\n\\n\""
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "probs = get_probs(known_train[1], cls, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.63157413e-04,   3.63424003e-04,   1.51173057e-01,\n",
       "         6.21180572e-02,   8.52935122e-03,   6.66184200e-02,\n",
       "         2.65660105e-01,   2.51886247e-02,   6.78698750e-02,\n",
       "         5.72158433e-02,   1.61350919e-02,   4.30569115e-02,\n",
       "         3.24679580e-02,   2.72565013e-03,   5.16004113e-02,\n",
       "         1.78756989e-02,   4.89348621e-02,   8.22035010e-02])"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.1805802583148276, 5),\n",
       " (0.1231674223073551, 2),\n",
       " (0.10326031229216154, 11),\n",
       " (0.10009544884921381, 17),\n",
       " (0.092155725662713031, 6)]"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_n(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.26566010542842122, 6),\n",
       " (0.15117305689684615, 2),\n",
       " (0.082203501020270964, 17),\n",
       " (0.067869875007503841, 8),\n",
       " (0.066618420001196957, 5)]"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_n(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKogZf8MwjAry/wEmbQNohwExmAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a244f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "same = [x for i, x in enumerate(corrs) if y_train[i]]\n",
    "diff = [x for i, x in enumerate(corrs) if not y_train[i]]\n",
    "plt.scatter(range(len(same)), same)\n",
    "plt.scatter(range(len(diff)), diff)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.00029119,  0.00032402,  0.14172081,  0.05993802,  0.01564071,\n",
       "         0.22899595,  0.13888442,  0.00563165,  0.03284061,  0.14503854,\n",
       "         0.02050938,  0.0361215 ,  0.01696059,  0.00197749,  0.06005562,\n",
       "         0.0150772 ,  0.02283913,  0.05715317]),\n",
       " array([ 0.00023309,  0.00023881,  0.08496533,  0.11064029,  0.04417158,\n",
       "         0.14932728,  0.14549685,  0.01243657,  0.01788193,  0.19083817,\n",
       "         0.0389822 ,  0.04178567,  0.01830458,  0.00146381,  0.0427958 ,\n",
       "         0.01741076,  0.03006612,  0.05296117]))"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.03387574e-03,   1.01290947e-04,   7.45448280e-02,\n",
       "         1.76205639e-02,   1.96825656e-04,   6.46562669e-02,\n",
       "         7.47156421e-02,   2.13687540e-02,   2.19551088e-01,\n",
       "         4.98825978e-02,   6.54541042e-02,   1.90048351e-01,\n",
       "         2.53348556e-02,   1.12693842e-02,   6.97480964e-02,\n",
       "         9.99790938e-02,   5.18478541e-03,   9.30959658e-03])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.01881601e-05,   1.80772714e-06,   1.78689765e-01,\n",
       "         3.39453184e-02,   3.81440424e-04,   8.12189767e-02,\n",
       "         2.18874760e-01,   6.62441578e-03,   1.41613855e-01,\n",
       "         6.64314386e-02,   3.85173243e-03,   6.84535453e-02,\n",
       "         1.91148434e-02,   1.75127143e-04,   2.00912820e-02,\n",
       "         7.47344561e-03,   2.67276545e-02,   1.26320404e-01])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56847087912774141"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3]]\n",
      "[[1, 2, 3]]\n"
     ]
    }
   ],
   "source": [
    "l = [0]\n",
    "\n",
    "k1 = [1,2,3]\n",
    "l[0] = k1\n",
    "print(l)\n",
    "\n",
    "k1 = [2,3,4]\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90 samples, validate on 10 samples\n",
      "Epoch 1/5\n",
      "90/90 [==============================] - 0s - loss: 0.3065 - val_loss: 0.3375\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/5\n",
      "90/90 [==============================] - 0s - loss: 0.2725 - val_loss: 0.3373\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/5\n",
      "90/90 [==============================] - 0s - loss: 0.2562 - val_loss: 0.3404\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 4/5\n",
      "90/90 [==============================] - 0s - loss: 0.2420 - val_loss: 0.3476\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 5/5\n",
      "90/90 [==============================] - 0s - loss: 0.2317 - val_loss: 0.3474\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x10a0f9d30>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(np.array(pairs), y_train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2, 18)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(pairs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_top_n(probs, n=5):\n",
    "    return sorted([(x,i) for i,x in enumerate(probs)], reverse=True)[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.41407982712199759, 8),\n",
       " (0.13531089958886042, 5),\n",
       " (0.10319671584461411, 16),\n",
       " (0.046755954452071098, 11),\n",
       " (0.0437303946445722, 12)]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_n(pairs[8][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.36099036134327556, 6),\n",
       " (0.28744036035580495, 9),\n",
       " (0.13028813482730509, 2),\n",
       " (0.11534161736975923, 5),\n",
       " (0.047291831823309906, 3)]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_n(pairs[8][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 1, 1, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'Alfred Russel Wallace___Island Life.txt'),\n",
       " (1,\n",
       "  'Charles Darwin___The Descent of Man and Selection in Relation to Sex Volume II (1st Edition).txt'),\n",
       " (2, 'Charlotte Bronte___Jane Eyre.txt'),\n",
       " (3, 'Charlotte Mary Yonge___The Trial.txt'),\n",
       " (4, 'D H Lawrence___The Rainbow.txt'),\n",
       " (5, \"Harriet Elizabeth Beecher Stowe___Uncle Tom's Cabin.txt\"),\n",
       " (6, 'Henry Rider Haggard___Dawn.txt'),\n",
       " (7, 'James Fenimore Cooper___Mercedes of Castile.txt'),\n",
       " (8, 'Lord Byron___The Works of Lord Byron: Letters and Journals. Vol. 2.txt'),\n",
       " (9, 'Louisa May Alcott___Little Women.txt'),\n",
       " (10, 'Mark Twain___Following the Equator, Complete.txt'),\n",
       " (11,\n",
       "  'Robert Louis Stevenson___The Works of Robert Louis Stevenson - Swanston Edition, Volume 25.txt'),\n",
       " (12, 'Sir Walter Scott___Guy Mannering.txt'),\n",
       " (13, 'Thomas Henry Huxley___Essays Upon Some Controverted Questions.txt'),\n",
       " (14, 'Wilkie Collins___The Moonstone.txt'),\n",
       " (15, 'William Dean Howells___Literature and Life.txt'),\n",
       " (16, 'William Makepeace Thackeray___The History of Henry Esmond, Esq.txt'),\n",
       " (17, 'Winston Churchill___Richard Carvel, Complete.txt')]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(i,x) for i,x in enumerate(cls.classes_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "k8 = ' '.join([x.strip() for x in known_train[8].split(\"\\n\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "u8 = ' '.join([x.strip() for x in unknown_train[8].split(\"\\n\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "k8p = get_probs(k8, cls, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "u8p = get_probs(u8, cls, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.41464804931744725, 8),\n",
       " (0.13512633722780415, 5),\n",
       " (0.1030241886305906, 16),\n",
       " (0.046696272055147647, 11),\n",
       " (0.043760428939142546, 12)]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_n(k8p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.36076764549530133, 6),\n",
       " (0.28798015720555897, 9),\n",
       " (0.13002743772955924, 2),\n",
       " (0.11548080453615225, 5),\n",
       " (0.04716723967697755, 3)]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_n(u8p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
