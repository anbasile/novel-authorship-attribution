{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import xmltodict\n",
    "import logging\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "\n",
    "# Main dataset loading utility\n",
    "class PanDataLoader:\n",
    "    \n",
    "    def __init__(self, logger=None):\n",
    "        if logger is None:\n",
    "            logging.basicConfig(level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "            self.log = logging.getLogger(__name__)\n",
    "        else:\n",
    "            self.log = logger\n",
    "                               \n",
    "    def load_17(self, directory):\n",
    "            \n",
    "        \"\"\"Load and return the pan17 gender and variation twitter dataset.\n",
    "        ==============                                      ==============\n",
    "        Samples total                                                10800\n",
    "        Targets            nominal [{male, female},\n",
    "                                    {ar, pt, es, en},\n",
    "                                    {'brazil', 'australia', 'venezuela',\n",
    "                                     'portugal', 'great britain', 'chile',\n",
    "                                     'levantine', 'egypt', 'colombia',\n",
    "                                     'peru', 'ireland', 'argentina',\n",
    "                                     'maghrebi', 'mexico', 'new zealand',\n",
    "                                     'spain', 'canada', 'gulf'}]\n",
    "        ==============                                      ==============\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputdir\n",
    "        The directory containing the training data, i.e. /data/training.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        data : Pandas dataframe\n",
    "            The interesting attributes are:\n",
    "            'text', the data to learn, ['gender','lang', variety],\n",
    "            the regression targets,\n",
    "        Examples\n",
    "        --------\n",
    "        >>> from datasets import load_pan17\n",
    "        >>> df_training = load_pan17(inputdir)\n",
    "        >>> print(df_training.corpus.shape)\n",
    "        (10800, 5)\n",
    "        \"\"\"\n",
    "\n",
    "        X_docs = glob.glob(os.path.join(directory, '*.xml'), recursive=True)\n",
    "        Y_doc = os.path.join(directory, 'truth.txt')\n",
    "        # check that the dataset is loaded correctly\n",
    "\n",
    "        X_tmp = []\n",
    "        for t in X_docs:\n",
    "            with open(t) as f:\n",
    "                doc = xmltodict.parse(f.read())\n",
    "            author = os.path.splitext(os.path.basename(t))[0]\n",
    "            lang = doc['author']['@lang']\n",
    "            text = doc['author']['documents']['document']\n",
    "            X_tmp.append((author, lang, text))\n",
    "\n",
    "        text = pd.DataFrame(X_tmp, columns=[\"author\", \"lang\", \"text\"])\n",
    "\n",
    "        Y_tmp = pd.read_csv(Y_doc,\n",
    "                             sep='\\:\\:\\:',\n",
    "                             names=['author', 'gender', 'variety'],\n",
    "                             engine='python')\n",
    "\n",
    "        corpus = pd.merge(text, Y_tmp, on='author')\n",
    "        return corpus\n",
    "    \n",
    "    def load_16(self, directory):\n",
    "        return self.load_14(directory)\n",
    "    \n",
    "    def load_15(self, directory):\n",
    "        X_docs = glob.glob(os.path.join(directory, '*.xml'), recursive=True)\n",
    "        Y_doc = os.path.join(directory, 'truth.txt')\n",
    "        X_tmp = []\n",
    "        for t in X_docs:\n",
    "            with open(t) as f:\n",
    "                doc = xmltodict.parse(f.read())\n",
    "            author = os.path.splitext(os.path.basename(t))[0]\n",
    "            lang = doc['author']['@lang']\n",
    "            text = doc['author']['document']\n",
    "            # print(author, lang, text[:100])\n",
    "            X_tmp.append((author, lang, text))\n",
    "\n",
    "        text = pd.DataFrame(X_tmp, columns=[\"author\", \"lang\", \"text\"])\n",
    "\n",
    "        Y_tmp = pd.read_csv(Y_doc,\n",
    "                             sep='\\:\\:\\:',\n",
    "                             names=['author', 'gender', 'age', '1','2','3','4', '5'],\n",
    "                             engine='python') \n",
    "\n",
    "\n",
    "        corpus = pd.merge(text, Y_tmp, on='author')\n",
    "        return corpus\n",
    "    \n",
    "    \n",
    "    def load_14(self, directory):\n",
    "        errors = 0\n",
    "        X_docs = glob.glob(os.path.join(directory, '*.xml'), recursive=True)\n",
    "        Y_doc = os.path.join(directory, 'truth.txt')\n",
    "        X_tmp = []\n",
    "        for t in X_docs:\n",
    "            with open(t) as f:\n",
    "                try:\n",
    "                    doc = xmltodict.parse(f.read())\n",
    "                except Exception as e:\n",
    "                    self.log.warning(e)\n",
    "                    self.log.warning(\"Skipping: {}\".format(t))\n",
    "                    continue\n",
    "            author = os.path.splitext(os.path.basename(t))[0]\n",
    "            lang = doc['author']['@lang']\n",
    "            text = []\n",
    "            for td in doc['author']['documents']['document']:\n",
    "                try:\n",
    "                    t = BeautifulSoup(td['#text'], \"lxml\").getText()\n",
    "                    text.append(t)\n",
    "                except Exception as e:\n",
    "                    errors += 1\n",
    "                    # log.warning(e)\n",
    "                    # self.log.warning(\"skipping {}\".format(td))\n",
    "                    continue\n",
    "            X_tmp.append((author, lang, text))\n",
    "\n",
    "        text = pd.DataFrame(X_tmp, columns=[\"author\", \"lang\", \"text\"])\n",
    "\n",
    "        Y_tmp = pd.read_csv(Y_doc,\n",
    "                             sep='\\:\\:\\:',\n",
    "                             names=['author', 'gender', 'age'],\n",
    "                             engine='python') \n",
    "\n",
    "        self.log.warning(\"Skipped {}\".format(errors))\n",
    "\n",
    "        corpus = pd.merge(text, Y_tmp, on='author')\n",
    "        return corpus\n",
    "    \n",
    "    def _load_all(self, loader_func, directories):\n",
    "        \"\"\"Concatenate across languages\"\"\"\n",
    "        corpora = []\n",
    "        for dr in directories:\n",
    "            corpus = loader_func(dr)\n",
    "            corpora.append(corpus)\n",
    "        return pd.concat(corpora)\n",
    "    \n",
    "    def load_all_17(self, directories):\n",
    "        return self._load_all(self.load_17, directories)\n",
    "    \n",
    "    def load_all_16(self, directories):\n",
    "        return self._load_all(self.load_16, directories)\n",
    "    \n",
    "    def load_all_15(self, directories):\n",
    "        return self._load_all(self.load_15, directories)\n",
    "    \n",
    "    def load_all_14(self, directories):\n",
    "        return self._load_all(self.load_14, directories)\n",
    "    \n",
    "    def clean_and_normalize(self, corpus):\n",
    "        \"\"\"Standardize to lowercase for gender and langauge, m/f for gender\n",
    "           Remove personality scores\"\"\"\n",
    "        # FIXME TODO -- how do you do this in place?\n",
    "        # FIXME TODO -- normalize age ranges?\n",
    "        corpus['gender'] = corpus['gender'].apply(lambda s: s[0].lower())\n",
    "        corpus['lang'] = corpus['lang'].apply(lambda s: s.lower())\n",
    "\n",
    "        for c in ['1', '2', '3', '4', '5']:\n",
    "            if c in corpus:\n",
    "                del corpus[c]\n",
    "        return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pdl = PanDataLoader()\n",
    "\n",
    "gender_data = pdl.load_17(\"/data/pan17/pan17-author-profiling-training-dataset-2017-03-10/en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "males = gender_data[(gender_data[\"gender\"]==\"male\") & (gender_data[\"lang\"]==\"en\")]\n",
    "females = gender_data[(gender_data[\"gender\"]==\"female\") & (gender_data[\"lang\"]==\"en\")]\n",
    "\n",
    "mtexts = [\" \" .join(males.iloc[i].text) for i in range(len(males))]\n",
    "ftexts = [\" \" .join(females.iloc[i].text) for i in range(len(females))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tokenizer' object has no attribute 'tokenize'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-06f4b10f3eec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmtok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tokenizer' object has no attribute 'tokenize'"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "mtok = tokenizer.tokenize(mtexts)\n",
    "\n",
    "\n",
    "max_review_length = 5000\n",
    "mX = sequence.pad_sequences(mtok, maxlen=max_review_length)\n",
    "fX = sequence.pad_sequences(ftok, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1800"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pairs = []\n",
    "labels = []\n",
    "i = 0\n",
    "while i < len(mX):\n",
    "    pairs.append([mX[i], mX[i+1]])\n",
    "    pairs.append([fX[i], fX[i+1]])\n",
    "    pairs.append([mX[i], fX[i]])\n",
    "    pairs.append([mX[i+1], fX[i+1]])\n",
    "    i += 2\n",
    "labels = [1,1,0,0] * (int(len(mX)/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(pairs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    9   132  2151    15   151    56    35    24   219     4   189    19\n",
      "    16    44     8  1008 42712 21060 25282    22   453  4943   535     5\n",
      "  2615    17    53    96  1855    39   263  2299   610   331   102 31659\n",
      "   134  3778  2954  8072     9  4066   302     4   369     9     4    85\n",
      "   538   112     5     4  1140   521     7   134  1872    18  2050  3684\n",
      "   286  5455    50    13    96    20   237     6  7238    67    32   112\n",
      "     4   144   359     4   805   868    14  7202   107   150 17183    46\n",
      "   240    88     5 32392    15    57   569   108     4   189   224    17\n",
      "    12     4   948   102]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[500][0][-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "feats.fit(ftexts + mtexts, [0] * len(ftexts) + [1] * len(mtexts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mX = feats.transform(mtexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fX = feats.transform(ftexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mX = mX.todense()\n",
    "fX = fX.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 378307)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fX2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pairs = []\n",
    "labels = []\n",
    "i = 0\n",
    "while i < mX.shape[0]:\n",
    "    pairs.append([mX[i], mX[i+1]])\n",
    "    pairs.append([fX[i], fX[i+1]])\n",
    "    pairs.append([mX[i], fX[i]])\n",
    "    pairs.append([mX[i+1], fX[i+1]])\n",
    "    i += 2\n",
    "labels = [1,1,0,0] * (int(mX.shape[0]/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7a27d3ed77df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tr_pairs, te_pairs, tr_y, te_y = train_test_split(pairs, labels, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "import random\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input, Lambda, Embedding, LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))\n",
    "\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    margin = 1\n",
    "    return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))\n",
    "\n",
    "\n",
    "def create_pairs(x, digit_indices):\n",
    "    '''Positive and negative pair creation.\n",
    "    Alternates between positive and negative pairs.\n",
    "    '''\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    n = min([len(digit_indices[d]) for d in range(10)]) - 1\n",
    "    for d in range(10):\n",
    "        for i in range(n):\n",
    "            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]\n",
    "            pairs += [[x[z1], x[z2]]]\n",
    "            inc = random.randrange(1, 10)\n",
    "            dn = (d + inc) % 10\n",
    "            z1, z2 = digit_indices[d][i], digit_indices[dn][i]\n",
    "            pairs += [[x[z1], x[z2]]]\n",
    "            labels += [1, 0]\n",
    "    return np.array(pairs), np.array(labels)\n",
    "\n",
    "\n",
    "def create_base_network(input_dim):\n",
    "    embedding_vecor_length = 32\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(100000, embedding_vecor_length, input_length=5000))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(128))\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_base_network_old(input_dim):\n",
    "    '''Base network to be shared (eq. to feature extraction).\n",
    "    '''\n",
    "    seq = Sequential()\n",
    "    seq.add(Dense(128, input_shape=(input_dim,), activation='relu'))\n",
    "    seq.add(Dense(128, activation='relu'))\n",
    "    # seq.add(Dropout(0.1))\n",
    "    # seq.add(Dense(512, activation='relu'))\n",
    "    # seq.add(Dense(512, activation='relu'))\n",
    "    # seq.add(Dense(512, activation='relu'))\n",
    "    # seq.add(Dropout(0.1))\n",
    "    # seq.add(Dense(512, activation='relu'))\n",
    "    return seq\n",
    "\n",
    "\n",
    "def compute_accuracy(predictions, labels):\n",
    "    return np.mean(np.equal(predictions.ravel() < 0.5, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f90f91fb16f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtr_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mte_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtr_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mte_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "tr_pairs = np.array(X_train)\n",
    "te_pairs = np.array(X_test)\n",
    "tr_y = np.array(y_train)\n",
    "te_y = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 2, 5000)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te_pairs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tr_pairs_s = tr_pairs.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 3600 into shape (3600,2,378307)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-7afa239055cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtr_pairs_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 3600 into shape (3600,2,378307)"
     ]
    }
   ],
   "source": [
    "tr_pairs_s.reshape(3600, 2, mX.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "mX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tr_pairs = tr_pairs.reshape(1800, 2, mX.shape[1])\n",
    "te_pairs = te_pairs.reshape(mX.shape[0], 2, mX.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-9d2f5e260ddb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_dim = tr_pairs.shape[-1]\n",
    "\n",
    "# network definition\n",
    "base_network = create_base_network(input_dim)\n",
    "\n",
    "input_a = Input(shape=(input_dim,))\n",
    "input_b = Input(shape=(input_dim,))\n",
    "\n",
    "# because we re-use the same instance `base_network`,\n",
    "# the weights of the network\n",
    "# will be shared across the two branches\n",
    "processed_a = base_network(input_a)\n",
    "processed_b = base_network(input_b)\n",
    "\n",
    "distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n",
    "\n",
    "model = Model(input=[input_a, input_b], output=distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 269 samples, validate on 2431 samples\n",
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "\n",
    "'''tr_pairs1 = tr_pairs[:250]\n",
    "te_pairs1 = tr_pairs[250:]\n",
    "tr_y1 = tr_y[:250]\n",
    "te_y1 = tr_y[250:]\n",
    "\n",
    "tr_pairs = tr_pairs1\n",
    "te_pairs = te_pairs1\n",
    "tr_y = tr_y1\n",
    "te_y = te_y1\n",
    "'''\n",
    "# tr_pairs = np.vstack([tr_pairs, te_pairs])\n",
    "# print(tr_pairs.shape)\n",
    "\n",
    "rms = RMSprop()\n",
    "model.compile(loss=contrastive_loss, optimizer='adam', metrics=['acc'])\n",
    "model.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,\n",
    "          validation_split=0.9,\n",
    "          batch_size=240,\n",
    "          epochs=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Accuracy on training set: 50.17%\n",
      "* Accuracy on test set: 49.83%\n"
     ]
    }
   ],
   "source": [
    "# compute final accuracy on training and test sets\n",
    "pred = model.predict([tr_pairs[:, 0], tr_pairs[:, 1]])\n",
    "tr_acc = compute_accuracy(pred, tr_y)\n",
    "pred = model.predict([te_pairs[:, 0], te_pairs[:, 1]])\n",
    "te_acc = compute_accuracy(pred, te_y)\n",
    "\n",
    "print('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))\n",
    "print('* Accuracy on test set: %0.2f%%' % (100 * te_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.387333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(te_y, [1 if x < 0.5 else 0 for x in pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 2, 1200)"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack([tr_pairs, te_pairs]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 2, 1200)"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_pairs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 1200)"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te_pairs[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "       1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "       1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack([tr_y,te_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
