{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Todo\n",
    "\n",
    "* Find most unusual features (that appear at least twice in text). Ignore if they are not unusual for texts of that *topic*\n",
    "* Give zscores of known and unknown texts to siamese network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "# third-party imports\n",
    "import textacy\n",
    "from spacy.en import English\n",
    "from statistics import mean, stdev\n",
    "\n",
    "def _normalize_counter(counter, c):\n",
    "    \"\"\"Divide all the values in a Counter by a constant and remove padding\"\"\"\n",
    "    for key in counter:\n",
    "        counter[key] = (counter[key] - 1) / c\n",
    "    return counter\n",
    "\n",
    "class TextAnalyser:\n",
    "    def __init__(self, nlp=None):\n",
    "        if nlp:\n",
    "            self.nlp = nlp\n",
    "        else:\n",
    "            self.nlp = English()\n",
    "            \n",
    "        # alphabet for letter ratios\n",
    "        self.alphabet = string.ascii_lowercase + \"!?:;,.'- \"\n",
    "        \n",
    "        # keys that we care about from textacy.stats\n",
    "        self.basic_keys = ['n_long_words', 'n_monosyllable_words', 'n_polysyllable_words', 'n_sents', 'n_syllables', 'n_unique_words', 'n_words']\n",
    "        \n",
    "        # keys that we care about for textacy readability stats\n",
    "        self.readability_keys = ['automated_readability_index','coleman_liau_index', 'flesch_kincaid_grade_level',\n",
    "                                 'flesch_readability_ease', 'gulpease_index', 'gunning_fog_index', 'lix',\n",
    "                                 'wiener_sachtextformel']\n",
    "        \n",
    "        # parts of speech that we care about from spacy (pos_ not tag_)\n",
    "        self.pos_keys = ['ADJ', 'ADP', 'ADV', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SPACE', 'SYM', 'VERB', 'X']\n",
    "        self.pos_keys_set = set(self.pos_keys)\n",
    "\n",
    "    def get_named_features(self, text):\n",
    "        # TODO: Add bigrams, trigrams?\n",
    "        processed = self.nlp(text, entity=False, tag=True, parse=True)\n",
    "        stats = textacy.text_stats.TextStats(processed)\n",
    "        basic_stats = stats.basic_counts\n",
    "        readability_stats = stats.readability_stats\n",
    "        cleaned_text = ''.join(filter(lambda x: x in self.alphabet, text.lower() + self.alphabet))\n",
    "        \n",
    "        stats_ratios = {key: (basic_stats[key] / len(text)) for key in self.basic_keys}\n",
    "        readability_ratios = {key: (readability_stats[key] / len(text)) for key in self.readability_keys}\n",
    "        stats_ratios.update(readability_ratios)\n",
    "\n",
    "        # get only the characters we care about \n",
    "        # append alphabet so that each character artificially appears once\n",
    "        char_ratios = Counter(cleaned_text)\n",
    "        char_ratios = _normalize_counter(char_ratios, len(text))\n",
    "\n",
    "        # calculate pos ratios\n",
    "        tags = [word.pos_ for word in processed if word.pos_ in self.pos_keys_set] + self.pos_keys\n",
    "        pos_ratios = Counter(tags)\n",
    "        pos_ratios = _normalize_counter(pos_ratios, len(processed)) # normalize by word length\n",
    "\n",
    "        res = stats_ratios\n",
    "        res.update(char_ratios)\n",
    "        res.update(pos_ratios)\n",
    "        return [(key, res[key]) for key in sorted(res)]\n",
    "    \n",
    "    def calculate_mean_and_std(self, extracted_texts):\n",
    "        \"\"\"finds unusual patterns by calculating mean and std deviation for a list of \n",
    "           extracted features and sorting by z-score\"\"\"\n",
    "        means = []\n",
    "        stds = []\n",
    "        sample = extracted_texts[0]  # get one text for feature size and names\n",
    "        num_features = len(sample)\n",
    "        # fi = feature index\n",
    "        for fi in range(num_features):\n",
    "            u = mean([stat[fi][1] for stat in extracted_texts])\n",
    "            o = stdev([stat[fi][1] for stat in extracted_texts])\n",
    "            means.append((sample[fi][0], u))\n",
    "            stds.append((sample[fi][0], o))\n",
    "        return means, stds\n",
    "    \n",
    "    def calculate_z_scores(self, extracted_text, means, stds):\n",
    "        \"\"\"Calculate the zscores for each features of a single text (extractions)\"\"\"\n",
    "        # z = (X - μ) / σ\n",
    "        zscores = []\n",
    "        num_features = len(extracted_text)\n",
    "        for fi in range(num_features):\n",
    "            try:\n",
    "                zscore = (extracted_text[fi][1] - means[fi][1]) / stds[fi][1]\n",
    "            except ZeroDivisionError:\n",
    "                zscore = 0\n",
    "            zscores.append((zscore, (text_features[fi][0])))\n",
    "        return zscores\n",
    "        \n",
    "def vectorize(str_text):\n",
    "    vecs = _vectorize(str_text)\n",
    "    return np.array([x[1] for x in vecs])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:SMOG score may be unreliable for n_sents < 30\n",
      "WARNING:root:SMOG score may be unreliable for n_sents < 30\n",
      "WARNING:root:SMOG score may be unreliable for n_sents < 30\n",
      "WARNING:root:SMOG score may be unreliable for n_sents < 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-1.4856533708302617, 'n_unique_words'), (-1.4255352930537208, 'o'), (-1.4016054795770003, 't'), (-1.3339880709789862, 'b'), (-1.3339880709789862, 'm'), (-1.3339880709789862, 'x'), (-1.2764444150951448, 'NOUN'), (-0.9919397397762781, 'flesch_readability_ease'), (-0.8589556903873334, 'DET'), (-0.8589556903873334, 'VERB'), (-0.8584192016190515, 'h'), (-0.7961318807627012, 'i'), (-0.7833494518006403, 'ADV'), (-0.7597124593094543, 'e'), (-0.7597124593094543, 'g'), (-0.7597124593094543, 'n_sents'), (-0.7080152423090047, 'gulpease_index'), (-0.6301260378126047, 'ADJ'), (-0.6101146782287797, 's'), (-0.5848909506882148, 'r'), (-0.5419025503979429, 'n_words'), (-0.5, 'l'), (0, '!'), (0, \"'\"), (0, ','), (0, '-'), (0, '.'), (0, ':'), (0, ';'), (0, '?'), (0, 'ADP'), (0, 'INTJ'), (0, 'NUM'), (0, 'PART'), (0, 'PRON'), (0, 'PROPN'), (0, 'PUNCT'), (0, 'SPACE'), (0, 'SYM'), (0, 'X'), (0, 'c'), (0, 'f'), (0, 'j'), (0, 'k'), (0, 'p'), (0, 'q'), (0, 'u'), (0, 'v'), (0, 'w'), (0, 'y'), (0, 'z'), (0.2247133458749715, 'automated_readability_index'), (0.23138048159602484, ' '), (0.6914599313460844, 'n_monosyllable_words'), (1.188991762679289, 'n_syllables'), (1.211540861116164, 'flesch_kincaid_grade_level'), (1.3419930333067465, 'coleman_liau_index'), (1.4127704580924036, 'wiener_sachtextformel'), (1.465873556268028, 'a'), (1.4870381696522048, 'n'), (1.4990379707317283, 'gunning_fog_index'), (1.4990379707317283, 'lix'), (1.5, 'CCONJ'), (1.5, 'd'), (1.5, 'n_long_words'), (1.5, 'n_polysyllable_words')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "ta = TextAnalyser(nlp)\n",
    "texts = [\"this is some boring text\", \"this is also some boring text\", \"more boring text\", \"and and and and and interesting\"]\n",
    "stats = [ta.get_named_features(text) for text in texts]\n",
    "means, stds = ta.calculate_mean_and_std(stats)\n",
    "zscores = ta.calculate_z_scores(stats[-1], means, stds)\n",
    "print(sorted(zscores))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
