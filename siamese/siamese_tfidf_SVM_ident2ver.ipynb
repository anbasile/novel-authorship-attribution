{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Siamese Networks for Authorship Verification\n",
    "\n",
    "Siamese Networks were first used decades ago for Signature Verification, and with the new interest in neural networks and deep learning, they are more recently being used for all kinds of verification tasks. Instead of teaching the network to recognise examples of a specific class by giving it lots of labeled examples of that specific class, you instead have it learn a distance function between pairs of examples. It learns to tell if two examples are from the same class or from different classes.\n",
    "\n",
    "For example, image re-identification is used for automatic access control. A single photo of a Alice as a reference or ID photo. When Alice wants to access the building, a new photo is taken at the door and compared to her reference photo. The door opens if the algorithm detects a match. With a simple classification neural net, we'd need to show the net many examples of Alice, in different poses, wearing different clothing, with her face at different angles. A siamese network can detect Alice even if has never seen her before. Instead of learning what Alice looks like, it learns to tell whether to photos are of the same person by learning a distance function between pairs of photos.\n",
    "\n",
    "We show here how a Siamese Network can be used for authorship verification -- given two texts, it predicts whether or not they are written by the same author, even if has never seen other texts by that author.\n",
    "\n",
    "Most of the code for the Siamese Network comes from here: https://github.com/fchollet/keras/blob/master/examples/mnist_siamese_graph.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "import random\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input, Lambda\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# paths to PAN datasets, available from http://pan.webis.de/clef15/pan15-web/author-identification.html\n",
    "# and http://pan.webis.de/clef14/pan14-web/author-identification.html\n",
    "pan15train = \"/data/panstuffs/pan15-authorship-verification-training-dataset-english-2015-04-19/\"\n",
    "pan15test = \"/data/panstuffs/pan15-authorship-verification-test-dataset2-english-2015-04-19/\"\n",
    "pan14train = \"/data/panstuffs/pan14-author-verification-training-corpus-english-novels-2014-04-22/\"\n",
    "pan14test = \"/data/panstuffs/pan14-author-verification-test-corpus2-english-novels-2014-04-22/\"\n",
    "pan14train_e = \"/data/panstuffs/pan14-author-verification-training-corpus-english-essays-2014-04-22/\"\n",
    "pan14test_e = \"/data/panstuffs/pan14-author-verification-test-corpus2-english-essays-2014-04-22/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def read_file(filepath):\n",
    "    with open(filepath) as f:\n",
    "        s = f.read()\n",
    "    return s\n",
    "\n",
    "def load_pan_data(directory, prefix=\"E\"):\n",
    "    \"\"\"Load known and unknown texts in the PAN data format\"\"\"\n",
    "    # FIXME: assumes one known file per author, which is fine for English datasets only\n",
    "    authors = sorted([x for x in os.listdir(directory) if x.startswith(prefix)])\n",
    "    known_texts = []\n",
    "    unknown_texts = []\n",
    "    for author in authors:\n",
    "        kf = os.path.join(directory, author, \"known01.txt\")\n",
    "        uf = os.path.join(directory, author, \"unknown.txt\")\n",
    "        known_texts.append(read_file(kf))\n",
    "        unknown_texts.append(read_file(uf))\n",
    "        \n",
    "    truthfile = os.path.join(directory, \"truth.txt\")\n",
    "    with open(truthfile) as f:\n",
    "        lines = f.read().strip().split(\"\\n\")\n",
    "    y = [1 if line.split()[1] == \"Y\" else 0 for line in lines]\n",
    "    y = np.array(y)\n",
    "    return known_texts, unknown_texts, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tr_known, tr_unknown, tr_labels = load_pan_data(pan15train)\n",
    "te_known ,te_unknown, te_labels = load_pan_data(pan15test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.pipeline import FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "svm = SVC(kernel='linear', probability=True)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "svm = OneVsRestClassifier(SVC(kernel='linear', probability=True, C=5))\n",
    "svm = MultinomialNB(alpha=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 500 500\n"
     ]
    }
   ],
   "source": [
    "tr_known, te_known = te_known, tr_known\n",
    "tr_labels, te_labels = te_labels, tr_labels\n",
    "tr_unknown, te_unknown = te_unknown, tr_unknown\n",
    "print(len(tr_known), len(tr_labels), len(tr_unknown))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cvec = TfidfVectorizer(analyzer='char', ngram_range=(2,5), min_df=0.1, lowercase=False, binary=True, sublinear_tf=True, use_idf=True)\n",
    "wvec = TfidfVectorizer(ngram_range=(1,3), min_df=0.01, lowercase=False, binary=True, sublinear_tf=True, use_idf=True)\n",
    "vec = FeatureUnion([\n",
    "    ('word', wvec),\n",
    "    ('char', cvec)\n",
    "])\n",
    "vec.fit(te_known + te_unknown)\n",
    "\n",
    "trk_vecs = vec.transform(te_known)\n",
    "tru_vecs = vec.transform(te_unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.0001, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.fit(trk_vecs, list(range(len(te_known))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "preds = svm.predict_proba(tru_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def in_top_half(predictions, index):\n",
    "    return index in [x[1] for x in sorted([(v,i) for i, v in enumerate(preds[index])], reverse=True)][:int(len(preds[0])/2)]\n",
    "\n",
    "bin_preds = [1 if in_top_half(preds, i) else 0 for i in range(len(preds))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60999999999999999"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(te_labels, bin_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
