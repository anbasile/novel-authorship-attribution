{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Activation, Embedding\n",
    "from keras.layers import LSTM, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import string\n",
    "\n",
    "embeddings_path = \"glove.840B.300d-char.txt\"\n",
    "embedding_dim = 300\n",
    "batch_size = 128\n",
    "lr = 0.001\n",
    "lr_decay = 1e-4\n",
    "maxlen = 10\n",
    "step = 3\n",
    "\n",
    "\n",
    "chars = \"sorted(list(set(string.ascii_lowercase + string.punctuation + \" \")))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "\n",
    "def vectorize(text):\n",
    "    text = text.lower()\n",
    "    sequences = []\n",
    "    next_chars = []\n",
    "    for i in range(0, len(text) - maxlen, step):\n",
    "        sequences.append(text[i: i + maxlen])\n",
    "    print(sequences)\n",
    "    \n",
    "    X = np.zeros((len(sequences), maxlen), dtype=np.int)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        for t, char in enumerate(seq):\n",
    "            try:\n",
    "                X[i, t] = char_indices[char]\n",
    "    return X\n",
    "\n",
    "def get_embeddings_matrix(embeddings_path):\n",
    "    embedding_vectors = {}\n",
    "    with open(embeddings_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line_split = line.strip().split(\" \")\n",
    "            vec = np.array(line_split[1:], dtype=float)\n",
    "            char = line_split[0]\n",
    "            embedding_vectors[char] = vec\n",
    "\n",
    "    embedding_matrix = np.zeros((len(chars), 300))\n",
    "    for char, i in char_indices.items():\n",
    "        embedding_vector = embedding_vectors.get(char)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embeddings_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = vectorize(\"This is a dog and a cat and it's longer than 100 charactrs. This means we'll have at least two sequdnces I think. HOw long should the text be anyway??? Do we need preprocessing??\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
