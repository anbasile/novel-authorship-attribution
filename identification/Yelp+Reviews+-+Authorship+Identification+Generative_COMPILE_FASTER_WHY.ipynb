{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.8 s, sys: 4.75 s, total: 44.5 s\n",
      "Wall time: 44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import json\n",
    "reviews = []\n",
    "with open(\"yelp_academic_dataset_review.json\") as f:\n",
    "    for line in f:\n",
    "        reviews.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stars': 5, 'funny': 0, 'useful': 0, 'review_id': 'NxL8SIC5yqOdnlXCg18IBg', 'business_id': '2aFiy99vNLklCx3T_tGS9A', 'user_id': 'KpkOkG6RIf4Ra25Lhhxf1A', 'type': 'review', 'text': \"If you enjoy service by someone who is as competent as he is personable, I would recommend Corey Kaplan highly. The time he has spent here has been very productive and working with him educational and enjoyable. I hope not to need him again (though this is highly unlikely) but knowing he is there if I do is very nice. By the way, I'm not from El Centro, CA. but Scottsdale, AZ.\", 'date': '2011-10-10', 'cool': 0}\n"
     ]
    }
   ],
   "source": [
    "print(reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "prolific_reviewers = Counter([review['user_id'] for review in reviews]).most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_ids = {pr[0] : 0 for pr in prolific_reviewers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "by_author = {} # author : \"review 1\\n review 2\\n review 3\"\n",
    "for review in reviews:\n",
    "    uid = review['user_id']\n",
    "    if uid in keep_ids:\n",
    "        uid = review['user_id']\n",
    "        if uid in by_author:\n",
    "            by_author[uid] += \"\\n{}\".format(review['text'])\n",
    "        else:\n",
    "            by_author[uid] = \"{}\".format(review['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(by_author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(276136, 'ffPY_bHX8vLebHu8LBEqfg'),\n",
       " (278427, 'PeLGa5vUR8_mcsn-fn42Jg'),\n",
       " (351311, 'cMEtAiW60I5wE_vLfTxoJQ'),\n",
       " (370129, 'iDlkZO2iILS8Jwfdy7DP9A'),\n",
       " (461333, 'dt9IHwfuZs9D9LOH7gjNew')]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that we have at least 200000 characters for each author\n",
    "sorted([(len(by_author[key]), key) for key in by_author])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_chunks(l, n):\n",
    "    n = max(1, n)\n",
    "    return [l[i:i+n] for i in range(0, len(l), n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_texts = []  # the first 100 000 chars from each author\n",
    "train_labels = [] # each author\n",
    "test_texts = []   # 100 texts of 1000 characters each (second 100 000 chars of each author)\n",
    "test_labels = []  # each author * 100\n",
    "\n",
    "author_int = {author: i for i,author in enumerate(by_author)}\n",
    "int_author = {author_int[author]: author for author in author_int}\n",
    "\n",
    "for author in by_author:\n",
    "    train_text = by_author[author][:50000]\n",
    "    train_label = author_int[author]\n",
    "    train_texts.append(train_text)\n",
    "    train_labels.append(train_label)\n",
    "    \n",
    "    short_texts = get_chunks(by_author[author][50000:100000], 1000)\n",
    "    for text in short_texts:\n",
    "        test_texts.append(text)\n",
    "        test_labels.append(author_int[author])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 1500\n"
     ]
    }
   ],
   "source": [
    "print(len(train_texts), len(test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vectorization - chars to ints\n",
    "import string\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    \"\"\"Sample predictions from a probability array\"\"\"\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds + 1e-6) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def generate(model, diversity=0.5, text=\"\"):\n",
    "    \"\"\"Generate text from a model\"\"\"\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    for i in range(5000):\n",
    "        x = np.zeros((1, maxlen), dtype=np.int)\n",
    "        for t, char in enumerate(sentence):\n",
    "            try:\n",
    "                x[0, t] = char_indices[char]\n",
    "            except:\n",
    "                print(sentence)\n",
    "        preds = model.predict(x, verbose=0)[0][0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    return\n",
    "\n",
    "def vectorize(text):\n",
    "    \"\"\"Convert text into character sequences\"\"\"\n",
    "    step = 3\n",
    "    sentences = []\n",
    "    next_chars = []\n",
    "    for i in range(0, len(text) - maxlen, step):\n",
    "        sentences.append(text[i: i + maxlen])\n",
    "        next_chars.append(text[i + maxlen])\n",
    "    X = np.zeros((len(sentences), maxlen), dtype=np.int)\n",
    "    y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for t, char in enumerate(sentence):\n",
    "            X[i, t] = char_indices[char]\n",
    "        y[i, char_indices[next_chars[i]]] = 1\n",
    "    return X, y\n",
    "\n",
    "def clean_text(text, charset):\n",
    "    text = \" \".join(text.split())  # all white space is one space\n",
    "    text = \"\".join([x for x in text if x in charset])  # remove characters that we don't care about\n",
    "    return text\n",
    "\n",
    "def get_model(modelfile, freeze=False):\n",
    "    model = load_model(modelfile)\n",
    "    if freeze:\n",
    "        for layer in model.layers[:6]:\n",
    "            layer.trainable = False\n",
    "    return model\n",
    "\n",
    "chars = \" \" + string.ascii_letters + string.punctuation  # sorted to keep indices consistent\n",
    "charset = set(chars)  # for lookup\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "maxlen = 100  # must match length which generated model - the sequence length\n",
    "\n",
    "# load a pretrained language model\n",
    "modelfile = \"charlm2/model_middlemarch_cnn.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test train an author specific model\n",
    "test_author_text = clean_text(train_texts[0], charset)\n",
    "test_author_model = get_model(modelfile, freeze=True)\n",
    "X, y = vectorize(test_author_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14929 samples, validate on 1659 samples\n",
      "Epoch 1/10\n",
      "14929/14929 [==============================] - 24s - loss: 2.4266 - main_out_loss: 1.9901 - aux_out_loss: 2.1820 - val_loss: 2.1216 - val_main_out_loss: 1.7226 - val_aux_out_loss: 1.9947\n",
      "Epoch 2/10\n",
      "14929/14929 [==============================] - 10s - loss: 1.9163 - main_out_loss: 1.4955 - aux_out_loss: 2.1040 - val_loss: 2.0452 - val_main_out_loss: 1.6527 - val_aux_out_loss: 1.9626\n",
      "Epoch 3/10\n",
      "14929/14929 [==============================] - 10s - loss: 1.6984 - main_out_loss: 1.2871 - aux_out_loss: 2.0563 - val_loss: 2.0439 - val_main_out_loss: 1.6540 - val_aux_out_loss: 1.9493\n",
      "Epoch 4/10\n",
      "14929/14929 [==============================] - 10s - loss: 1.5484 - main_out_loss: 1.1430 - aux_out_loss: 2.0268 - val_loss: 2.0482 - val_main_out_loss: 1.6605 - val_aux_out_loss: 1.9385\n",
      "Epoch 5/10\n",
      "14929/14929 [==============================] - 10s - loss: 1.4167 - main_out_loss: 1.0163 - aux_out_loss: 2.0018 - val_loss: 2.0876 - val_main_out_loss: 1.7009 - val_aux_out_loss: 1.9334\n",
      "Epoch 6/10\n",
      "14929/14929 [==============================] - 10s - loss: 1.3073 - main_out_loss: 0.9119 - aux_out_loss: 1.9769 - val_loss: 2.1351 - val_main_out_loss: 1.7493 - val_aux_out_loss: 1.9286\n",
      "Epoch 7/10\n",
      "14929/14929 [==============================] - 10s - loss: 1.2254 - main_out_loss: 0.8332 - aux_out_loss: 1.9610 - val_loss: 2.1935 - val_main_out_loss: 1.8082 - val_aux_out_loss: 1.9262\n",
      "Epoch 8/10\n",
      "14929/14929 [==============================] - 10s - loss: 1.1378 - main_out_loss: 0.7494 - aux_out_loss: 1.9423 - val_loss: 2.2317 - val_main_out_loss: 1.8473 - val_aux_out_loss: 1.9217\n",
      "Epoch 9/10\n",
      "14929/14929 [==============================] - 10s - loss: 1.0679 - main_out_loss: 0.6832 - aux_out_loss: 1.9235 - val_loss: 2.2866 - val_main_out_loss: 1.9024 - val_aux_out_loss: 1.9208\n",
      "Epoch 10/10\n",
      "14929/14929 [==============================] - 10s - loss: 0.9974 - main_out_loss: 0.6144 - aux_out_loss: 1.9151 - val_loss: 2.3689 - val_main_out_loss: 1.9852 - val_aux_out_loss: 1.9184\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f733e050a58>"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_author_model = get_model(modelfile, freeze=True)\n",
    "test_author_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", loss_weights=[1., 0.2])\n",
    "test_author_model.fit(X, [y, y], epochs=10, batch_size=128, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14929 samples, validate on 1659 samples\n",
      "Epoch 1/10\n",
      "14929/14929 [==============================] - 36s - loss: 2.5179 - main_out_loss: 2.0782 - aux_out_loss: 2.1987 - val_loss: 2.1857 - val_main_out_loss: 1.7821 - val_aux_out_loss: 2.0181\n",
      "Epoch 2/10\n",
      "14929/14929 [==============================] - 25s - loss: 2.1684 - main_out_loss: 1.7447 - aux_out_loss: 2.1184 - val_loss: 2.1105 - val_main_out_loss: 1.7126 - val_aux_out_loss: 1.9896\n",
      "Epoch 3/10\n",
      "14929/14929 [==============================] - 25s - loss: 1.9924 - main_out_loss: 1.5778 - aux_out_loss: 2.0733 - val_loss: 2.0837 - val_main_out_loss: 1.6885 - val_aux_out_loss: 1.9760\n",
      "Epoch 4/10\n",
      "14929/14929 [==============================] - 25s - loss: 1.8622 - main_out_loss: 1.4546 - aux_out_loss: 2.0377 - val_loss: 2.0641 - val_main_out_loss: 1.6719 - val_aux_out_loss: 1.9609\n",
      "Epoch 5/10\n",
      "14929/14929 [==============================] - 25s - loss: 1.7702 - main_out_loss: 1.3686 - aux_out_loss: 2.0077 - val_loss: 2.0514 - val_main_out_loss: 1.6606 - val_aux_out_loss: 1.9538\n",
      "Epoch 6/10\n",
      "14929/14929 [==============================] - 25s - loss: 1.6867 - main_out_loss: 1.2899 - aux_out_loss: 1.9838 - val_loss: 2.0434 - val_main_out_loss: 1.6534 - val_aux_out_loss: 1.9499\n",
      "Epoch 7/10\n",
      "14929/14929 [==============================] - 25s - loss: 1.6173 - main_out_loss: 1.2237 - aux_out_loss: 1.9677 - val_loss: 2.0439 - val_main_out_loss: 1.6559 - val_aux_out_loss: 1.9403\n",
      "Epoch 8/10\n",
      "14929/14929 [==============================] - 25s - loss: 1.5544 - main_out_loss: 1.1647 - aux_out_loss: 1.9486 - val_loss: 2.0464 - val_main_out_loss: 1.6588 - val_aux_out_loss: 1.9377\n",
      "Epoch 9/10\n",
      "14929/14929 [==============================] - 25s - loss: 1.5085 - main_out_loss: 1.1220 - aux_out_loss: 1.9326 - val_loss: 2.0482 - val_main_out_loss: 1.6614 - val_aux_out_loss: 1.9341\n",
      "Epoch 10/10\n",
      "14929/14929 [==============================] - 25s - loss: 1.4524 - main_out_loss: 1.0691 - aux_out_loss: 1.9166 - val_loss: 2.0539 - val_main_out_loss: 1.6673 - val_aux_out_loss: 1.9329\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f733c857f28>"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_author_model = get_model(modelfile, freeze=True)\n",
    "test_author_model.fit(X, [y, y], epochs=10, batch_size=128, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.optimizers.Adam at 0x7f73457bc940>"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_author_model.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.80500852288\n",
      "5.88146182806\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "\n",
    "print(mean(scores[:50]))\n",
    "print(mean(scores[50:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f74376ee9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(get_chunks(scores, 50)[:20]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.1628624592851473,\n",
       " 2.1628624592851473,\n",
       " 2.1628624592851473,\n",
       " 2.1628624592851473,\n",
       " 2.1628624592851473,\n",
       " 2.1628624592851473,\n",
       " 2.1628624592851473,\n",
       " 2.1628624592851473,\n",
       " 2.1628624592851473,\n",
       " 2.1628624592851473]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Generating with seed: \"some test text does it really matter what it says this is some test text does it really matter what \"\n",
      "some test text does it really matter what it says this is some test text does it really matter what I can go be saying the toucest did. I giving a cout before I went here that I will tryink my be and I had abun that I have had a lotakors are their is a countrination has totarchy the supposed this place on proble was pretty and has really knowledge into the house or a werd attentaning and carden  andb anamarh open something and then an ind in the harde that mornings) which was really shakow in the waster to a coufst in the seat fr. especially the schalut and it was not a good deal card)s has been an interroursed with a hat time Will salak was more back their brought and husband but the same go beal action to super it ip another as such suggestions, and round. It wasn't servant to and event, which was tasty and the spinat.. It is the lasticg are so deperse, I will be back that and the experience farl loung inco that their too. Things I think she recomoond. I can mean account with perfect I have to be caying to a content and a stupy in the coptabliness...  that we make the firm something, that but to some teach more quite hours, and the measured (orarke (yes compation red a couple good. They was why too tawstich the Bwackes and a lot o durn up. I got me sot, oberving observer to take it into it wasn't sarcy is a pretty spily spacon to the Told over and was more athe. The favoo was dirgery rising a confeed the poll lacking of the pastrious when was a woman was light and weiked the people suser (a thought. These borrowed someda fuither...oon it must regard back the hotate was past. Sir is what was shamed on from was freedo bluncissedly in the inerpieal. Then on the chain that sure chickening. It went for breakfast burnitouurs of dropose (the was in the pice in the pwony which was more drink, they could are been before their mush gree clains and the restaurant) and justilg Prices and  asto hair never burne schak an inneceivatence of a thiceabl counts, orgego the ise an excersing she has made here and a good.. I beeauve that an in the bar. I bene this incongrup oteer... Then came very very cook, I had imagined. The mean they are thinks have been into burgerting and speak, but it is night to buy book in the bar slipped very for the fire so came back to the dive over a bicks their wasn't one fast tasty much and it is berolautant conket she could venturing light as well amount and salawy than towards the effective with oprious towards or a tertation wasn't for. I teld good at the tena minut on those who did, they had the fladoc at the Vincys... The time was postibly never turned holds also side impeds the hotato's busy good. The mine flavor with Tertain that she had an att the event since like being was a fair we had perpotuenry it had been an unmapph out the apsong chain notes to definitely regard the effect of grreicu speak and the phr open and runnitg place for them the disgeroas in the walling great ended one is and chilled sorrocustfien and incenvoutationes for a she was jlate all good saluk with some into I was not eses this new sacring tenter or off to with the philing better living. It wasn't ourtioned some backwars.....was. I had a very good. They went to the atiablillng at the Tormers which like to do titure with service a both since free weight (sheteratded and got the service has got to the best good from Pobkly tottone. I went here an interestion and a good because. It has an ad sacarody places who orsert, I feel that. This is a picitous redinite chooounoss, they kinds, but I could not spare to the pho chillic insthen.. That is a step with a good because some banckture and it was for a little flavors into exhraunt with a thought partier and breakfast to the chicken, was starttening and care. The calling her browning rrive it was offidionuul.....routre by totting to side was notshpach sense it innoce, was nature confescedd the only said of couple.....ly fanulth open the touchest and hot generous and disappointed (goud because place touns with man have a great chan that has fear amoult at the Tayuns....roonly. On touch a got to sttard we she was scacks the phial. The patt and would recommend at the tit over that the place was cheeses with barryfow burning was really humt she walking and decommend to the earily emgreives and was this clase for a choal chacouse....loony) deatilnsly cheeseva cofterable on such here, but not I feel got the stlaik humi goces to to tcot. I would chocolt and trying the peculiar people on fungoues pize was drinks for a little hurter....lyww now one phile masba overle. The paving salag and was nince they but a vid generation, was generation, they were an ungen the anae and the cho fend it is morryarky the hopala and the edged. The fewm macking had really shredd frequent. It is an aar notel (it would be a bloosy to look for them went but orgginouus or their pare for a tears cofkees with me. te wound out too, and it was have out tought touce outuuntanna for to arather the sension and lingered by the food seat with powe and cheese, pountsnat, the worked from other place me any a cou"
     ]
    }
   ],
   "source": [
    "generate(test_author_model, diversity=0.7, text=\"this is some test text does it really matter what it says \"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 30\n",
      "1 / 30\n",
      "2 / 30\n",
      "3 / 30\n",
      "4 / 30\n",
      "5 / 30\n",
      "6 / 30\n",
      "7 / 30\n",
      "8 / 30\n",
      "9 / 30\n",
      "10 / 30\n",
      "11 / 30\n",
      "12 / 30\n",
      "13 / 30\n",
      "14 / 30\n",
      "15 / 30\n",
      "16 / 30\n",
      "17 / 30\n",
      "18 / 30\n",
      "19 / 30\n",
      "20 / 30\n",
      "21 / 30\n",
      "22 / 30\n",
      "23 / 30\n",
      "24 / 30\n",
      "25 / 30\n",
      "26 / 30\n",
      "27 / 30\n",
      "28 / 30\n",
      "29 / 30\n",
      "CPU times: user 6h 28min 4s, sys: 30min 56s, total: 6h 59min 1s\n",
      "Wall time: 55min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "author_models = []  # [(author_model, author_id), (author_model, author_id), ...] - ids are ints\n",
    "for i, train_text in enumerate(train_texts):\n",
    "    print(\"{} / {}\".format(i, len(train_texts)))\n",
    "    ct = clean_text(train_text, charset)\n",
    "    am = get_model(modelfile, freeze=True)\n",
    "    X, y = vectorize(ct)\n",
    "    am.fit(X, [y, y], epochs=3, batch_size=128, verbose=False)\n",
    "    author_models.append((am, train_labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "1500\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "print(len(author_models))\n",
    "print(len(test_texts))\n",
    "print(len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182.57733333333334"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = [text.count(\" \") for text in test_texts]\n",
    "mean(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 182 words is quite short\n",
    "# Try to join 5 tests texts together\n",
    "longer_test_texts = get_chunks(test_texts, 5)\n",
    "longer_test_labels = get_chunks(test_labels, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all([len(set(x)) == 1 for x in longer_test_labels])  # Make sure that all combined labels are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "longer_test_texts = ['\\n'.join(chunk) for chunk in longer_test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "longer_test_labels = [chunk[0] for chunk in longer_test_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 300 .............................. 0:05:31.748432\n",
      "1 / 300 .............................. 0:00:41.397148\n",
      "2 / 300 .............................. 0:00:41.535609\n",
      "3 / 300 .............................. 0:00:41.479349\n",
      "4 / 300 .............................. 0:00:41.152374\n",
      "5 / 300 .............................. 0:00:41.341354\n",
      "6 / 300 .............................. 0:00:41.549168\n",
      "7 / 300 .............................. 0:00:41.479860\n",
      "8 / 300 .............................. 0:00:41.368123\n",
      "9 / 300 .............................. 0:00:41.514281\n",
      "10 / 300 .............................. 0:00:40.703178\n",
      "11 / 300 .............................. 0:00:40.389044\n",
      "12 / 300 .............................. 0:00:40.530118\n",
      "13 / 300 .............................. 0:00:41.263114\n",
      "14 / 300 .............................. 0:00:40.575675\n",
      "15 / 300 .............................. 0:00:40.374385\n",
      "16 / 300 .............................. 0:00:40.514698\n",
      "17 / 300 .............................. 0:00:41.142076\n",
      "18 / 300 .............................. 0:00:40.396425\n",
      "19 / 300 .............................. 0:00:40.631787\n",
      "20 / 300 .............................. 0:00:40.968754\n",
      "21 / 300 .............................. 0:00:41.041473\n",
      "22 / 300 .............................. 0:00:41.200501\n",
      "23 / 300 .............................. 0:00:41.120069\n",
      "24 / 300 .............................. 0:00:41.226798\n",
      "25 / 300 .............................. 0:00:39.550173\n",
      "26 / 300 .............................. 0:00:38.656925\n",
      "27 / 300 .............................. 0:00:38.606374\n",
      "28 / 300 .............................. 0:00:38.283627\n",
      "29 / 300 .............................. 0:00:38.579143\n",
      "30 / 300 .............................. 0:00:39.463868\n",
      "31 / 300 .............................. 0:00:39.474963\n",
      "32 / 300 .............................. 0:00:40.067295\n",
      "33 / 300 .............................. 0:00:39.431574\n",
      "34 / 300 .............................. 0:00:39.715739\n",
      "35 / 300 .............................. 0:00:39.409319\n",
      "36 / 300 .............................. 0:00:39.469307\n",
      "37 / 300 .............................. 0:00:41.266525\n",
      "38 / 300 .............................. 0:00:40.501540\n",
      "39 / 300 .............................. 0:00:41.642366\n",
      "40 / 300 .............................. 0:00:42.905945\n",
      "41 / 300 .............................. 0:00:40.553986\n",
      "42 / 300 .............................. 0:00:39.146261\n",
      "43 / 300 .............................. 0:00:38.985081\n",
      "44 / 300 .............................. 0:00:39.934981\n",
      "45 / 300 .............................. 0:00:39.517940\n",
      "46 / 300 .............................. 0:00:38.923861\n",
      "47 / 300 .............................. 0:00:39.577221\n",
      "48 / 300 .............................. 0:00:40.335574\n",
      "49 / 300 .............................. 0:00:40.890051\n",
      "50 / 300 .............................. 0:00:41.051403\n",
      "51 / 300 .............................. 0:00:40.673710\n",
      "52 / 300 .............................. 0:00:39.784654\n",
      "53 / 300 .............................. 0:00:39.683003\n",
      "54 / 300 .............................. 0:00:41.096262\n",
      "55 / 300 .............................. 0:00:41.494539\n",
      "56 / 300 .............................. 0:00:42.159378\n",
      "57 / 300 .............................. 0:00:41.604133\n",
      "58 / 300 .............................. 0:00:42.063691\n",
      "59 / 300 .............................. 0:00:42.145543\n",
      "60 / 300 .............................. 0:00:42.392241\n",
      "61 / 300 .............................. 0:00:42.612229\n",
      "62 / 300 .............................. 0:00:42.538202\n",
      "63 / 300 .............................. 0:00:42.398143\n",
      "64 / 300 .............................. 0:00:42.483554\n",
      "65 / 300 .............................. 0:00:42.528572\n",
      "66 / 300 .............................. 0:00:41.890057\n",
      "67 / 300 .............................. 0:00:41.726908\n",
      "68 / 300 .............................. 0:00:42.074908\n",
      "69 / 300 .............................. 0:00:42.316804\n",
      "70 / 300 .............................. 0:00:40.808615\n",
      "71 / 300 .............................. 0:00:39.113676\n",
      "72 / 300 .............................. 0:00:39.248360\n",
      "73 / 300 .............................. 0:00:40.794788\n",
      "74 / 300 .............................. 0:00:40.863755\n",
      "75 / 300 .............................. 0:00:41.807952\n",
      "76 / 300 .............................. 0:00:42.560373\n",
      "77 / 300 .............................. 0:00:43.128970\n",
      "78 / 300 .............................. 0:00:41.519944\n",
      "79 / 300 .............................. 0:00:43.332951\n",
      "80 / 300 .............................. 0:00:43.580812\n",
      "81 / 300 .............................. 0:00:43.641690\n",
      "82 / 300 .............................. 0:00:43.199529\n",
      "83 / 300 .............................. 0:00:43.085184\n",
      "84 / 300 .............................. 0:00:42.514240\n",
      "85 / 300 .............................. 0:00:42.717766\n",
      "86 / 300 .............................. 0:00:42.692970\n",
      "87 / 300 .............................. 0:00:42.627576\n",
      "88 / 300 .............................. 0:00:42.637020\n",
      "89 / 300 .............................. 0:00:42.811161\n",
      "90 / 300 .............................. 0:00:42.670833\n",
      "91 / 300 .............................. 0:00:41.852790\n",
      "92 / 300 .............................. 0:00:41.775506\n",
      "93 / 300 .............................. 0:00:41.376592\n",
      "94 / 300 .............................. 0:00:41.336654\n",
      "95 / 300 .............................. 0:00:41.362982\n",
      "96 / 300 .............................. 0:00:41.180583\n",
      "97 / 300 .............................. 0:00:41.231065\n",
      "98 / 300 .............................. 0:00:42.066988\n",
      "99 / 300 .............................. 0:00:42.100336\n",
      "100 / 300 .............................. 0:00:41.651390\n",
      "101 / 300 .............................. 0:00:41.008101\n",
      "102 / 300 .............................. 0:00:41.148380\n",
      "103 / 300 .............................. 0:00:41.745355\n",
      "104 / 300 .............................. 0:00:42.230778\n",
      "105 / 300 .............................. 0:00:42.715930\n",
      "106 / 300 .............................. 0:00:42.013384\n",
      "107 / 300 .............................. 0:00:42.109486\n",
      "108 / 300 .............................. 0:00:41.528022\n",
      "109 / 300 .............................. 0:00:41.803204\n",
      "110 / 300 .............................. 0:00:42.011706\n",
      "111 / 300 .............................. 0:00:42.386537\n",
      "112 / 300 .............................. 0:00:42.244710\n",
      "113 / 300 .............................. 0:00:41.741825\n",
      "114 / 300 .............................. 0:00:41.930172\n",
      "115 / 300 .............................. 0:00:42.307297\n",
      "116 / 300 .............................. 0:00:41.693650\n",
      "117 / 300 .............................. 0:00:41.914205\n",
      "118 / 300 .............................. 0:00:42.146817\n",
      "119 / 300 .............................. 0:00:41.987281\n",
      "120 / 300 .............................. 0:00:41.715864\n",
      "121 / 300 .............................. 0:00:41.603405\n",
      "122 / 300 .............................. 0:00:41.102339\n",
      "123 / 300 .............................. 0:00:41.127641\n",
      "124 / 300 .............................. 0:00:41.745729\n",
      "125 / 300 .............................. 0:00:41.225251\n",
      "126 / 300 .............................. 0:00:40.839314\n",
      "127 / 300 .............................. 0:00:39.968153\n",
      "128 / 300 .............................. 0:00:39.964465\n",
      "129 / 300 .............................. 0:00:40.618662\n",
      "130 / 300 .............................. 0:00:40.682838\n",
      "131 / 300 .............................. 0:00:40.668386\n",
      "132 / 300 .............................. 0:00:40.566657\n",
      "133 / 300 .............................. 0:00:40.433330\n",
      "134 / 300 .............................. 0:00:40.702629\n",
      "135 / 300 .............................. 0:00:40.597844\n",
      "136 / 300 .............................. 0:00:40.570112\n",
      "137 / 300 .............................. 0:00:40.572449\n",
      "138 / 300 .............................. 0:00:40.657975\n",
      "139 / 300 .............................. 0:00:40.407133\n",
      "140 / 300 .............................. 0:00:40.108083\n",
      "141 / 300 .............................. 0:00:40.234003\n",
      "142 / 300 .............................. 0:00:40.338420\n",
      "143 / 300 .............................. 0:00:40.340553\n",
      "144 / 300 .............................. 0:00:40.264774\n",
      "145 / 300 .............................. 0:00:39.861487\n",
      "146 / 300 .............................. 0:00:40.007872\n",
      "147 / 300 .............................. 0:00:39.868639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148 / 300 .............................. 0:00:39.522352\n",
      "149 / 300 .............................. 0:00:38.304991\n",
      "150 / 300 .............................. 0:00:38.897472\n",
      "151 / 300 .............................. 0:00:38.679245\n",
      "152 / 300 .............................. 0:00:38.771781\n",
      "153 / 300 .............................. 0:00:38.821587\n",
      "154 / 300 .............................. 0:00:38.741388\n",
      "155 / 300 .............................. 0:00:38.672861\n",
      "156 / 300 .............................. 0:00:38.832906\n",
      "157 / 300 .............................. 0:00:38.827954\n",
      "158 / 300 .............................. 0:00:38.769065\n",
      "159 / 300 .............................. 0:00:39.530681\n",
      "160 / 300 .............................. 0:00:39.006905\n",
      "161 / 300 .............................. 0:00:39.760114\n",
      "162 / 300 .............................. 0:00:39.368285\n",
      "163 / 300 .............................. 0:00:38.947883\n",
      "164 / 300 .............................. 0:00:38.951897\n",
      "165 / 300 .............................. 0:00:39.415626\n",
      "166 / 300 .............................. 0:00:40.676411\n",
      "167 / 300 .............................. 0:00:40.862446\n",
      "168 / 300 .............................. 0:00:43.137666\n",
      "169 / 300 .............................. 0:00:42.334836\n",
      "170 / 300 .............................. 0:00:40.994363\n",
      "171 / 300 .............................. 0:00:40.406033\n",
      "172 / 300 .............................. 0:00:41.014881\n",
      "173 / 300 .............................. 0:00:41.358601\n",
      "174 / 300 .............................. 0:00:41.086067\n",
      "175 / 300 .............................. 0:00:40.700405\n",
      "176 / 300 .............................. 0:00:40.400232\n",
      "177 / 300 .............................. 0:00:41.843320\n",
      "178 / 300 .............................. 0:00:41.794664\n",
      "179 / 300 .............................. 0:00:41.900758\n",
      "180 / 300 .............................. 0:00:42.153649\n",
      "181 / 300 .............................. 0:00:42.456985\n",
      "182 / 300 .............................. 0:00:42.562467\n",
      "183 / 300 .............................. 0:00:42.340022\n",
      "184 / 300 .............................. 0:00:42.051417\n",
      "185 / 300 .............................. 0:00:42.770628\n",
      "186 / 300 .............................. 0:00:41.957768\n",
      "187 / 300 .............................. 0:00:41.560370\n",
      "188 / 300 .............................. 0:00:41.452063\n",
      "189 / 300 .............................. 0:00:41.826402\n",
      "190 / 300 .............................. 0:00:42.016951\n",
      "191 / 300 .............................. 0:00:43.379175\n",
      "192 / 300 .............................. 0:00:43.571953\n",
      "193 / 300 .............................. 0:00:41.790882\n",
      "194 / 300 .............................. 0:00:43.467232\n",
      "195 / 300 .............................. 0:00:43.654435\n",
      "196 / 300 .............................. 0:00:43.633589\n",
      "197 / 300 .............................. 0:00:43.107169\n",
      "198 / 300 .............................. 0:00:42.994235\n",
      "199 / 300 .............................. 0:00:42.728880\n",
      "200 / 300 .............................. 0:00:42.837395\n",
      "201 / 300 .............................. 0:00:42.836139\n",
      "202 / 300 .............................. 0:00:42.793976\n",
      "203 / 300 .............................. 0:00:42.575640\n",
      "204 / 300 .............................. 0:00:42.257803\n",
      "205 / 300 .............................. 0:00:41.904312\n",
      "206 / 300 .............................. 0:00:42.776132\n",
      "207 / 300 .............................. 0:00:41.545927\n",
      "208 / 300 .............................. 0:00:42.419957\n",
      "209 / 300 .............................. 0:00:41.116003\n",
      "210 / 300 .............................. 0:00:40.880410\n",
      "211 / 300 .............................. 0:00:41.145994\n",
      "212 / 300 .............................. 0:00:41.071901\n",
      "213 / 300 .............................. 0:00:41.280306\n",
      "214 / 300 .............................. 0:00:41.796318\n",
      "215 / 300 .............................. 0:00:41.692718\n",
      "216 / 300 .............................. 0:00:41.716676\n",
      "217 / 300 .............................. 0:00:42.080311\n",
      "218 / 300 .............................. 0:00:42.133906\n",
      "219 / 300 .............................. 0:00:42.673906\n",
      "220 / 300 .............................. 0:00:42.607591\n",
      "221 / 300 .............................. 0:00:42.354194\n",
      "222 / 300 .............................. 0:00:41.809441\n",
      "223 / 300 .............................. 0:00:42.050481\n",
      "224 / 300 .............................. 0:00:42.209501\n",
      "225 / 300 .............................. 0:00:41.910173\n",
      "226 / 300 .............................. 0:00:42.222803\n",
      "227 / 300 .............................. 0:00:42.117665\n",
      "228 / 300 .............................. 0:00:42.162236\n",
      "229 / 300 .............................. 0:00:41.482968\n",
      "230 / 300 .............................. 0:00:40.561727\n",
      "231 / 300 .............................. 0:00:40.277857\n",
      "232 / 300 .............................. 0:00:40.017279\n",
      "233 / 300 .............................. 0:00:40.275410\n",
      "234 / 300 .............................. 0:00:40.189819\n",
      "235 / 300 .............................. 0:00:40.258725\n",
      "236 / 300 .............................. 0:00:40.168755\n",
      "237 / 300 .............................. 0:00:40.208232\n",
      "238 / 300 .............................. 0:00:40.220404\n",
      "239 / 300 .............................. 0:00:40.132636\n",
      "240 / 300 .............................. 0:00:40.005178\n",
      "241 / 300 .............................. 0:00:39.945692\n",
      "242 / 300 .............................. 0:00:40.104547\n",
      "243 / 300 .............................. 0:00:40.129491\n",
      "244 / 300 .............................. 0:00:39.986973\n",
      "245 / 300 .............................. 0:00:39.828479\n",
      "246 / 300 .............................. 0:00:40.032119\n",
      "247 / 300 .............................. 0:00:39.968867\n",
      "248 / 300 .............................. 0:00:39.928357\n",
      "249 / 300 .............................. 0:00:40.176649\n",
      "250 / 300 .............................. 0:00:40.015073\n",
      "251 / 300 .............................. 0:00:39.996424\n",
      "252 / 300 .............................. 0:00:40.383439\n",
      "253 / 300 .............................. 0:00:41.955297\n",
      "254 / 300 .............................. 0:00:41.595085\n",
      "255 / 300 .............................. 0:00:41.509493\n",
      "256 / 300 .............................. 0:00:40.783571\n",
      "257 / 300 .............................. 0:00:41.669239\n",
      "258 / 300 .............................. 0:00:41.070736\n",
      "259 / 300 .............................. 0:00:40.102988\n",
      "260 / 300 .............................. 0:00:40.315039\n",
      "261 / 300 .............................. 0:00:40.117097\n",
      "262 / 300 .............................. 0:00:40.214856\n",
      "263 / 300 .............................. 0:00:39.976218\n",
      "264 / 300 .............................. 0:00:40.034867\n",
      "265 / 300 .............................. 0:00:40.249196\n",
      "266 / 300 .............................. 0:00:40.285858\n",
      "267 / 300 .............................. 0:00:39.666003\n",
      "268 / 300 .............................. 0:00:39.839506\n",
      "269 / 300 .............................. 0:00:39.885683\n",
      "270 / 300 .............................. 0:00:39.785370\n",
      "271 / 300 .............................. 0:00:39.877055\n",
      "272 / 300 .............................. 0:00:39.673465\n",
      "273 / 300 .............................. 0:00:39.725608\n",
      "274 / 300 .............................. 0:00:39.789904\n",
      "275 / 300 .............................. 0:00:39.972300\n",
      "276 / 300 .............................. 0:00:40.758082\n",
      "277 / 300 .............................. 0:00:39.760281\n",
      "278 / 300 .............................. 0:00:39.738824\n",
      "279 / 300 .............................. 0:00:39.620829\n",
      "280 / 300 .............................. 0:00:39.908452\n",
      "281 / 300 .............................. 0:00:39.810006\n",
      "282 / 300 .............................. 0:00:39.933958\n",
      "283 / 300 .............................. 0:00:39.794260\n",
      "284 / 300 .............................. 0:00:39.586896\n",
      "285 / 300 .............................. 0:00:39.762401\n",
      "286 / 300 .............................. 0:00:39.722216\n",
      "287 / 300 .............................. 0:00:39.619936\n",
      "288 / 300 .............................. 0:00:39.831471\n",
      "289 / 300 .............................. 0:00:39.683987\n",
      "290 / 300 .............................. 0:00:39.822693\n",
      "291 / 300 .............................. 0:00:39.896464\n",
      "292 / 300 .............................. 0:00:39.723701\n",
      "293 / 300 .............................. 0:00:39.848818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294 / 300 .............................. 0:00:40.019344\n",
      "295 / 300 .............................. 0:00:40.220888\n",
      "296 / 300 .............................. 0:00:40.058392\n",
      "297 / 300 .............................. 0:00:39.858766\n",
      "298 / 300 .............................. 0:00:39.781350\n",
      "299 / 300 .............................. 0:00:39.895876\n",
      "CPU times: user 22h 45min 34s, sys: 2h 10min 9s, total: 1d 55min 43s\n",
      "Wall time: 3h 29min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from random import shuffle\n",
    "from datetime import datetime\n",
    "\n",
    "def get_predictions(author_models, test_texts, test_labels):\n",
    "    \"\"\"Evaluate each text for each author_model and append first metric to predictions\"\"\"\n",
    "    indicies = list(range(len(test_texts)))\n",
    "\n",
    "    test_texts = np.array(test_texts)\n",
    "    test_labels = np.array(test_labels)\n",
    "\n",
    "    test_texts = test_texts[indicies]\n",
    "    test_labels = test_labels[indicies]\n",
    "\n",
    "    predictions = []\n",
    "    for i, text in enumerate(test_texts):\n",
    "        t1 = datetime.now()\n",
    "        print(\"{} / {}\".format(i, len(test_texts)), end=\" \")\n",
    "        X, y = vectorize(clean_text(text, charset))\n",
    "\n",
    "        losses = []\n",
    "        for am in author_models:\n",
    "            print(\".\", end=\"\")\n",
    "            model = am[0]\n",
    "            label = am[1]\n",
    "            loss = model.evaluate(X, [y, y], verbose=0)\n",
    "            losses.append((loss[0], label))\n",
    "        print(\" {}\".format(datetime.now() - t1))\n",
    "        predictions.append(losses)\n",
    "    return predictions\n",
    "    \n",
    "\n",
    "predictions_long = get_predictions(author_models, longer_test_texts, longer_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n"
     ]
    }
   ],
   "source": [
    "print(\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_is = []\n",
    "for pred in predictions_long:\n",
    "    pred_i = [p[0] for p in pred]\n",
    "    pred_is.append(pred_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labs = [np.argmin(pred) for pred in pred_is]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83666666666666667"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(longer_test_labels, pred_labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 100)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 100, 300)      25500       input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 100, 300)      0           embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)                (None, 96, 64)        96064       dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)   (None, 24, 64)        0           conv1d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 256)           328704      max_pooling1d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 512)           131072      lstm_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 512)           2048        dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 512)           0           batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 256)           131072      activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 256)           1024        dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 256)           0           batch_normalization_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 85)            21845       activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 85)            21845       lstm_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "main_out (Activation)            (None, 85)            0           dense_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "aux_out (Activation)             (None, 85)            0           dense_1[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 759,174\n",
      "Trainable params: 43,690\n",
      "Non-trainable params: 715,484\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "test_author_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Model.evaluate of <keras.engine.training.Model object at 0x7f7457285b70>>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_author_model.evaluatei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "word_vec = TfidfVectorizer(min_df=3, ngram_range=(1,2))\n",
    "char_vec = TfidfVectorizer(min_df=3, ngram_range=(2,5))\n",
    "\n",
    "fu = FeatureUnion([\n",
    "    ('word', word_vec),\n",
    "    ('char', char_vec)\n",
    "])\n",
    "\n",
    "\n",
    "X_train = fu.fit_transform(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = fu.transform(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.fit(X_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52200000000000002"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_longer = fu.transform(longer_test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = svm.predict(X_test_longer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92666666666666664"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(longer_test_labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16565, 100)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = vectorize(clean_text(train_texts[3], charset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16524, 100)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16524, 85)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False,  True, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False], dtype=bool)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([46,  8,  9, 19,  0,  9, 19,  0, 20,  8,  5,  0,  3, 12, 15, 19,  5,\n",
       "       19, 20,  0, 19, 13, 15,  7,  0, 19, 16, 15, 20,  0,  6, 15, 18,  0,\n",
       "       13,  5, 65,  0,  5, 24, 20,  5, 18,  9, 15, 18,  0,  9, 19, 14, 59,\n",
       "       20,  0,  1, 14, 25, 20,  8,  9, 14,  7,  0,  6,  1, 14,  3, 25, 64,\n",
       "        0,  1, 14,  4,  0, 14, 15,  0, 15, 14,  5,  0, 23,  1, 19,  0,  8,\n",
       "        5, 18,  5,  0, 23,  8,  5, 14,  0, 35,  0, 23,  1, 12, 11])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x7f73ab7bc0b8>,\n",
       " <keras.layers.embeddings.Embedding at 0x7f73adb33710>,\n",
       " <keras.layers.core.Dropout at 0x7f73ab7bc2e8>,\n",
       " <keras.layers.convolutional.Conv1D at 0x7f73ab7bc2b0>,\n",
       " <keras.layers.pooling.MaxPooling1D at 0x7f73ab718710>,\n",
       " <keras.layers.recurrent.LSTM at 0x7f73ab7bc278>]"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_author_model.layers[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
