{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "['BernardHickey', 'JimGilchrist', 'HeatherScoffield', 'AlanCrosby', 'RobinSidel', 'DarrenSchuettler', 'EricAuchard', 'MichaelConnor', 'WilliamKazer', 'GrahamEarnshaw']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "authors = os.listdir(\"C50/C50train/\")\n",
    "print(len(authors))\n",
    "\n",
    "print(authors[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_chunks(l, n):\n",
    "    n = max(1, n)\n",
    "    return [l[i:i+n] for i in range(0, len(l), n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_texts_for_author(data_path, author_name):\n",
    "    fpath = os.path.join(data_path, author_name)\n",
    "    fnames = os.listdir(fpath)\n",
    "    text_paths = [os.path.join(fpath, name) for name in fnames]\n",
    "    texts = []\n",
    "    for tp in text_paths:\n",
    "        with open(tp) as f:\n",
    "            s = f.read()\n",
    "            texts.append(s)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "c50_train = \"C50/C50train/\"\n",
    "\n",
    "def get_all_c50(c50_path):  # path to train or tests\n",
    "    all_texts = [] \n",
    "    all_labels = []\n",
    "    for author in authors:\n",
    "        author_texts = get_texts_for_author(c50_path, author)\n",
    "        all_texts += author_texts\n",
    "        all_labels += [author] * len(author_texts)\n",
    "        if len(author_texts) != 50:\n",
    "            print(author, \"not 50\")\n",
    "    return all_texts, all_labels\n",
    "\n",
    "all_texts, all_labels = get_all_c50(c50_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# concatenate texts for train authors\n",
    "\n",
    "by_author_texts = []\n",
    "author_chunks = get_chunks(all_texts, 50)\n",
    "for chunk in author_chunks:\n",
    "    complete_text = ' '.join([text for text in chunk])\n",
    "    by_author_texts.append(complete_text)\n",
    "\n",
    "by_author_labels = []\n",
    "label_chunks = get_chunks(all_labels, 50)\n",
    "for chunk in label_chunks:\n",
    "    assert(len(set(chunk)) == 1)\n",
    "    by_author_labels.append(chunk[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# vectorization - chars to ints\n",
    "import string\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    \"\"\"Sample predictions from a probability array\"\"\"\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds + 1e-6) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def generate(model, diversity=0.5, text=\"\"):\n",
    "    \"\"\"Generate text from a model\"\"\"\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    for i in range(5000):\n",
    "        x = np.zeros((1, maxlen), dtype=np.int)\n",
    "        for t, char in enumerate(sentence):\n",
    "            try:\n",
    "                x[0, t] = char_indices[char]\n",
    "            except:\n",
    "                print(sentence)\n",
    "        preds = model.predict(x, verbose=0)[0][0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    return\n",
    "\n",
    "def vectorize(text):\n",
    "    \"\"\"Convert text into character sequences\"\"\"\n",
    "    step = 3\n",
    "    sentences = []\n",
    "    next_chars = []\n",
    "    for i in range(0, len(text) - maxlen, step):\n",
    "        sentences.append(text[i: i + maxlen])\n",
    "        next_chars.append(text[i + maxlen])\n",
    "    X = np.zeros((len(sentences), maxlen), dtype=np.int)\n",
    "    y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for t, char in enumerate(sentence):\n",
    "            X[i, t] = char_indices[char]\n",
    "        y[i, char_indices[next_chars[i]]] = 1\n",
    "    return X, y\n",
    "\n",
    "def clean_text(text, charset):\n",
    "    text = \" \".join(text.split())  # all white space is one space\n",
    "    text = \"\".join([x for x in text if x in charset])  # remove characters that we don't care about\n",
    "    return text\n",
    "\n",
    "def get_model(modelfile, freeze=False):\n",
    "    model = load_model(modelfile)\n",
    "    if freeze:\n",
    "        for layer in model.layers[:6]:\n",
    "            layer.trainable = False\n",
    "    return model\n",
    "\n",
    "chars = \" \" + string.ascii_letters + string.punctuation  # sorted to keep indices consistent\n",
    "charset = set(chars)  # for lookup\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "maxlen = 100  # must match length which generated model - the sequence length\n",
    "\n",
    "# load a pretrained language model\n",
    "modelfile = \"charlm2/model_middlemarch_cnn.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM, Input, Embedding, Conv1D, MaxPooling1D, BatchNormalization, GRU\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def get_gru_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(GRU(256))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(y.shape[1], activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37097 samples, validate on 9275 samples\n",
      "Epoch 1/5\n",
      "37097/37097 [==============================] - 151s - loss: 2.4624 - val_loss: 1.9483\n",
      "Epoch 2/5\n",
      "37097/37097 [==============================] - 149s - loss: 1.8687 - val_loss: 1.7529\n",
      "Epoch 3/5\n",
      "37097/37097 [==============================] - 149s - loss: 1.6638 - val_loss: 1.6446\n",
      "Epoch 4/5\n",
      "37097/37097 [==============================] - 149s - loss: 1.5168 - val_loss: 1.5930\n",
      "Epoch 5/5\n",
      "37097/37097 [==============================] - 148s - loss: 1.4126 - val_loss: 1.5589\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f18687a6cf8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test one to make sure we don't overfit\n",
    "test_text = by_author_texts[0]\n",
    "ct = clean_text(test_text, charset)\n",
    "X, y = vectorize(ct)\n",
    "model = get_gru_model()\n",
    "model.fit(X, y, validation_split=0.2, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 50\n",
      "1 / 50\n",
      "2 / 50\n",
      "3 / 50\n",
      "4 / 50\n",
      "5 / 50\n",
      "6 / 50\n",
      "7 / 50\n",
      "8 / 50\n",
      "9 / 50\n",
      "10 / 50\n",
      "11 / 50\n",
      "12 / 50\n",
      "13 / 50\n",
      "14 / 50\n",
      "15 / 50\n",
      "16 / 50\n",
      "17 / 50\n",
      "18 / 50\n",
      "19 / 50\n",
      "20 / 50\n",
      "21 / 50\n",
      "22 / 50\n",
      "23 / 50\n",
      "24 / 50\n",
      "25 / 50\n",
      "26 / 50\n",
      "27 / 50\n",
      "28 / 50\n",
      "29 / 50\n",
      "30 / 50\n",
      "31 / 50\n",
      "32 / 50\n",
      "33 / 50\n",
      "34 / 50\n",
      "35 / 50\n",
      "36 / 50\n",
      "37 / 50\n",
      "38 / 50\n",
      "39 / 50\n",
      "40 / 50\n",
      "41 / 50\n",
      "42 / 50\n",
      "43 / 50\n",
      "44 / 50\n",
      "45 / 50\n",
      "46 / 50\n",
      "47 / 50\n",
      "48 / 50\n",
      "49 / 50\n",
      "CPU times: user 5h 20min 59s, sys: 25min 47s, total: 5h 46min 46s\n",
      "Wall time: 3h 34min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "author_models = []  # [(author_model, author_id), (author_model, author_id), ...] - ids are ints\n",
    "for i, train_text in enumerate(by_author_texts):\n",
    "    print(\"{} / {}\".format(i, len(by_author_texts)))\n",
    "    ct = clean_text(train_text, charset)\n",
    "    am = get_gru_model()\n",
    "    X, y = vectorize(ct)\n",
    "    am.fit(X, y, epochs=5, batch_size=128, verbose=0)\n",
    "    author_models.append((am, by_author_labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "print(len(author_models))\n",
    "author_models[0]\n",
    "# print(len(test_texts))\n",
    "# print(len(test_labels))\n",
    "\n",
    "for (model, author) in author_models:\n",
    "    fname = \"./authormodels/{}.hdf5\".format(author)\n",
    "    model.save(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<keras.models.Sequential at 0x7f1860c74048>, 'BernardHickey'),\n",
       " (<keras.models.Sequential at 0x7f1862db0cc0>, 'JimGilchrist'),\n",
       " (<keras.models.Sequential at 0x7f18605a4208>, 'HeatherScoffield'),\n",
       " (<keras.models.Sequential at 0x7f1848debac8>, 'AlanCrosby'),\n",
       " (<keras.models.Sequential at 0x7f18479c4f60>, 'RobinSidel')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_models[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28685"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statistics import mean\n",
    "word_counts = [text.count(\" \") for text in by_author_texts]\n",
    "max(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "c50_test_texts, c50_test_labels = get_all_c50(\"C50/C50test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "indicies = list(range(len(c50_test_texts)))\n",
    "shuffle(indicies)\n",
    "indicies = np.array(indicies)\n",
    "c50_test_texts = np.array(c50_test_texts)[indicies]\n",
    "c50_test_labels = np.array(c50_test_labels)[indicies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['LydiaZajc', 'MureDickie', 'RobinSidel', 'MartinWolk',\n",
       "       'KirstinRidley', 'PierreTran', 'NickLouth', \"LynneO'Donnell\",\n",
       "       'BenjaminKangLim', 'FumikoFujisaki'],\n",
       "      dtype='<U17')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 182 words is quite short\n",
    "# Try to join 5 tests texts together\n",
    "longer_test_texts = get_chunks(test_texts, 5)\n",
    "longer_test_labels = get_chunks(test_labels, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all([len(set(x)) == 1 for x in longer_test_labels])  # Make sure that all combined labels are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "longer_test_texts = ['\\n'.join(chunk) for chunk in longer_test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "longer_test_labels = [chunk[0] for chunk in longer_test_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['LydiaZajc', 'MureDickie', 'RobinSidel', 'MartinWolk',\n",
       "       'KirstinRidley', 'PierreTran', 'NickLouth', \"LynneO'Donnell\",\n",
       "       'BenjaminKangLim', 'FumikoFujisaki'],\n",
       "      dtype='<U17')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c50_test_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 500 .................................................. 0:00:36.031044\n",
      "1 / 500 .................................................. 0:00:44.276223\n",
      "2 / 500 .................................................. 0:01:25.236239\n",
      "3 / 500 .................................................. 0:00:59.698112\n",
      "4 / 500 .................................................. 0:00:59.605113\n",
      "5 / 500 .................................................. 0:00:49.339650\n",
      "6 / 500 .................................................. 0:00:37.502912\n",
      "7 / 500 .................................................. 0:00:59.707956\n",
      "8 / 500 .................................................. 0:00:59.678306\n",
      "9 / 500 .................................................. 0:00:59.658917\n",
      "10 / 500 .................................................. 0:01:04.766140\n",
      "11 / 500 .................................................. 0:00:45.952228\n",
      "12 / 500 .................................................. 0:00:51.168298\n",
      "13 / 500 .................................................. 0:00:47.711908\n",
      "14 / 500 .................................................. 0:01:09.858853\n",
      "15 / 500 .................................................. 0:00:56.308354\n",
      "16 / 500 .................................................. 0:00:59.728124\n",
      "17 / 500 .................................................. 0:00:51.091396\n",
      "18 / 500 .................................................. 0:00:39.194885\n",
      "19 / 500 .................................................. 0:00:57.971072\n",
      "20 / 500 .................................................. 0:00:52.736725\n",
      "21 / 500 .................................................. 0:01:13.314163\n",
      "22 / 500 .................................................. 0:00:30.696255\n",
      "23 / 500 .................................................. 0:00:49.459774\n",
      "24 / 500 .................................................. 0:01:03.081233\n",
      "25 / 500 .................................................. 0:00:56.263294\n",
      "26 / 500 .................................................. 0:00:56.238239\n",
      "27 / 500 .................................................. 0:00:49.381689\n",
      "28 / 500 .................................................. 0:00:59.646321\n",
      "29 / 500 .................................................. 0:01:03.013073\n",
      "30 / 500 .................................................. 0:01:13.344605\n",
      "31 / 500 .................................................. 0:00:57.953738\n",
      "32 / 500 .................................................. 0:00:20.418852\n",
      "33 / 500 .................................................. 0:01:01.276828\n",
      "34 / 500 .................................................. 0:01:06.550700\n",
      "35 / 500 .................................................. 0:00:34.030754\n",
      "36 / 500 .................................................. 0:01:01.418917\n",
      "37 / 500 .................................................. 0:00:44.192440\n",
      "38 / 500 .................................................. 0:00:59.694200\n",
      "39 / 500 .................................................. 0:00:57.906182\n",
      "40 / 500 .................................................. 0:00:44.219149\n",
      "41 / 500 .................................................. 0:01:14.918583\n",
      "42 / 500 .................................................. 0:00:49.411629\n",
      "43 / 500 .................................................. 0:00:51.227286\n",
      "44 / 500 .................................................. 0:00:57.971567\n",
      "45 / 500 .................................................. 0:00:35.827273\n",
      "46 / 500 .................................................. 0:00:51.112888\n",
      "47 / 500 .................................................. 0:00:51.149159\n",
      "48 / 500 .................................................. 0:00:47.723902\n",
      "49 / 500 .................................................. 0:00:46.003814\n",
      "50 / 500 .................................................. 0:01:11.529028\n",
      "51 / 500 .................................................. 0:00:57.933156\n",
      "52 / 500 .................................................. 0:00:59.631053\n",
      "53 / 500 .................................................. 0:01:01.307411\n",
      "54 / 500 .................................................. 0:00:51.178267\n",
      "55 / 500 .................................................. 0:00:59.621498\n",
      "56 / 500 .................................................. 0:00:57.908862\n",
      "57 / 500 .................................................. 0:00:47.686098\n",
      "58 / 500 .................................................. 0:00:40.934811\n",
      "59 / 500 .................................................. 0:00:47.750712\n",
      "60 / 500 .................................................. 0:00:57.908855\n",
      "61 / 500 .................................................. 0:01:09.791535\n",
      "62 / 500 .................................................. 0:00:57.983043\n",
      "63 / 500 .................................................. 0:00:54.554986\n",
      "64 / 500 .................................................. 0:00:42.594392\n",
      "65 / 500 .................................................. 0:00:51.036358\n",
      "66 / 500 .................................................. 0:00:52.887171\n",
      "67 / 500 .................................................. 0:00:25.539096\n",
      "68 / 500 .................................................. 0:00:46.005958\n",
      "69 / 500 .................................................. 0:01:04.812144\n",
      "70 / 500 .................................................. 0:00:35.729806\n",
      "71 / 500 .................................................. 0:00:49.324167\n",
      "72 / 500 .................................................. 0:00:59.716252\n",
      "73 / 500 .................................................. 0:00:51.075364\n",
      "74 / 500 .................................................. 0:00:44.309707\n",
      "75 / 500 .................................................. 0:00:56.160011\n",
      "76 / 500 .................................................. 0:00:44.216511\n",
      "77 / 500 .................................................. 0:01:01.366432\n",
      "78 / 500 .................................................. 0:00:57.882905\n",
      "79 / 500 .................................................. 0:01:30.351869\n",
      "80 / 500 .................................................. 0:00:47.660316\n",
      "81 / 500 .................................................. 0:00:18.751400\n",
      "82 / 500 .................................................. 0:01:01.417951\n",
      "83 / 500 .................................................. 0:00:42.592608\n",
      "84 / 500 .................................................. 0:02:30.101279\n",
      "85 / 500 .................................................. 0:00:39.188256\n",
      "86 / 500 .................................................. 0:00:52.782056\n",
      "87 / 500 .................................................. 0:01:01.323353\n",
      "88 / 500 .................................................. 0:00:51.041740\n",
      "89 / 500 .................................................. 0:00:57.949290\n",
      "90 / 500 .................................................. 0:00:54.491314\n",
      "91 / 500 .................................................. 0:00:54.578359\n",
      "92 / 500 .................................................. 0:00:49.392170\n",
      "93 / 500 .................................................. 0:00:40.829861\n",
      "94 / 500 .................................................. 0:00:52.912142\n",
      "95 / 500 .................................................. 0:01:01.432651\n",
      "96 / 500 .................................................. 0:01:06.470390\n",
      "97 / 500 .................................................. 0:01:26.965020\n",
      "98 / 500 .................................................. 0:01:08.144295\n",
      "99 / 500 .................................................. 0:00:34.075829\n",
      "100 / 500 .................................................. 0:00:57.847769\n",
      "101 / 500 .................................................. 0:01:04.832234\n",
      "102 / 500 .................................................. 0:00:56.296087\n",
      "103 / 500 .................................................. 0:00:54.547712\n",
      "104 / 500 .................................................. 0:00:59.582853\n",
      "105 / 500 .................................................. 0:00:52.880088\n",
      "106 / 500 .................................................. 0:00:47.746549\n",
      "107 / 500 .................................................. 0:00:27.231249\n",
      "108 / 500 .................................................. 0:00:23.874323\n",
      "109 / 500 .................................................. 0:00:44.316068\n",
      "110 / 500 .................................................. 0:00:42.592785\n",
      "111 / 500 .................................................. 0:00:59.692328\n",
      "112 / 500 .................................................. 0:01:01.349962\n",
      "113 / 500 .................................................. 0:01:03.124319\n",
      "114 / 500 .................................................. 0:00:42.604214\n",
      "115 / 500 .................................................. 0:01:03.162410\n",
      "116 / 500 .................................................. 0:00:39.201600\n",
      "117 / 500 .................................................. 0:01:50.876737\n",
      "118 / 500 .................................................. 0:00:57.910460\n",
      "119 / 500 .................................................. 0:00:52.817279\n",
      "120 / 500 .................................................. 0:00:34.091873\n",
      "121 / 500 .................................................. 0:00:57.989515\n",
      "122 / 500 .................................................. 0:00:47.713424\n",
      "123 / 500 .................................................. 0:00:57.856082\n",
      "124 / 500 .................................................. 0:00:56.252363\n",
      "125 / 500 .................................................. 0:00:25.440661\n",
      "126 / 500 .................................................. 0:01:20.052247\n",
      "127 / 500 .................................................. 0:00:54.456339\n",
      "128 / 500 .................................................. 0:01:08.208599\n",
      "129 / 500 .................................................. 0:00:56.306338\n",
      "130 / 500 .................................................. 0:01:04.707571\n",
      "131 / 500 .................................................. 0:00:49.425025\n",
      "132 / 500 .................................................. 0:01:20.113124\n",
      "133 / 500 .................................................. 0:01:03.054074\n",
      "134 / 500 .................................................. 0:00:56.196941\n",
      "135 / 500 .................................................. 0:00:58.007170\n",
      "136 / 500 .................................................. 0:00:37.518008\n",
      "137 / 500 .................................................. 0:00:44.235650\n",
      "138 / 500 .................................................. 0:01:02.974938\n",
      "139 / 500 .................................................. 0:00:45.963444\n",
      "140 / 500 .................................................. 0:00:54.506888\n",
      "141 / 500 .................................................. 0:01:03.081724\n",
      "142 / 500 .................................................. 0:00:52.856195\n",
      "143 / 500 .................................................. 0:00:42.552402\n",
      "144 / 500 .................................................. 0:01:04.782186\n",
      "145 / 500 .................................................. 0:00:59.602648\n",
      "146 / 500 .................................................. 0:00:42.484927\n",
      "147 / 500 .................................................. 0:00:39.244013\n",
      "148 / 500 .................................................. 0:00:32.310485\n",
      "149 / 500 .................................................. 0:01:01.269428\n",
      "150 / 500 .................................................. 0:00:06.759122\n",
      "151 / 500 .................................................. 0:01:03.046403\n",
      "152 / 500 .................................................. 0:00:37.439939\n",
      "153 / 500 .................................................. 0:00:35.753974\n",
      "154 / 500 .................................................. 0:00:32.418609\n",
      "155 / 500 .................................................. 0:00:08.539571\n",
      "156 / 500 .................................................. 0:01:18.459398\n",
      "157 / 500 .................................................. 0:00:59.652251\n",
      "158 / 500 .................................................. 0:01:21.840802\n",
      "159 / 500 .................................................. 0:00:54.596063\n",
      "160 / 500 .................................................. 0:00:37.438446\n",
      "161 / 500 .................................................. 0:00:52.905096\n",
      "162 / 500 .................................................. 0:00:49.339037\n",
      "163 / 500 .................................................. 0:00:59.671112\n",
      "164 / 500 .................................................. 0:00:54.597865\n",
      "165 / 500 .................................................. 0:00:44.343564\n",
      "166 / 500 .................................................. 0:01:06.442467\n",
      "167 / 500 .................................................. 0:00:44.324237\n",
      "168 / 500 .................................................. 0:00:59.618393\n",
      "169 / 500 .................................................. 0:01:42.363841\n",
      "170 / 500 .................................................. 0:00:46.024430\n",
      "171 / 500 .................................................. 0:00:51.086671\n",
      "172 / 500 .................................................. 0:00:45.967004\n",
      "173 / 500 .................................................. 0:00:40.860242\n",
      "174 / 500 .................................................. 0:00:47.716337\n",
      "175 / 500 .................................................. 0:01:01.360670\n",
      "176 / 500 .................................................. 0:00:59.591263\n",
      "177 / 500 .................................................. 0:01:09.833108\n",
      "178 / 500 .................................................. 0:00:52.783567\n",
      "179 / 500 .................................................. 0:00:49.362480\n",
      "180 / 500 .................................................. 0:01:04.774179\n",
      "181 / 500 .................................................. 0:00:57.964635\n",
      "182 / 500 .................................................. 0:00:57.936821\n",
      "183 / 500 .................................................. 0:00:52.804759\n",
      "184 / 500 .................................................. 0:00:54.551244\n",
      "185 / 500 .................................................. 0:01:04.764279\n",
      "186 / 500 .................................................. 0:00:46.009380\n",
      "187 / 500 .................................................. 0:00:27.236486\n",
      "188 / 500 .................................................. 0:00:37.456843\n",
      "189 / 500 .................................................. 0:00:47.700830\n",
      "190 / 500 .................................................. 0:01:01.434639\n",
      "191 / 500 .................................................. 0:00:47.654897\n",
      "192 / 500 .................................................. 0:01:11.577333\n",
      "193 / 500 .................................................. 0:01:13.351096\n",
      "194 / 500 .................................................. 0:00:49.468778\n",
      "195 / 500 .................................................. 0:00:56.283457\n",
      "196 / 500 .................................................. 0:00:47.680255\n",
      "197 / 500 .................................................. 0:01:20.119961\n",
      "198 / 500 .................................................. 0:00:54.528674\n",
      "199 / 500 .................................................. 0:00:39.157696\n",
      "200 / 500 .................................................. 0:00:47.714352\n",
      "201 / 500 .................................................. 0:01:25.411872\n",
      "202 / 500 .................................................. 0:01:27.074527\n",
      "203 / 500 .................................................. 0:00:27.259820\n",
      "204 / 500 .................................................. 0:00:59.752877\n",
      "205 / 500 .................................................. 0:00:57.911731\n",
      "206 / 500 .................................................. 0:01:21.841314\n",
      "207 / 500 .................................................. 0:00:39.241212\n",
      "208 / 500 .................................................. 0:00:47.734960\n",
      "209 / 500 .................................................. 0:01:11.655685\n",
      "210 / 500 .................................................. 0:01:11.574253\n",
      "211 / 500 .................................................. 0:00:54.547298\n",
      "212 / 500 .................................................. 0:00:51.220287\n",
      "213 / 500 .................................................. 0:01:09.932552\n",
      "214 / 500 .................................................. 0:00:45.976120\n",
      "215 / 500 .................................................. 0:01:06.404428\n",
      "216 / 500 .................................................. 0:00:49.450605\n",
      "217 / 500 .................................................. 0:00:49.455954\n",
      "218 / 500 .................................................. 0:00:52.853713\n",
      "219 / 500 .................................................. 0:00:42.549404\n",
      "220 / 500 .................................................. 0:00:27.252012\n",
      "221 / 500 .................................................. 0:01:01.324135\n",
      "222 / 500 .................................................. 0:00:39.278277\n",
      "223 / 500 .................................................. 0:00:57.970111\n",
      "224 / 500 .................................................. 0:00:37.405283\n",
      "225 / 500 .................................................. 0:00:56.277230\n",
      "226 / 500 .................................................. 0:01:01.359310\n",
      "227 / 500 .................................................. 0:00:44.386623\n",
      "228 / 500 .................................................. 0:01:09.886547\n",
      "229 / 500 .................................................. 0:01:23.589010\n",
      "230 / 500 .................................................. 0:01:04.876797\n",
      "231 / 500 .................................................. 0:00:52.801940\n",
      "232 / 500 .................................................. 0:01:37.058063\n",
      "233 / 500 .................................................. 0:00:40.946905\n",
      "234 / 500 .................................................. 0:01:01.249989\n",
      "235 / 500 .................................................. 0:00:42.609558\n",
      "236 / 500 .................................................. 0:00:40.959331\n",
      "237 / 500 .................................................. 0:00:40.881745\n",
      "238 / 500 .................................................. 0:01:01.404695\n",
      "239 / 500 .................................................. 0:00:59.603938\n",
      "240 / 500 .................................................. 0:00:49.458640\n",
      "241 / 500 .................................................. 0:00:56.277364\n",
      "242 / 500 .................................................. 0:00:54.523331\n",
      "243 / 500 .................................................. 0:00:51.071645\n",
      "244 / 500 .................................................. 0:00:42.648865\n",
      "245 / 500 .................................................. 0:00:54.473438\n",
      "246 / 500 .................................................. 0:00:44.228081\n",
      "247 / 500 .................................................. 0:00:39.164518\n",
      "248 / 500 .................................................. 0:00:57.920083\n",
      "249 / 500 .................................................. 0:00:47.768182\n",
      "250 / 500 .................................................. 0:01:04.748742\n",
      "251 / 500 .................................................. 0:00:56.302034\n",
      "252 / 500 .................................................. 0:00:46.011852\n",
      "253 / 500 .................................................. 0:00:27.139807\n",
      "254 / 500 .................................................. 0:00:52.756648\n",
      "255 / 500 .................................................. 0:01:08.111510\n",
      "256 / 500 .................................................. 0:00:56.242011\n",
      "257 / 500 .................................................. 0:01:04.718437\n",
      "258 / 500 .................................................. 0:00:35.685036\n",
      "259 / 500 .................................................. 0:00:52.838765\n",
      "260 / 500 .................................................. 0:00:59.645074\n",
      "261 / 500 .................................................. 0:01:09.898751\n",
      "262 / 500 .................................................. 0:00:54.531226\n",
      "263 / 500 .................................................. 0:00:44.268530\n",
      "264 / 500 .................................................. 0:01:06.550019\n",
      "265 / 500 .................................................. 0:00:57.932112\n",
      "266 / 500 .................................................. 0:00:59.691399\n",
      "267 / 500 .................................................. 0:00:34.066370\n",
      "268 / 500 .................................................. 0:01:23.596832\n",
      "269 / 500 .................................................. 0:00:51.055767\n",
      "270 / 500 .................................................. 0:00:45.997297\n",
      "271 / 500 .................................................. 0:00:57.923566\n",
      "272 / 500 .................................................. 0:01:59.330391\n",
      "273 / 500 .................................................. 0:00:56.196683\n",
      "274 / 500 .................................................. 0:00:56.248960\n",
      "275 / 500 .................................................. 0:01:13.349639\n",
      "276 / 500 .................................................. 0:00:49.381679\n",
      "277 / 500 .................................................. 0:01:04.639851\n",
      "278 / 500 .................................................. 0:00:06.813915\n",
      "279 / 500 .................................................. 0:00:11.811856\n",
      "280 / 500 .................................................. 0:00:59.561869\n",
      "281 / 500 .................................................. 0:00:49.376450\n",
      "282 / 500 .................................................. 0:00:51.052186\n",
      "283 / 500 .................................................. 0:00:44.264906\n",
      "284 / 500 .................................................. 0:01:45.678887\n",
      "285 / 500 .................................................. 0:01:04.826974\n",
      "286 / 500 .................................................. 0:00:49.310627\n",
      "287 / 500 .................................................. 0:00:57.877945\n",
      "288 / 500 .................................................. 0:01:08.064830\n",
      "289 / 500 .................................................. 0:00:54.551574\n",
      "290 / 500 .................................................. 0:00:56.257596\n",
      "291 / 500 .................................................. 0:00:56.266252\n",
      "292 / 500 .................................................. 0:01:03.030447\n",
      "293 / 500 .................................................. 0:01:06.478952\n",
      "294 / 500 .................................................. 0:00:47.681794\n",
      "295 / 500 .................................................. 0:01:01.314256\n",
      "296 / 500 .................................................. 0:00:51.152931\n",
      "297 / 500 .................................................. 0:00:57.948184\n",
      "298 / 500 .................................................. 0:00:54.600503\n",
      "299 / 500 .................................................. 0:00:57.999811\n",
      "300 / 500 .................................................. 0:00:49.391114\n",
      "301 / 500 .................................................. 0:00:52.817554\n",
      "302 / 500 .................................................. 0:00:35.777766\n",
      "303 / 500 .................................................. 0:00:47.641948\n",
      "304 / 500 .................................................. 0:00:46.070976\n",
      "305 / 500 .................................................. 0:00:52.787855\n",
      "306 / 500 .................................................. 0:01:11.696979\n",
      "307 / 500 .................................................. 0:00:54.570420\n",
      "308 / 500 .................................................. 0:01:23.452712\n",
      "309 / 500 .................................................. 0:00:30.669170\n",
      "310 / 500 .................................................. 0:00:34.116564\n",
      "311 / 500 .................................................. 0:00:39.226452\n",
      "312 / 500 .................................................. 0:01:03.079845\n",
      "313 / 500 .................................................. 0:00:58.006954\n",
      "314 / 500 .................................................. 0:00:54.436073\n",
      "315 / 500 .................................................. 0:00:49.496770\n",
      "316 / 500 .................................................. 0:00:30.618614\n",
      "317 / 500 .................................................. 0:01:13.324137\n",
      "318 / 500 .................................................. 0:00:57.916560\n",
      "319 / 500 .................................................. 0:00:52.900043\n",
      "320 / 500 .................................................. 0:00:18.668498\n",
      "321 / 500 .................................................. 0:01:06.409905\n",
      "322 / 500 .................................................. 0:00:32.324738\n",
      "323 / 500 .................................................. 0:01:15.035395\n",
      "324 / 500 .................................................. 0:00:56.259029\n",
      "325 / 500 .................................................. 0:00:54.443585\n",
      "326 / 500 .................................................. 0:00:32.409997\n",
      "327 / 500 .................................................. 0:00:46.036801\n",
      "328 / 500 .................................................. 0:01:02.990235\n",
      "329 / 500 .................................................. 0:00:25.502150\n",
      "330 / 500 .................................................. 0:00:49.453557\n",
      "331 / 500 .................................................. 0:00:56.216520\n",
      "332 / 500 .................................................. 0:00:45.991309\n",
      "333 / 500 .................................................. 0:00:39.217571\n",
      "334 / 500 .................................................. 0:01:13.331513\n",
      "335 / 500 .................................................. 0:01:01.379156\n",
      "336 / 500 .................................................. 0:00:39.130079\n",
      "337 / 500 .................................................. 0:00:56.310487\n",
      "338 / 500 .................................................. 0:00:23.833345\n",
      "339 / 500 .................................................. 0:01:03.042189\n",
      "340 / 500 .................................................. 0:00:30.747139\n",
      "341 / 500 .................................................. 0:01:08.188046\n",
      "342 / 500 .................................................. 0:01:08.101486\n",
      "343 / 500 .................................................. 0:00:39.264076\n",
      "344 / 500 .................................................. 0:00:52.787511\n",
      "345 / 500 .................................................. 0:01:25.164714\n",
      "346 / 500 .................................................. 0:00:54.561123\n",
      "347 / 500 .................................................. 0:01:04.773745\n",
      "348 / 500 .................................................. 0:01:09.823611\n",
      "349 / 500 .................................................. 0:01:01.373698\n",
      "350 / 500 .................................................. 0:00:44.313883\n",
      "351 / 500 .................................................. 0:00:40.811598\n",
      "352 / 500 .................................................. 0:00:47.682845\n",
      "353 / 500 .................................................. 0:00:59.716400\n",
      "354 / 500 .................................................. 0:00:52.860907\n",
      "355 / 500 .................................................. 0:00:23.841649\n",
      "356 / 500 .................................................. 0:00:35.835855\n",
      "357 / 500 .................................................. 0:00:44.207056\n",
      "358 / 500 .................................................. 0:00:47.637678\n",
      "359 / 500 .................................................. 0:01:04.696438\n",
      "360 / 500 .................................................. 0:00:52.813652\n",
      "361 / 500 .................................................. 0:00:52.904683\n",
      "362 / 500 .................................................. 0:00:51.252067\n",
      "363 / 500 .................................................. 0:01:15.032993\n",
      "364 / 500 .................................................. 0:01:13.349289\n",
      "365 / 500 .................................................. 0:00:51.162029\n",
      "366 / 500 .................................................. 0:00:37.470700\n",
      "367 / 500 .................................................. 0:00:47.788158\n",
      "368 / 500 .................................................. 0:01:20.260046\n",
      "369 / 500 .................................................. 0:00:37.531668\n",
      "370 / 500 .................................................. 0:00:39.226608\n",
      "371 / 500 .................................................. 0:01:03.099021\n",
      "372 / 500 .................................................. 0:00:52.771252\n",
      "373 / 500 .................................................. 0:00:52.931160\n",
      "374 / 500 .................................................. 0:00:37.492784\n",
      "375 / 500 .................................................. 0:00:39.226265\n",
      "376 / 500 .................................................. 0:01:06.454586\n",
      "377 / 500 .................................................. 0:00:49.461737\n",
      "378 / 500 .................................................. 0:00:56.262737\n",
      "379 / 500 .................................................. 0:00:54.525067\n",
      "380 / 500 .................................................. 0:00:51.139377\n",
      "381 / 500 .................................................. 0:00:11.892135\n",
      "382 / 500 .................................................. 0:01:01.357786\n",
      "383 / 500 .................................................. 0:00:47.695857\n",
      "384 / 500 .................................................. 0:01:04.717563\n",
      "385 / 500 .................................................. 0:00:52.800858\n",
      "386 / 500 .................................................. 0:00:57.855685\n",
      "387 / 500 .................................................. 0:01:04.799938\n",
      "388 / 500 .................................................. 0:00:42.611827\n",
      "389 / 500 .................................................. 0:00:18.719124\n",
      "390 / 500 .................................................. 0:01:04.807621\n",
      "391 / 500 .................................................. 0:00:51.191287\n",
      "392 / 500 .................................................. 0:00:49.419785\n",
      "393 / 500 .................................................. 0:00:56.205387\n",
      "394 / 500 .................................................. 0:00:47.771053\n",
      "395 / 500 .................................................. 0:01:33.818467\n",
      "396 / 500 .................................................. 0:00:44.349495\n",
      "397 / 500 .................................................. 0:00:29.008510\n",
      "398 / 500 .................................................. 0:00:40.840791\n",
      "399 / 500 .................................................. 0:00:52.826254\n",
      "400 / 500 .................................................. 0:00:56.225557\n",
      "401 / 500 .................................................. 0:01:04.742455\n",
      "402 / 500 .................................................. 0:01:01.334476\n",
      "403 / 500 .................................................. 0:00:59.626606\n",
      "404 / 500 .................................................. 0:00:49.475097\n",
      "405 / 500 .................................................. 0:00:47.655408\n",
      "406 / 500 .................................................. 0:00:54.591820\n",
      "407 / 500 .................................................. 0:01:11.554561\n",
      "408 / 500 .................................................. 0:00:35.772496\n",
      "409 / 500 .................................................. 0:01:09.940244\n",
      "410 / 500 .................................................. 0:00:52.850612\n",
      "411 / 500 .................................................. 0:01:01.401777\n",
      "412 / 500 .................................................. 0:00:39.122175\n",
      "413 / 500 .................................................. 0:00:49.515873\n",
      "414 / 500 .................................................. 0:01:04.810907\n",
      "415 / 500 .................................................. 0:01:04.704867\n",
      "416 / 500 .................................................. 0:00:49.432223\n",
      "417 / 500 .................................................. 0:01:03.104317\n",
      "418 / 500 .................................................. 0:00:35.841359\n",
      "419 / 500 .................................................. 0:00:35.731637\n",
      "420 / 500 .................................................. 0:00:58.000438\n",
      "421 / 500 .................................................. 0:01:28.690893\n",
      "422 / 500 .................................................. 0:00:52.804474\n",
      "423 / 500 .................................................. 0:00:47.740217\n",
      "424 / 500 .................................................. 0:00:51.139359\n",
      "425 / 500 .................................................. 0:00:51.094855\n",
      "426 / 500 .................................................. 0:00:51.078115\n",
      "427 / 500 .................................................. 0:00:49.350442\n",
      "428 / 500 .................................................. 0:00:44.306296\n",
      "429 / 500 .................................................. 0:01:16.628702\n",
      "430 / 500 .................................................. 0:00:49.415573\n",
      "431 / 500 .................................................. 0:01:08.141191\n",
      "432 / 500 .................................................. 0:00:44.237141\n",
      "433 / 500 .................................................. 0:00:16.974825\n",
      "434 / 500 .................................................. 0:00:23.767627\n",
      "435 / 500 .................................................. 0:01:01.408143\n",
      "436 / 500 .................................................. 0:01:01.358582\n",
      "437 / 500 .................................................. 0:00:52.861261\n",
      "438 / 500 .................................................. 0:01:20.106728\n",
      "439 / 500 .................................................. 0:01:08.164883\n",
      "440 / 500 .................................................. 0:00:49.500672\n",
      "441 / 500 .................................................. 0:01:08.068016\n",
      "442 / 500 .................................................. 0:00:52.748069\n",
      "443 / 500 .................................................. 0:00:57.887772\n",
      "444 / 500 .................................................. 0:00:57.961443\n",
      "445 / 500 .................................................. 0:00:47.789063\n",
      "446 / 500 .................................................. 0:00:35.842169\n",
      "447 / 500 .................................................. 0:01:09.808556\n",
      "448 / 500 .................................................. 0:00:59.700603\n",
      "449 / 500 .................................................. 0:01:06.506620\n",
      "450 / 500 .................................................. 0:00:56.288466\n",
      "451 / 500 .................................................. 0:00:59.553465\n",
      "452 / 500 .................................................. 0:00:51.075754\n",
      "453 / 500 .................................................. 0:00:35.829160\n",
      "454 / 500 .................................................. 0:00:51.027527\n",
      "455 / 500 .................................................. 0:00:57.876994\n",
      "456 / 500 .................................................. 0:01:04.874427\n",
      "457 / 500 .................................................. 0:01:11.666980\n",
      "458 / 500 .................................................. 0:00:59.695654\n",
      "459 / 500 .................................................. 0:00:40.885942\n",
      "460 / 500 .................................................. 0:00:44.323484\n",
      "461 / 500 .................................................. 0:00:46.072421\n",
      "462 / 500 .................................................. 0:00:52.817167\n",
      "463 / 500 .................................................. 0:01:21.947499\n",
      "464 / 500 .................................................. 0:01:02.991742\n",
      "465 / 500 .................................................. 0:00:57.985993\n",
      "466 / 500 .................................................. 0:01:25.187796\n",
      "467 / 500 .................................................. 0:00:37.490609\n",
      "468 / 500 .................................................. 0:00:37.530053\n",
      "469 / 500 .................................................. 0:01:06.483495\n",
      "470 / 500 .................................................. 0:01:03.115833\n",
      "471 / 500 .................................................. 0:00:58.020747\n",
      "472 / 500 .................................................. 0:01:33.784629\n",
      "473 / 500 .................................................. 0:00:39.130237\n",
      "474 / 500 .................................................. 0:00:42.641423\n",
      "475 / 500 .................................................. 0:00:59.714824\n",
      "476 / 500 .................................................. 0:00:57.865560\n",
      "477 / 500 .................................................. 0:01:16.759805\n",
      "478 / 500 .................................................. 0:01:04.808430\n",
      "479 / 500 .................................................. 0:01:01.372078\n",
      "480 / 500 .................................................. 0:00:57.860829\n",
      "481 / 500 .................................................. 0:01:03.087257\n",
      "482 / 500 .................................................. 0:00:51.135728\n",
      "483 / 500 .................................................. 0:00:58.004300\n",
      "484 / 500 .................................................. 0:00:52.798243\n",
      "485 / 500 .................................................. 0:00:54.498803\n",
      "486 / 500 .................................................. 0:00:08.491112\n",
      "487 / 500 .................................................. 0:00:51.182194\n",
      "488 / 500 .................................................. 0:00:08.529635\n",
      "489 / 500 .................................................. 0:00:23.871978\n",
      "490 / 500 .................................................. 0:00:52.900583\n",
      "491 / 500 .................................................. 0:00:57.931414\n",
      "492 / 500 .................................................. 0:01:08.206138\n",
      "493 / 500 .................................................. 0:00:57.938750\n",
      "494 / 500 .................................................. 0:00:39.101181\n",
      "495 / 500 .................................................. 0:01:18.477228\n",
      "496 / 500 .................................................. 0:00:52.905409\n",
      "497 / 500 .................................................. 0:01:28.609274\n",
      "498 / 500 .................................................. 0:00:57.918479\n",
      "499 / 500 .................................................. 0:00:28.966760\n",
      "CPU times: user 6h 32min 10s, sys: 0 ns, total: 6h 32min 10s\n",
      "Wall time: 7h 30min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from random import shuffle\n",
    "from datetime import datetime\n",
    "\n",
    "def get_predictions(author_models, test_texts, test_labels):\n",
    "    \"\"\"Evaluate each text for each author_model and append first metric to predictions\"\"\"\n",
    "    indicies = list(range(len(test_texts)))\n",
    "\n",
    "    test_texts = np.array(test_texts)\n",
    "    test_labels = np.array(test_labels)\n",
    "\n",
    "    test_texts = test_texts[indicies]\n",
    "    test_labels = test_labels[indicies]\n",
    "\n",
    "    predictions = []\n",
    "    for i, text in enumerate(test_texts):\n",
    "        t1 = datetime.now()\n",
    "        print(\"{} / {}\".format(i, len(test_texts)), end=\" \")\n",
    "        X, y = vectorize(clean_text(text, charset))\n",
    "\n",
    "        losses = []\n",
    "        for am in author_models:\n",
    "            print(\".\", end=\"\")\n",
    "            model = am[0]\n",
    "            label = am[1]\n",
    "            loss = model.evaluate(X, y, verbose=0)\n",
    "            losses.append((loss, label))\n",
    "        print(\" {}\".format(datetime.now() - t1))\n",
    "        predictions.append(losses)\n",
    "    return predictions\n",
    "    \n",
    "\n",
    "predictions = get_predictions(author_models, c50_test_texts[:500], c50_test_labels[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LydiaZajc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(2.0694980791109634, 'BernardHickey'),\n",
       " (2.2756346526898836, 'JimGilchrist'),\n",
       " (1.9072724757917894, 'HeatherScoffield'),\n",
       " (2.1537862226320863, 'AlanCrosby'),\n",
       " (2.1100960619309368, 'RobinSidel'),\n",
       " (1.7867581442044616, 'DarrenSchuettler'),\n",
       " (2.0638196387158088, 'EricAuchard'),\n",
       " (2.0492954010564843, 'MichaelConnor'),\n",
       " (2.1467253969918834, 'WilliamKazer'),\n",
       " (2.0326404962746349, 'GrahamEarnshaw'),\n",
       " (2.0347747551767448, 'KevinDrawbaugh'),\n",
       " (2.0896197546377269, 'KevinMorrison'),\n",
       " (2.2370971673782396, 'KouroshKarimkhany'),\n",
       " (2.2285476653568517, 'MatthewBunce'),\n",
       " (2.1947651979723952, 'JaneMacartney'),\n",
       " (2.1129373765951338, 'TheresePoletti'),\n",
       " (2.1946548352669635, \"LynneO'Donnell\"),\n",
       " (2.1010171747060014, 'MarkBendeich'),\n",
       " (2.2557203732776938, 'MarcelMichelson'),\n",
       " (2.1211482880654349, 'JohnMastrini'),\n",
       " (2.154840397391895, 'TimFarrand'),\n",
       " (2.0737582707183648, 'MartinWolk'),\n",
       " (2.085022070828606, 'JonathanBirt'),\n",
       " (2.1425213341373408, 'EdnaFernandes'),\n",
       " (2.1110903235042797, 'PierreTran'),\n",
       " (2.2651851118164537, 'RogerFillion'),\n",
       " (2.2588206075662431, 'TanEeLyn'),\n",
       " (1.6146654698870868, 'LydiaZajc'),\n",
       " (2.1343845398433436, 'NickLouth'),\n",
       " (2.067864031983603, 'KirstinRidley'),\n",
       " (2.2540766757339146, 'BenjaminKangLim'),\n",
       " (2.1329571226432966, 'KeithWeir'),\n",
       " (2.1143036922065099, 'JanLopatka'),\n",
       " (2.1967348343816706, 'ScottHillis'),\n",
       " (2.0497028303589246, 'LynnleyBrowning'),\n",
       " (2.0600182976885106, 'SamuelPerry'),\n",
       " (2.3085135660673441, 'PeterHumphrey'),\n",
       " (2.1813734072280742, 'MureDickie'),\n",
       " (2.2538659078048848, 'JoWinterbottom'),\n",
       " (1.9996070788002605, 'BradDorfman'),\n",
       " (2.0978950775075624, 'FumikoFujisaki'),\n",
       " (2.2645119919496426, 'DavidLawder'),\n",
       " (2.1242105377716913, 'ToddNissen'),\n",
       " (2.0428217335751184, 'AlexanderSmith'),\n",
       " (2.1329591071273519, 'PatriciaCommins'),\n",
       " (2.1078265148788784, 'SimonCowell'),\n",
       " (2.2940573352778291, 'AaronPressman'),\n",
       " (2.2479639953873107, 'KarlPenhaul'),\n",
       " (2.0074652975926828, 'JoeOrtiz'),\n",
       " (1.9358724433202124, 'SarahDavison')]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(c50_test_labels[0])\n",
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "c50_test_labels[1]\n",
    "\n",
    "author_index = {pred[1]: i for i, pred in enumerate(predictions[0])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2.0831662245701761, 'BernardHickey'), (2.2198001456879672, 'JimGilchrist'), (2.0750580340917342, 'HeatherScoffield'), (2.1432573063200673, 'AlanCrosby'), (1.9998730573679788, 'RobinSidel'), (1.9823461254473338, 'DarrenSchuettler'), (1.6140351901656298, 'EricAuchard'), (1.9735047365793077, 'MichaelConnor'), (2.0324745573096759, 'WilliamKazer'), (2.0248399109528763, 'GrahamEarnshaw'), (1.9383127999924716, 'KevinDrawbaugh'), (1.9436475790923691, 'KevinMorrison'), (1.6844351953337422, 'KouroshKarimkhany'), (2.1069769634129005, 'MatthewBunce'), (2.0418894876831861, 'JaneMacartney'), (1.5810203720868008, 'TheresePoletti'), (2.1662888173664268, \"LynneO'Donnell\"), (2.0859753987355858, 'MarkBendeich'), (1.9605231668229817, 'MarcelMichelson'), (2.0734674589059772, 'JohnMastrini'), (2.0973696851687658, 'TimFarrand'), (1.6761696850922669, 'MartinWolk'), (2.0534447117799504, 'JonathanBirt'), (2.0293640977266971, 'EdnaFernandes'), (2.0233807946916338, 'PierreTran'), (1.9543541267500888, 'RogerFillion'), (2.1294531535120447, 'TanEeLyn'), (2.1900976997672603, 'LydiaZajc'), (1.8451996097206109, 'NickLouth'), (1.8581434344220824, 'KirstinRidley'), (2.2149012058893272, 'BenjaminKangLim'), (1.9704761180689854, 'KeithWeir'), (2.1522608932471212, 'JanLopatka'), (2.0760062261682166, 'ScottHillis'), (2.0160573618478188, 'LynnleyBrowning'), (1.5879657721028546, 'SamuelPerry'), (2.1528017015038801, 'PeterHumphrey'), (2.0007654794541021, 'MureDickie'), (2.0852172460103526, 'JoWinterbottom'), (1.9703584659707898, 'BradDorfman'), (2.0772074711781694, 'FumikoFujisaki'), (2.1418116017975972, 'DavidLawder'), (1.9335724453682759, 'ToddNissen'), (1.96275101309924, 'AlexanderSmith'), (1.9712577284503003, 'PatriciaCommins'), (2.036120366708658, 'SimonCowell'), (1.9276635047038324, 'AaronPressman'), (2.2253562810704874, 'KarlPenhaul'), (2.0383160438725429, 'JoeOrtiz'), (1.9993571956561096, 'SarahDavison')]\n",
      "(1.5810203720868008, 'TheresePoletti')\n",
      "MartinWolk\n"
     ]
    }
   ],
   "source": [
    "print(predictions[3])\n",
    "print(min(predictions[3]))\n",
    "print(c50_test_labels[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "author_index\n",
    "index_author = {author_index[author]: author for author in author_index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no unique mode; found 500 equally common values\n",
      "1.99349083611\n",
      "no unique mode; found 500 equally common values\n",
      "2.09272587334\n",
      "no unique mode; found 500 equally common values\n",
      "2.0044997065\n",
      "no unique mode; found 500 equally common values\n",
      "2.06208831822\n",
      "no unique mode; found 500 equally common values\n",
      "2.01058155494\n",
      "no unique mode; found 500 equally common values\n",
      "1.9532321233\n",
      "no unique mode; found 500 equally common values\n",
      "1.94649277314\n",
      "no unique mode; found 500 equally common values\n",
      "1.98714570273\n",
      "no unique mode; found 500 equally common values\n",
      "1.92850433424\n",
      "no unique mode; found 500 equally common values\n",
      "1.98429591956\n",
      "no unique mode; found 500 equally common values\n",
      "1.95621899949\n",
      "no unique mode; found 500 equally common values\n",
      "1.95305268386\n",
      "no unique mode; found 500 equally common values\n",
      "2.05843863892\n",
      "no unique mode; found 500 equally common values\n",
      "2.02130407747\n",
      "no unique mode; found 500 equally common values\n",
      "1.92922557104\n",
      "no unique mode; found 500 equally common values\n",
      "1.95382156184\n",
      "no unique mode; found 500 equally common values\n",
      "2.00870317585\n",
      "no unique mode; found 500 equally common values\n",
      "1.96449052234\n",
      "no unique mode; found 500 equally common values\n",
      "1.98590169735\n",
      "no unique mode; found 500 equally common values\n",
      "1.95938179521\n",
      "no unique mode; found 500 equally common values\n",
      "2.00625388921\n",
      "no unique mode; found 500 equally common values\n",
      "1.96706134591\n",
      "no unique mode; found 500 equally common values\n",
      "1.95983907525\n",
      "no unique mode; found 500 equally common values\n",
      "1.95848050115\n",
      "no unique mode; found 500 equally common values\n",
      "1.95588757514\n",
      "no unique mode; found 500 equally common values\n",
      "2.01409758275\n",
      "no unique mode; found 500 equally common values\n",
      "1.98337506953\n",
      "no unique mode; found 500 equally common values\n",
      "2.11545624248\n",
      "no unique mode; found 500 equally common values\n",
      "1.95045408976\n",
      "no unique mode; found 500 equally common values\n",
      "1.92064306133\n",
      "no unique mode; found 500 equally common values\n",
      "2.02282561135\n",
      "no unique mode; found 500 equally common values\n",
      "1.93690677579\n",
      "no unique mode; found 500 equally common values\n",
      "2.01413409249\n",
      "no unique mode; found 500 equally common values\n",
      "1.94553067014\n",
      "no unique mode; found 500 equally common values\n",
      "1.96219691338\n",
      "no unique mode; found 500 equally common values\n",
      "1.91782724021\n",
      "no unique mode; found 500 equally common values\n",
      "2.02300231883\n",
      "no unique mode; found 500 equally common values\n",
      "1.94803361344\n",
      "no unique mode; found 500 equally common values\n",
      "1.98633201813\n",
      "no unique mode; found 500 equally common values\n",
      "1.93521103454\n",
      "no unique mode; found 500 equally common values\n",
      "1.99018866094\n",
      "no unique mode; found 500 equally common values\n",
      "2.0996188137\n",
      "no unique mode; found 500 equally common values\n",
      "1.98440796832\n",
      "no unique mode; found 500 equally common values\n",
      "1.8894542127\n",
      "no unique mode; found 500 equally common values\n",
      "1.98173553004\n",
      "no unique mode; found 500 equally common values\n",
      "1.95621332477\n",
      "no unique mode; found 500 equally common values\n",
      "2.03505550141\n",
      "no unique mode; found 500 equally common values\n",
      "2.04181142906\n",
      "no unique mode; found 500 equally common values\n",
      "1.9507079679\n",
      "no unique mode; found 500 equally common values\n",
      "1.87069775225\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean, mode\n",
    "model_means = []\n",
    "er = 0\n",
    "for i in range(50):\n",
    "    try:\n",
    "        means = mode([p[i][0] for p in predictions])\n",
    "        model_means.append(means)\n",
    "    except Exception as e:\n",
    "        er += 1\n",
    "        print(e)\n",
    "        print(mean([p[i][0] for p in predictions]))\n",
    "print(er)\n",
    "\n",
    "pred_is = []\n",
    "for pred in predictions:\n",
    "    # pred_i = [p[0] - model_means[i] for i, p in enumerate(pred)]\n",
    "    pred_i = [p[0] for p in pred]\n",
    "    pred_is.append(pred_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "foo = [p[3][0] for p in predictions][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.1537862226320863, 2.3319116932342232, 2.2289197248852783, 2.1432573063200673, 2.1117479712232012, 1.8893041892147278, 2.169575651258016, 2.0724795832830165, 2.0525566351274489, 2.1720845848592352]\n"
     ]
    },
    {
     "ename": "StatisticsError",
     "evalue": "no unique mode; found 10 equally common values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStatisticsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-174-b75831a7152f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatistics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfoo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfoo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.4/statistics.py\u001b[0m in \u001b[0;36mmode\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m         raise StatisticsError(\n\u001b[0;32m--> 434\u001b[0;31m                 \u001b[0;34m'no unique mode; found %d equally common values'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m                 )\n\u001b[1;32m    436\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStatisticsError\u001b[0m: no unique mode; found 10 equally common values"
     ]
    }
   ],
   "source": [
    "from statistics import mode\n",
    "print(foo)\n",
    "print(mode(foo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.9934908361106956,\n",
       " 2.0927258733422343,\n",
       " 2.0044997065007077,\n",
       " 2.0620883182184224,\n",
       " 2.0105815549435544,\n",
       " 1.953232123302568,\n",
       " 1.9464927731370281,\n",
       " 1.9871457027280786,\n",
       " 1.9285043342358887,\n",
       " 1.9842959195572087]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_means[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pred_labs = [np.argmin(pred) for pred in pred_is]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pred_labs = [index_author[i] for i in pred_labs]\n",
    "# pred_labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66000000000000003"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(c50_test_labels[:500], pred_labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['LydiaZajc', 'MureDickie', 'RobinSidel', 'MartinWolk',\n",
       "       'KirstinRidley', 'PierreTran', 'NickLouth', \"LynneO'Donnell\",\n",
       "       'BenjaminKangLim', 'FumikoFujisaki'],\n",
       "      dtype='<U17')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c50_test_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27, 37, 4, 15, 29, 24, 28, 16, 30, 40]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_texts, train_labels = get_all_c50(c50_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "word_vec = TfidfVectorizer(min_df=3, ngram_range=(1,2))\n",
    "char_vec = TfidfVectorizer(min_df=3, ngram_range=(2,5))\n",
    "\n",
    "fu = FeatureUnion([\n",
    "    ('word', word_vec),\n",
    "    ('char', char_vec)\n",
    "])\n",
    "\n",
    "\n",
    "X_train = fu.fit_transform(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_test = fu.transform(c50_test_texts[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.fit(X_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "preds = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72399999999999998"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(c50_test_labels[:500], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_test_longer = fu.transform(longer_test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "preds = svm.predict(X_test_longer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "accuracy_score(longer_test_labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X, y = vectorize(clean_text(train_texts[3], charset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_author_model.layers[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14725 samples, validate on 1637 samples\n",
      "Epoch 1/13\n",
      "14725/14725 [==============================] - 18s - loss: 3.1623 - val_loss: 3.9301\n",
      "Epoch 2/13\n",
      "14725/14725 [==============================] - 15s - loss: 2.4849 - val_loss: 3.2724\n",
      "Epoch 3/13\n",
      "14725/14725 [==============================] - 15s - loss: 2.2303 - val_loss: 2.7735\n",
      "Epoch 4/13\n",
      "14725/14725 [==============================] - 15s - loss: 2.0664 - val_loss: 2.3324\n",
      "Epoch 5/13\n",
      "14725/14725 [==============================] - 15s - loss: 1.9487 - val_loss: 2.0202\n",
      "Epoch 6/13\n",
      "14725/14725 [==============================] - 15s - loss: 1.8679 - val_loss: 1.9499\n",
      "Epoch 7/13\n",
      "14725/14725 [==============================] - 15s - loss: 1.7817 - val_loss: 1.9287\n",
      "Epoch 8/13\n",
      "14725/14725 [==============================] - 15s - loss: 1.6960 - val_loss: 1.9165\n",
      "Epoch 9/13\n",
      "14725/14725 [==============================] - 15s - loss: 1.6509 - val_loss: 1.9007\n",
      "Epoch 10/13\n",
      "14725/14725 [==============================] - 15s - loss: 1.5811 - val_loss: 1.8963\n",
      "Epoch 11/13\n",
      "14725/14725 [==============================] - 15s - loss: 1.5102 - val_loss: 1.8982\n",
      "Epoch 12/13\n",
      "14725/14725 [==============================] - 15s - loss: 1.4347 - val_loss: 1.9258\n",
      "Epoch 13/13\n",
      "14725/14725 [==============================] - 15s - loss: 1.3818 - val_loss: 1.9303\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nLSTM, BatchNorm\\nTrain on 14929 samples, validate on 1659 samples\\nEpoch 1/5\\n14929/14929 [==============================] - 75s - loss: 3.0403 - val_loss: 3.4757\\nEpoch 2/5\\n14929/14929 [==============================] - 60s - loss: 2.3156 - val_loss: 2.9687\\n\\nLSTM\\nTrain on 14929 samples, validate on 1659 samples\\nEpoch 1/5\\n14929/14929 [==============================] - 75s - loss: 3.1259 - val_loss: 2.7865\\nEpoch 2/5\\n14929/14929 [==============================] - 60s - loss: 2.6150 - val_loss: 2.3894\\n\\nCNN(5), LSTM  # faster, needs more epochs\\nTrain on 14929 samples, validate on 1659 samples\\nEpoch 1/5\\n14929/14929 [==============================] - 42s - loss: 3.1579 - val_loss: 2.9987\\nEpoch 2/5\\n14929/14929 [==============================] - 26s - loss: 2.8874 - val_loss: 2.6994\\nEpoch 3/5\\n14929/14929 [==============================] - 26s - loss: 2.6220 - val_loss: 2.4879\\nEpoch 4/5\\n14929/14929 [==============================] - 26s - loss: 2.4309 - val_loss: 2.3942\\nEpoch 5/5\\n14929/14929 [==============================] - 26s - loss: 2.2950 - val_loss: 2.2902\\n\\nCNN(5), CNN(3), LSTM doesn't drop below 3.0 in 5 epochs\\n\\n\\nEmbedding, BatchNorm, GRU, BatchNorm\\nTrain on 14725 samples, validate on 1637 samples\\nEpoch 1/20\\n14725/14725 [==============================] - 16s - loss: 2.9240 - val_loss: 3.9516\\nEpoch 2/20\\n14725/14725 [==============================] - 15s - loss: 2.2447 - val_loss: 3.3667\\nEpoch 3/20\\n14725/14725 [==============================] - 15s - loss: 2.0054 - val_loss: 2.8011\\nEpoch 4/20\\n14725/14725 [==============================] - 15s - loss: 1.8388 - val_loss: 2.3477\\nEpoch 5/20\\n14725/14725 [==============================] - 15s - loss: 1.7122 - val_loss: 2.0196\\nEpoch 6/20\\n14725/14725 [==============================] - 15s - loss: 1.6069 - val_loss: 1.9417\\nEpoch 7/20\\n14725/14725 [==============================] - 15s - loss: 1.5044 - val_loss: 1.9541\\nEpoch 8/20\\n14725/14725 [==============================] - 15s - loss: 1.3987 - val_loss: 1.9512\\nEpoch 9/20\\n14725/14725 [==============================] - 15s - loss: 1.2940 - val_loss: 1.9921\\nEpoch 10/20\\n14725/14725 [==============================] - 15s - loss: 1.1850 - val_loss: 2.0424\\n\\nTrain on 14725 samples, validate on 1637 samples\\nEpoch 1/20\\n14725/14725 [==============================] - 17s - loss: 3.2577 - val_loss: 3.8892\\nEpoch 2/20\\n14725/14725 [==============================] - 15s - loss: 2.5444 - val_loss: 3.2364\\nEpoch 3/20\\n14725/14725 [==============================] - 15s - loss: 2.2915 - val_loss: 2.7205\\nEpoch 4/20\\n14725/14725 [==============================] - 15s - loss: 2.1258 - val_loss: 2.3477\\nEpoch 5/20\\n14725/14725 [==============================] - 15s - loss: 2.0219 - val_loss: 2.0588\\nEpoch 6/20\\n14725/14725 [==============================] - 15s - loss: 1.9386 - val_loss: 1.9478\\nEpoch 7/20\\n14725/14725 [==============================] - 15s - loss: 1.8762 - val_loss: 1.9152\\nEpoch 8/20\\n14725/14725 [==============================] - 15s - loss: 1.7994 - val_loss: 1.9076\\nEpoch 9/20\\n14725/14725 [==============================] - 15s - loss: 1.7224 - val_loss: 1.8829\\nEpoch 10/20\\n14725/14725 [==============================] - 15s - loss: 1.6783 - val_loss: 1.9007\\nEpoch 11/20\\n14725/14725 [==============================] - 15s - loss: 1.6154 - val_loss: 1.8910\\nEpoch 12/20\\n14725/14725 [==============================] - 15s - loss: 1.5485 - val_loss: 1.8920\\nEpoch 13/20\\n14725/14725 [==============================] - 15s - loss: 1.4844 - val_loss: 1.9198\\nEpoch 14/20\\n14725/14725 [==============================] - 15s - loss: 1.4291 - val_loss: 1.9193\\nEpoch 15/20\\n14725/14725 [==============================] - 15s - loss: 1.3670 - val_loss: 1.9295\\nEpoch 16/20\\n14725/14725 [==============================] - 15s - loss: 1.3028 - val_loss: 1.9752\\nEpoch 17/20\\n14725/14725 [==============================] - 15s - loss: 1.2321 - val_loss: 1.9969\\nEpoch 18/20\\n14725/14725 [==============================] - 15s - loss: 1.2159 - val_loss: 2.0431\\nEpoch 19/20\\n14725/14725 [==============================] - 15s - loss: 1.1391 - val_loss: 2.0709\\nEpoch 20/20\\n14725/14725 [==============================] - 15s - loss: 1.1321 - val_loss: 2.1172\\n\\n\\nmodel = Sequential()\\nmodel.add(Embedding(input_dim=len(charset), output_dim=100))\\nmodel.add(BatchNormalization())\\nmodel.add(GRU(256))\\nmodel.add(Dropout(0.5))\\nmodel.add(BatchNormalization())\\nmodel.add(Dense(y.shape[1], activation='softmax'))\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\\nmodel.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\\nTrain on 14725 samples, validate on 1637 samples\\nEpoch 1/20\\n14725/14725 [==============================] - 17s - loss: 3.2577 - val_loss: 3.8892\\nEpoch 2/20\\n14725/14725 [==============================] - 15s - loss: 2.5444 - val_loss: 3.2364\\nEpoch 3/20\\n14725/14725 [==============================] - 15s - loss: 2.2915 - val_loss: 2.7205\\nEpoch 4/20\\n14725/14725 [==============================] - 15s - loss: 2.1258 - val_loss: 2.3477\\nEpoch 5/20\\n14725/14725 [==============================] - 15s - loss: 2.0219 - val_loss: 2.0588\\nEpoch 6/20\\n14725/14725 [==============================] - 15s - loss: 1.9386 - val_loss: 1.9478\\nEpoch 7/20\\n14725/14725 [==============================] - 15s - loss: 1.8762 - val_loss: 1.9152\\nEpoch 8/20\\n14725/14725 [==============================] - 15s - loss: 1.7994 - val_loss: 1.9076\\nEpoch 9/20\\n14725/14725 [==============================] - 15s - loss: 1.7224 - val_loss: 1.8829\\nEpoch 10/20\\n14725/14725 [==============================] - 15s - loss: 1.6783 - val_loss: 1.9007\\nEpoch 11/20\\n14725/14725 [==============================] - 15s - loss: 1.6154 - val_loss: 1.8910\\nEpoch 12/20\\n14725/14725 [==============================] - 15s - loss: 1.5485 - val_loss: 1.8920\\nEpoch 13/20\\n14725/14725 [==============================] - 15s - loss: 1.4844 - val_loss: 1.9198\\nEpoch 14/20\\n14725/14725 [==============================] - 15s - loss: 1.4291 - val_loss: 1.9193\\nEpoch 15/20\\n14725/14725 [==============================] - 15s - loss: 1.3670 - val_loss: 1.9295\\nEpoch 16/20\\n14725/14725 [==============================] - 15s - loss: 1.3028 - val_loss: 1.9752\\nEpoch 17/20\\n14725/14725 [==============================] - 15s - loss: 1.2321 - val_loss: 1.9969\\nEpoch 18/20\\n14725/14725 [==============================] - 15s - loss: 1.2159 - val_loss: 2.0431\\nEpoch 19/20\\n14725/14725 [==============================] - 15s - loss: 1.1391 - val_loss: 2.0709\\nEpoch 20/20\\n14725/14725 [==============================] - 15s - loss: 1.1321 - val_loss: 2.1172\\n\\n\\nmodel = Sequential()\\nmodel.add(Embedding(input_dim=len(charset), output_dim=100))\\nmodel.add(GRU(256))\\nmodel.add(BatchNormalization())\\nmodel.add(Dropout(0.5))\\nmodel.add(Dense(y.shape[1], activation='softmax'))\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\\nmodel.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\\nTrain on 14725 samples, validate on 1637 samples\\nEpoch 1/20\\n14725/14725 [==============================] - 16s - loss: 3.1731 - val_loss: 3.5648\\nEpoch 2/20\\n14725/14725 [==============================] - 14s - loss: 2.4964 - val_loss: 2.9875\\nEpoch 3/20\\n14725/14725 [==============================] - 14s - loss: 2.3446 - val_loss: 2.7695\\nEpoch 4/20\\n14725/14725 [==============================] - 14s - loss: 2.2928 - val_loss: 2.6010\\nEpoch 5/20\\n14725/14725 [==============================] - 14s - loss: 2.2642 - val_loss: 2.3900\\nEpoch 6/20\\n14725/14725 [==============================] - 14s - loss: 2.2373 - val_loss: 2.5023\\nEpoch 7/20\\n14725/14725 [==============================] - 14s - loss: 2.2186 - val_loss: 2.3780\\nEpoch 8/20\\n14725/14725 [==============================] - 14s - loss: 2.2029 - val_loss: 2.4928\\nEpoch 9/20\\n14725/14725 [==============================] - 14s - loss: 2.1852 - val_loss: 2.3480\\nEpoch 10/20\\n14725/14725 [==============================] - 14s - loss: 2.1745 - val_loss: 2.4801\\nEpoch 11/20\\n14725/14725 [==============================] - 14s - loss: 2.1563 - val_loss: 2.3951\\nEpoch 12/20\\n14725/14725 [==============================] - 14s - loss: 2.1391 - val_loss: 2.4133\\nEpoch 13/20\\n14725/14725 [==============================] - 14s - loss: 2.1192 - val_loss: 2.5896\\nEpoch 14/20\\n14725/14725 [==============================] - 14s - loss: 2.1020 - val_loss: 2.2692\\nEpoch 15/20\\n14725/14725 [==============================] - 14s - loss: 2.0770 - val_loss: 2.2179\\nEpoch 16/20\\n14725/14725 [==============================] - 14s - loss: 2.0643 - val_loss: 2.2822\\nEpoch 17/20\\n 6784/14725 [============>.................] - ETA: 7s - loss: 2.0302\\n\\n\\nmodel = Sequential()\\nmodel.add(Embedding(input_dim=len(charset), output_dim=100))\\nmodel.add(Dropout(0.2))\\nmodel.add(BatchNormalization())\\nmodel.add(GRU(256))\\nmodel.add(Dropout(0.2))\\nmodel.add(BatchNormalization())\\nmodel.add(Dense(y.shape[1], activation='softmax'))\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\\nmodel.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\\nTrain on 14725 samples, validate on 1637 samples\\nEpoch 1/20\\n14725/14725 [==============================] - 17s - loss: 3.1528 - val_loss: 3.9042\\nEpoch 2/20\\n14725/14725 [==============================] - 15s - loss: 2.4658 - val_loss: 3.2093\\nEpoch 3/20\\n14725/14725 [==============================] - 15s - loss: 2.2001 - val_loss: 2.6764\\nEpoch 4/20\\n14725/14725 [==============================] - 15s - loss: 2.0358 - val_loss: 2.2919\\nEpoch 5/20\\n14725/14725 [==============================] - 15s - loss: 1.9497 - val_loss: 2.0060\\nEpoch 6/20\\n14725/14725 [==============================] - 15s - loss: 1.8588 - val_loss: 1.9313\\nEpoch 7/20\\n14725/14725 [==============================] - 15s - loss: 1.7883 - val_loss: 1.9153\\nEpoch 8/20\\n14725/14725 [==============================] - 15s - loss: 1.7034 - val_loss: 1.9145\\nEpoch 9/20\\n14725/14725 [==============================] - 15s - loss: 1.6382 - val_loss: 1.8979\\nEpoch 10/20\\n14725/14725 [==============================] - 15s - loss: 1.5827 - val_loss: 1.8864\\nEpoch 11/20\\n14725/14725 [==============================] - 15s - loss: 1.5093 - val_loss: 1.8967\\nEpoch 12/20\\n14725/14725 [==============================] - 15s - loss: 1.4472 - val_loss: 1.9040\\nEpoch 13/20\\n14725/14725 [==============================] - 15s - loss: 1.3809 - val_loss: 1.9227\\nEpoch 14/20\\n14725/14725 [==============================] - 15s - loss: 1.3225 - val_loss: 1.9469\\nEpoch 15/20\\n14725/14725 [==============================] - 15s - loss: 1.2516 - val_loss: 1.9862\\nEpoch 16/20\\n14725/14725 [==============================] - 15s - loss: 1.2094 - val_loss: 1.9963\\nEpoch 17/20\\n14725/14725 [==============================] - 15s - loss: 1.1658 - val_loss: 2.0331\\nEpoch 18/20\\n14725/14725 [==============================] - 15s - loss: 1.0851 - val_loss: 2.0452\\nEpoch 19/20\\n14725/14725 [==============================] - 15s - loss: 1.0394 - val_loss: 2.0810\\nEpoch 20/20\\n14725/14725 [==============================] - 15s - loss: 0.9903 - val_loss: 2.1283\\n\\n\\nmodel = Sequential()\\nmodel.add(Embedding(input_dim=len(charset), output_dim=100))\\nmodel.add(Dropout(0.3))\\nmodel.add(BatchNormalization())\\nmodel.add(GRU(256))\\nmodel.add(Dropout(0.3))\\nmodel.add(BatchNormalization())\\nmodel.add(Dense(y.shape[1], activation='softmax'))\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\\nmodel.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\\nTrain on 14725 samples, validate on 1637 samples\\nEpoch 1/20\\n14725/14725 [==============================] - 17s - loss: 3.2991 - val_loss: 3.8902\\nEpoch 2/20\\n14725/14725 [==============================] - 15s - loss: 2.5672 - val_loss: 3.1627\\nEpoch 3/20\\n14725/14725 [==============================] - 15s - loss: 2.2731 - val_loss: 2.6340\\nEpoch 4/20\\n14725/14725 [==============================] - 15s - loss: 2.1316 - val_loss: 2.2594\\nEpoch 5/20\\n14725/14725 [==============================] - 15s - loss: 2.0249 - val_loss: 2.0159\\nEpoch 6/20\\n14725/14725 [==============================] - 15s - loss: 1.9571 - val_loss: 1.9456\\nEpoch 7/20\\n14725/14725 [==============================] - 15s - loss: 1.8789 - val_loss: 1.9213\\nEpoch 8/20\\n14725/14725 [==============================] - 15s - loss: 1.8233 - val_loss: 1.8924\\nEpoch 9/20\\n14725/14725 [==============================] - 15s - loss: 1.7575 - val_loss: 1.8987\\n\\n\\n\\nmodel = Sequential()\\nmodel.add(Embedding(input_dim=len(charset), output_dim=100))\\nmodel.add(Dropout(0.5))\\nmodel.add(BatchNormalization())\\nmodel.add(GRU(256))\\nmodel.add(Dropout(0.5))\\nmodel.add(BatchNormalization())\\nmodel.add(Dense(y.shape[1], activation='softmax'))\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\\nmodel.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\\nTrain on 14725 samples, validate on 1637 samples\\nEpoch 1/20\\n14725/14725 [==============================] - 17s - loss: 3.6036 - val_loss: 3.7924\\nEpoch 2/20\\n14725/14725 [==============================] - 15s - loss: 2.7765 - val_loss: 3.0022\\nEpoch 3/20\\n14725/14725 [==============================] - 15s - loss: 2.4773 - val_loss: 2.5697\\nEpoch 4/20\\n14725/14725 [==============================] - 15s - loss: 2.3218 - val_loss: 2.2606\\nEpoch 5/20\\n14725/14725 [==============================] - 15s - loss: 2.2328 - val_loss: 2.0832\\nEpoch 6/20\\n14725/14725 [==============================] - 15s - loss: 2.1748 - val_loss: 2.0248\\nEpoch 7/20\\n14725/14725 [==============================] - 15s - loss: 2.1174 - val_loss: 1.9865\\nEpoch 8/20\\n14725/14725 [==============================] - 15s - loss: 2.0617 - val_loss: 1.9640\\nEpoch 9/20\\n14725/14725 [==============================] - 15s - loss: 2.0206 - val_loss: 1.9461\\nEpoch 10/20\\n14725/14725 [==============================] - 15s - loss: 1.9758 - val_loss: 1.9334\\nEpoch 11/20\\n14725/14725 [==============================] - 15s - loss: 1.9546 - val_loss: 1.9148\\nEpoch 12/20\\n14725/14725 [==============================] - 15s - loss: 1.9045 - val_loss: 1.9121\\nEpoch 13/20\\n14725/14725 [==============================] - 15s - loss: 1.8757 - val_loss: 1.8888\\nEpoch 14/20\\n14725/14725 [==============================] - 15s - loss: 1.8437 - val_loss: 1.8874\\nEpoch 15/20\\n14725/14725 [==============================] - 15s - loss: 1.8145 - val_loss: 1.8822\\nEpoch 16/20\\n14725/14725 [==============================] - 15s - loss: 1.7805 - val_loss: 1.8785\\nEpoch 17/20\\n14725/14725 [==============================] - 15s - loss: 1.7558 - val_loss: 1.8868\\nEpoch 18/20\\n14725/14725 [==============================] - 15s - loss: 1.7218 - val_loss: 1.8670\\nEpoch 19/20\\n14725/14725 [==============================] - 15s - loss: 1.7032 - val_loss: 1.8759\\nEpoch 20/20\\n14725/14725 [==============================] - 15s - loss: 1.6832 - val_loss: 1.8834\\n\\n\\nmodel = Sequential()\\nmodel.add(Embedding(input_dim=len(charset), output_dim=100))\\nmodel.add(Dropout(0.5))\\nmodel.add(BatchNormalization())\\nmodel.add(GRU(256, return_sequences=True))\\nmodel.add(Dropout(0.5))\\nmodel.add(BatchNormalization())\\nmodel.add(GRU(256))\\nmodel.add(Dropout(0.5))\\nmodel.add(BatchNormalization())\\nmodel.add(Dense(y.shape[1], activation='softmax'))\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\\nmodel.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\\nTrain on 14725 samples, validate on 1637 samples\\nEpoch 1/20\\n14725/14725 [==============================] - 33s - loss: 3.7814 - val_loss: 3.7282\\nEpoch 2/20\\n14725/14725 [==============================] - 29s - loss: 2.8453 - val_loss: 2.8542\\nEpoch 3/20\\n14725/14725 [==============================] - 29s - loss: 2.5178 - val_loss: 2.4434\\nEpoch 4/20\\n14725/14725 [==============================] - 29s - loss: 2.3762 - val_loss: 2.1894\\nEpoch 5/20\\n14725/14725 [==============================] - 29s - loss: 2.2896 - val_loss: 2.0862\\nEpoch 6/20\\n14725/14725 [==============================] - 29s - loss: 2.2254 - val_loss: 2.0516\\nEpoch 7/20\\n14725/14725 [==============================] - 29s - loss: 2.1565 - val_loss: 2.0133\\nEpoch 8/20\\n14725/14725 [==============================] - 29s - loss: 2.1132 - val_loss: 1.9992\\nEpoch 9/20\\n14725/14725 [==============================] - 29s - loss: 2.0798 - val_loss: 1.9881\\nEpoch 10/20\\n14725/14725 [==============================] - 29s - loss: 2.0509 - val_loss: 1.9784\\nEpoch 11/20\\n14725/14725 [==============================] - 29s - loss: 2.0198 - val_loss: 1.9618\\nEpoch 12/20\\n14725/14725 [==============================] - 29s - loss: 1.9822 - val_loss: 1.9383\\nEpoch 13/20\\n14725/14725 [==============================] - 29s - loss: 1.9437 - val_loss: 1.9300\\nEpoch 14/20\\n14725/14725 [==============================] - 29s - loss: 1.9198 - val_loss: 1.9163\\nEpoch 15/20\\n14725/14725 [==============================] - 29s - loss: 1.8989 - val_loss: 1.9160\\nEpoch 16/20\\n14725/14725 [==============================] - 29s - loss: 1.8866 - val_loss: 1.9085\\nEpoch 17/20\\n14725/14725 [==============================] - 29s - loss: 1.8493 - val_loss: 1.8965\\nEpoch 18/20\\n14725/14725 [==============================] - 29s - loss: 1.8248 - val_loss: 1.8878\\nEpoch 19/20\\n14725/14725 [==============================] - 29s - loss: 1.8037 - val_loss: 1.8870\\nEpoch 20/20\\n14725/14725 [==============================] - 29s - loss: 1.7724 - val_loss: 1.8862\\n\\n\\nmodel = Sequential()\\nmodel.add(Embedding(input_dim=len(charset), output_dim=100))\\nmodel.add(Dropout(0.2))\\nmodel.add(BatchNormalization())\\nmodel.add(GRU(256, return_sequences=True))\\nmodel.add(Dropout(0.2))\\nmodel.add(BatchNormalization())\\nmodel.add(GRU(256))\\nmodel.add(Dropout(0.2))\\nmodel.add(BatchNormalization())\\nmodel.add(Dense(y.shape[1], activation='softmax'))\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\\nmodel.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\\nTrain on 14725 samples, validate on 1637 samples\\nEpoch 1/20\\n14725/14725 [==============================] - 32s - loss: 3.2809 - val_loss: 3.7595\\nEpoch 2/20\\n14725/14725 [==============================] - 29s - loss: 2.4379 - val_loss: 2.9869\\nEpoch 3/20\\n14725/14725 [==============================] - 29s - loss: 2.1504 - val_loss: 2.5361\\nEpoch 4/20\\n14725/14725 [==============================] - 29s - loss: 1.9887 - val_loss: 2.1294\\nEpoch 5/20\\n14725/14725 [==============================] - 29s - loss: 1.8984 - val_loss: 1.9727\\nEpoch 6/20\\n14725/14725 [==============================] - 29s - loss: 1.7892 - val_loss: 1.9264\\nEpoch 7/20\\n14725/14725 [==============================] - 29s - loss: 1.7172 - val_loss: 1.9100\\nEpoch 8/20\\n14725/14725 [==============================] - 29s - loss: 1.6361 - val_loss: 1.9124\\nEpoch 9/20\\n14725/14725 [==============================] - 29s - loss: 1.5621 - val_loss: 1.9122\\nEpoch 10/20\\n14725/14725 [==============================] - 29s - loss: 1.4863 - val_loss: 1.9045\\nEpoch 11/20\\n14725/14725 [==============================] - 29s - loss: 1.4150 - val_loss: 1.9278\\nEpoch 12/20\\n14725/14725 [==============================] - 29s - loss: 1.3691 - val_loss: 1.9181\\nEpoch 13/20\\n14725/14725 [==============================] - 29s - loss: 1.2970 - val_loss: 1.9414\\nEpoch 14/20\\n 1536/14725 [==>...........................] - ETA: 25s - loss: 1.1475\\n \\n \\n \\nmodel = Sequential()\\nmodel.add(Embedding(input_dim=len(charset), output_dim=100))\\nmodel.add(BatchNormalization())\\nmodel.add(GRU(256))\\nmodel.add(Dropout(0.4))\\nmodel.add(BatchNormalization())\\nmodel.add(Dense(y.shape[1], activation='softmax'))\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\\nmodel.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\\n Train on 14725 samples, validate on 1637 samples\\nEpoch 1/20\\n14725/14725 [==============================] - 17s - loss: 3.1787 - val_loss: 3.9282\\nEpoch 2/20\\n14725/14725 [==============================] - 15s - loss: 2.5070 - val_loss: 3.2479\\nEpoch 3/20\\n14725/14725 [==============================] - 15s - loss: 2.2191 - val_loss: 2.7522\\nEpoch 4/20\\n14725/14725 [==============================] - 15s - loss: 2.0637 - val_loss: 2.3331\\nEpoch 5/20\\n14725/14725 [==============================] - 15s - loss: 1.9539 - val_loss: 2.0326\\nEpoch 6/20\\n14725/14725 [==============================] - 15s - loss: 1.8622 - val_loss: 1.9440\\nEpoch 7/20\\n14725/14725 [==============================] - 15s - loss: 1.7821 - val_loss: 1.9166\\nEpoch 8/20\\n14725/14725 [==============================] - 15s - loss: 1.7169 - val_loss: 1.8996\\nEpoch 9/20\\n14725/14725 [==============================] - 15s - loss: 1.6561 - val_loss: 1.8849\\nEpoch 10/20\\n14725/14725 [==============================] - 15s - loss: 1.5910 - val_loss: 1.9032\\nEpoch 11/20\\n14725/14725 [==============================] - 15s - loss: 1.5082 - val_loss: 1.8878\\nEpoch 12/20\\n14725/14725 [==============================] - 15s - loss: 1.4513 - val_loss: 1.9252\\nEpoch 13/20\\n 7552/14725 [==============>...............] - ETA: 7s - loss: 1.3534\\n \\n \\nmodel = Sequential()\\nmodel.add(Embedding(input_dim=len(charset), output_dim=100))\\nmodel.add(BatchNormalization())\\nmodel.add(GRU(512))\\nmodel.add(Dropout(0.4))\\nmodel.add(BatchNormalization())\\nmodel.add(Dense(y.shape[1], activation='softmax'))\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\\nmodel.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\\n Train on 14725 samples, validate on 1637 samples\\nEpoch 1/20\\n14725/14725 [==============================] - 23s - loss: 3.1173 - val_loss: 3.8415\\nEpoch 2/20\\n14725/14725 [==============================] - 20s - loss: 2.4202 - val_loss: 3.1512\\nEpoch 3/20\\n14725/14725 [==============================] - 20s - loss: 2.1302 - val_loss: 2.7191\\nEpoch 4/20\\n14725/14725 [==============================] - 20s - loss: 1.9435 - val_loss: 2.3341\\nEpoch 5/20\\n14725/14725 [==============================] - 20s - loss: 1.7966 - val_loss: 1.9877\\nEpoch 6/20\\n14725/14725 [==============================] - 20s - loss: 1.6621 - val_loss: 1.9349\\nEpoch 7/20\\n14725/14725 [==============================] - 20s - loss: 1.5190 - val_loss: 1.9632\\nEpoch 8/20\\n14725/14725 [==============================] - 20s - loss: 1.3925 - val_loss: 1.9735\\n\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM, Input, Embedding, Conv1D, MaxPooling1D, BatchNormalization, GRU\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# load ascii text and covert to lowercase\n",
    "\n",
    "# cnn = Dropout(0.2)(embedded)\n",
    "# cnn = Conv1D(128, 5, activation='relu')(cnn)\n",
    "# cnn = MaxPooling1D(pool_size=4)(cnn)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(256))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=13, batch_size=128, validation_split=0.1)\n",
    "\n",
    "\"\"\"\n",
    "LSTM, BatchNorm\n",
    "Train on 14929 samples, validate on 1659 samples\n",
    "Epoch 1/5\n",
    "14929/14929 [==============================] - 75s - loss: 3.0403 - val_loss: 3.4757\n",
    "Epoch 2/5\n",
    "14929/14929 [==============================] - 60s - loss: 2.3156 - val_loss: 2.9687\n",
    "\n",
    "LSTM\n",
    "Train on 14929 samples, validate on 1659 samples\n",
    "Epoch 1/5\n",
    "14929/14929 [==============================] - 75s - loss: 3.1259 - val_loss: 2.7865\n",
    "Epoch 2/5\n",
    "14929/14929 [==============================] - 60s - loss: 2.6150 - val_loss: 2.3894\n",
    "\n",
    "CNN(5), LSTM  # faster, needs more epochs\n",
    "Train on 14929 samples, validate on 1659 samples\n",
    "Epoch 1/5\n",
    "14929/14929 [==============================] - 42s - loss: 3.1579 - val_loss: 2.9987\n",
    "Epoch 2/5\n",
    "14929/14929 [==============================] - 26s - loss: 2.8874 - val_loss: 2.6994\n",
    "Epoch 3/5\n",
    "14929/14929 [==============================] - 26s - loss: 2.6220 - val_loss: 2.4879\n",
    "Epoch 4/5\n",
    "14929/14929 [==============================] - 26s - loss: 2.4309 - val_loss: 2.3942\n",
    "Epoch 5/5\n",
    "14929/14929 [==============================] - 26s - loss: 2.2950 - val_loss: 2.2902\n",
    "\n",
    "CNN(5), CNN(3), LSTM doesn't drop below 3.0 in 5 epochs\n",
    "\n",
    "\n",
    "Embedding, BatchNorm, GRU, BatchNorm\n",
    "Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 16s - loss: 2.9240 - val_loss: 3.9516\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.2447 - val_loss: 3.3667\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.0054 - val_loss: 2.8011\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.8388 - val_loss: 2.3477\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7122 - val_loss: 2.0196\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.6069 - val_loss: 1.9417\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.5044 - val_loss: 1.9541\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.3987 - val_loss: 1.9512\n",
    "Epoch 9/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.2940 - val_loss: 1.9921\n",
    "Epoch 10/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.1850 - val_loss: 2.0424\n",
    "\n",
    "Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 17s - loss: 3.2577 - val_loss: 3.8892\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.5444 - val_loss: 3.2364\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.2915 - val_loss: 2.7205\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.1258 - val_loss: 2.3477\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.0219 - val_loss: 2.0588\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.9386 - val_loss: 1.9478\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.8762 - val_loss: 1.9152\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7994 - val_loss: 1.9076\n",
    "Epoch 9/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7224 - val_loss: 1.8829\n",
    "Epoch 10/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.6783 - val_loss: 1.9007\n",
    "Epoch 11/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.6154 - val_loss: 1.8910\n",
    "Epoch 12/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.5485 - val_loss: 1.8920\n",
    "Epoch 13/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.4844 - val_loss: 1.9198\n",
    "Epoch 14/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.4291 - val_loss: 1.9193\n",
    "Epoch 15/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.3670 - val_loss: 1.9295\n",
    "Epoch 16/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.3028 - val_loss: 1.9752\n",
    "Epoch 17/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.2321 - val_loss: 1.9969\n",
    "Epoch 18/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.2159 - val_loss: 2.0431\n",
    "Epoch 19/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.1391 - val_loss: 2.0709\n",
    "Epoch 20/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.1321 - val_loss: 2.1172\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(256))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\n",
    "Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 17s - loss: 3.2577 - val_loss: 3.8892\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.5444 - val_loss: 3.2364\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.2915 - val_loss: 2.7205\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.1258 - val_loss: 2.3477\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.0219 - val_loss: 2.0588\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.9386 - val_loss: 1.9478\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.8762 - val_loss: 1.9152\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7994 - val_loss: 1.9076\n",
    "Epoch 9/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7224 - val_loss: 1.8829\n",
    "Epoch 10/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.6783 - val_loss: 1.9007\n",
    "Epoch 11/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.6154 - val_loss: 1.8910\n",
    "Epoch 12/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.5485 - val_loss: 1.8920\n",
    "Epoch 13/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.4844 - val_loss: 1.9198\n",
    "Epoch 14/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.4291 - val_loss: 1.9193\n",
    "Epoch 15/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.3670 - val_loss: 1.9295\n",
    "Epoch 16/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.3028 - val_loss: 1.9752\n",
    "Epoch 17/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.2321 - val_loss: 1.9969\n",
    "Epoch 18/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.2159 - val_loss: 2.0431\n",
    "Epoch 19/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.1391 - val_loss: 2.0709\n",
    "Epoch 20/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.1321 - val_loss: 2.1172\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "model.add(GRU(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\n",
    "Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 16s - loss: 3.1731 - val_loss: 3.5648\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.4964 - val_loss: 2.9875\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.3446 - val_loss: 2.7695\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.2928 - val_loss: 2.6010\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.2642 - val_loss: 2.3900\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.2373 - val_loss: 2.5023\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.2186 - val_loss: 2.3780\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.2029 - val_loss: 2.4928\n",
    "Epoch 9/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.1852 - val_loss: 2.3480\n",
    "Epoch 10/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.1745 - val_loss: 2.4801\n",
    "Epoch 11/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.1563 - val_loss: 2.3951\n",
    "Epoch 12/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.1391 - val_loss: 2.4133\n",
    "Epoch 13/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.1192 - val_loss: 2.5896\n",
    "Epoch 14/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.1020 - val_loss: 2.2692\n",
    "Epoch 15/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.0770 - val_loss: 2.2179\n",
    "Epoch 16/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.0643 - val_loss: 2.2822\n",
    "Epoch 17/20\n",
    " 6784/14725 [============>.................] - ETA: 7s - loss: 2.0302\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\n",
    "Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 17s - loss: 3.1528 - val_loss: 3.9042\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.4658 - val_loss: 3.2093\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.2001 - val_loss: 2.6764\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.0358 - val_loss: 2.2919\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.9497 - val_loss: 2.0060\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.8588 - val_loss: 1.9313\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7883 - val_loss: 1.9153\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7034 - val_loss: 1.9145\n",
    "Epoch 9/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.6382 - val_loss: 1.8979\n",
    "Epoch 10/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.5827 - val_loss: 1.8864\n",
    "Epoch 11/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.5093 - val_loss: 1.8967\n",
    "Epoch 12/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.4472 - val_loss: 1.9040\n",
    "Epoch 13/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.3809 - val_loss: 1.9227\n",
    "Epoch 14/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.3225 - val_loss: 1.9469\n",
    "Epoch 15/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.2516 - val_loss: 1.9862\n",
    "Epoch 16/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.2094 - val_loss: 1.9963\n",
    "Epoch 17/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.1658 - val_loss: 2.0331\n",
    "Epoch 18/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.0851 - val_loss: 2.0452\n",
    "Epoch 19/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.0394 - val_loss: 2.0810\n",
    "Epoch 20/20\n",
    "14725/14725 [==============================] - 15s - loss: 0.9903 - val_loss: 2.1283\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(256))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\n",
    "Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 17s - loss: 3.2991 - val_loss: 3.8902\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.5672 - val_loss: 3.1627\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.2731 - val_loss: 2.6340\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.1316 - val_loss: 2.2594\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.0249 - val_loss: 2.0159\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.9571 - val_loss: 1.9456\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.8789 - val_loss: 1.9213\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.8233 - val_loss: 1.8924\n",
    "Epoch 9/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7575 - val_loss: 1.8987\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(256))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\n",
    "Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 17s - loss: 3.6036 - val_loss: 3.7924\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.7765 - val_loss: 3.0022\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.4773 - val_loss: 2.5697\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.3218 - val_loss: 2.2606\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.2328 - val_loss: 2.0832\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.1748 - val_loss: 2.0248\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.1174 - val_loss: 1.9865\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.0617 - val_loss: 1.9640\n",
    "Epoch 9/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.0206 - val_loss: 1.9461\n",
    "Epoch 10/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.9758 - val_loss: 1.9334\n",
    "Epoch 11/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.9546 - val_loss: 1.9148\n",
    "Epoch 12/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.9045 - val_loss: 1.9121\n",
    "Epoch 13/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.8757 - val_loss: 1.8888\n",
    "Epoch 14/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.8437 - val_loss: 1.8874\n",
    "Epoch 15/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.8145 - val_loss: 1.8822\n",
    "Epoch 16/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7805 - val_loss: 1.8785\n",
    "Epoch 17/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7558 - val_loss: 1.8868\n",
    "Epoch 18/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7218 - val_loss: 1.8670\n",
    "Epoch 19/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7032 - val_loss: 1.8759\n",
    "Epoch 20/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.6832 - val_loss: 1.8834\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(256, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(256))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\n",
    "Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 33s - loss: 3.7814 - val_loss: 3.7282\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.8453 - val_loss: 2.8542\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.5178 - val_loss: 2.4434\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.3762 - val_loss: 2.1894\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.2896 - val_loss: 2.0862\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.2254 - val_loss: 2.0516\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.1565 - val_loss: 2.0133\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.1132 - val_loss: 1.9992\n",
    "Epoch 9/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.0798 - val_loss: 1.9881\n",
    "Epoch 10/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.0509 - val_loss: 1.9784\n",
    "Epoch 11/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.0198 - val_loss: 1.9618\n",
    "Epoch 12/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.9822 - val_loss: 1.9383\n",
    "Epoch 13/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.9437 - val_loss: 1.9300\n",
    "Epoch 14/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.9198 - val_loss: 1.9163\n",
    "Epoch 15/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.8989 - val_loss: 1.9160\n",
    "Epoch 16/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.8866 - val_loss: 1.9085\n",
    "Epoch 17/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.8493 - val_loss: 1.8965\n",
    "Epoch 18/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.8248 - val_loss: 1.8878\n",
    "Epoch 19/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.8037 - val_loss: 1.8870\n",
    "Epoch 20/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.7724 - val_loss: 1.8862\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\n",
    "Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 32s - loss: 3.2809 - val_loss: 3.7595\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.4379 - val_loss: 2.9869\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.1504 - val_loss: 2.5361\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.9887 - val_loss: 2.1294\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.8984 - val_loss: 1.9727\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.7892 - val_loss: 1.9264\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.7172 - val_loss: 1.9100\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.6361 - val_loss: 1.9124\n",
    "Epoch 9/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.5621 - val_loss: 1.9122\n",
    "Epoch 10/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.4863 - val_loss: 1.9045\n",
    "Epoch 11/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.4150 - val_loss: 1.9278\n",
    "Epoch 12/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.3691 - val_loss: 1.9181\n",
    "Epoch 13/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.2970 - val_loss: 1.9414\n",
    "Epoch 14/20\n",
    " 1536/14725 [==>...........................] - ETA: 25s - loss: 1.1475\n",
    " \n",
    " \n",
    " \n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(256))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\n",
    " Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 17s - loss: 3.1787 - val_loss: 3.9282\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.5070 - val_loss: 3.2479\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.2191 - val_loss: 2.7522\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.0637 - val_loss: 2.3331\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.9539 - val_loss: 2.0326\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.8622 - val_loss: 1.9440\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7821 - val_loss: 1.9166\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7169 - val_loss: 1.8996\n",
    "Epoch 9/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.6561 - val_loss: 1.8849\n",
    "Epoch 10/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.5910 - val_loss: 1.9032\n",
    "Epoch 11/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.5082 - val_loss: 1.8878\n",
    "Epoch 12/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.4513 - val_loss: 1.9252\n",
    "Epoch 13/20\n",
    " 7552/14725 [==============>...............] - ETA: 7s - loss: 1.3534\n",
    " \n",
    " \n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(512))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\n",
    " Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 23s - loss: 3.1173 - val_loss: 3.8415\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 20s - loss: 2.4202 - val_loss: 3.1512\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 20s - loss: 2.1302 - val_loss: 2.7191\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 20s - loss: 1.9435 - val_loss: 2.3341\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 20s - loss: 1.7966 - val_loss: 1.9877\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 20s - loss: 1.6621 - val_loss: 1.9349\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 20s - loss: 1.5190 - val_loss: 1.9632\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 20s - loss: 1.3925 - val_loss: 1.9735\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.fit(X, y, epochs=5, batch_size=128, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_author_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Generating with seed: \"oes it really matter what it says this is some test text does it really matter what it says this is \"\n",
      "oes it really matter what it says this is some test text does it really matter what it says this is "
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.float64' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-af9aa51fe5bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"this is some test text does it really matter what it says \"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-16c3c1fabb4f>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(model, diversity, text)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mnext_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mnext_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mgenerated\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnext_char\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-16c3c1fabb4f>\u001b[0m in \u001b[0;36msample\u001b[0;34m(preds, temperature)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mexp_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_preds\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mprobas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.multinomial (numpy/random/mtrand/mtrand.c:37721)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'numpy.float64' has no len()"
     ]
    }
   ],
   "source": [
    "generate(model, diversity=0.7, text=\"this is some test text does it really matter what it says \" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generate(model, diversity=0.5, text=\"\"):\n",
    "    \"\"\"Generate text from a model\"\"\"\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    for i in range(5000):\n",
    "        x = np.zeros((1, maxlen), dtype=np.int)\n",
    "        for t, char in enumerate(sentence):\n",
    "            try:\n",
    "                x[0, t] = char_indices[char]\n",
    "            except:\n",
    "                print(sentence)\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Generating with seed: \" what it says this is some test text does it really matter what it says this is some test text does \"\n",
      " what it says this is some test text does it really matter what it says this is some test text does the chan store if the store in for Vis a singic and the places I nor for the store in find store is a gitter beat to the trist and the counders here to find. The sele the store with the storit and the chansic I was next the back the truck out. This is a git and the courder the trust and the ching store with the me to good and the next the countret to the this store is little back to the clourder the selsection for the stration's and my for probebles to have the store if friends out befould the chan net store with the park is can sele, I was not longer casteral friends. The store store is little this is this lace in findst and the inclure the selmen store in finds a befter the beet the car this store if the chan strater and closed to the counders. The strail also here in find store with the store with me and selestanding and the strorthors. So I had things a back to the mant I was not look in fried. The store in finds. The mest to the had a leng so this shore sprob food. The lent for the store for the store with the mant and the pricked out a find the chings realing and the mettreer smore with the store is later with got and the counders here for the stort and consorthing. The store store store is littely to get the onlite of the been to this shorth and I was trinks a fead a feas and my next the car the store in find store better here and class. The me of the only convenienter the the cound store. The store store is little but the only Nortar Crip and my ore also head with the things. The me so this shores and class store screet and the fings and long sometime store store is little also this is a back is this is a bittle this was small with the metr seet and my rear to was my sor the counder fead and was counders. The line for the strort and the Chrack is bak in friends and the chan in finds. The strail was come to this store is little bas in finds on the mettranter but the me store is little any the fincers and land store in findlest your for the stort office all park in things around findss. The ment store is fried was converite back the park in finds. The stores store with the counderth and check is this is a good sement and the boxt of the best thing to hand a fead and the me sell this stort of the one of the straf a feat and the chises with the only got for the stroirs and the mark is a firle a feed and the only metter slow with the stricket and my could my dere for real peating. The me something the counders to but the chan store is a Still a was a glass in finds that was conser this store shore for the strorth only regulay same but if my ere fur nite before the bands. The lane I was to gas here and the counder for the store is this is this is a bask in finds and the curont real for the store, The store in findst. The start in friends which when I gound the plation for Crome was strort and a fead all peat for your for the store and the chisies with the manity look in friends. The last in firl also but hive the was next the coure fent and the plattor for your for the store is little back to be. And the me of the stor"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-49291dfdfef3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"this is some test text does it really matter what it says \"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-9c3c76047201>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(model, diversity, text)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mnext_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mnext_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.4/site-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 902\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.4/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1583\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m         return self._predict_loop(f, ins,\n\u001b[0;32m-> 1585\u001b[0;31m                                   batch_size=batch_size, verbose=verbose)\n\u001b[0m\u001b[1;32m   1586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1587\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/.local/lib/python3.4/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1210\u001b[0m                 \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1212\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1213\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.4/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2227\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2228\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2229\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generate(model, diversity=0.5, text=\"this is some test text does it really matter what it says \" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
