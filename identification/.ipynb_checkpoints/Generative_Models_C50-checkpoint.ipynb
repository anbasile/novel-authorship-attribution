{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "authors = os.listdir(\"/data/C50/C50train/\")\n",
    "print(len(authors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_chunks(l, n):\n",
    "    n = max(1, n)\n",
    "    return [l[i:i+n] for i in range(0, len(l), n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_texts_for_author(data_path, author_name):\n",
    "    fpath = os.path.join(data_path, author_name)\n",
    "    fnames = os.listdir(fpath)\n",
    "    text_paths = [os.path.join(fpath, name) for name in fnames]\n",
    "    texts = []\n",
    "    for tp in text_paths:\n",
    "        with open(tp) as f:\n",
    "            s = f.read()\n",
    "            texts.append(s)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "c50_test = \"/data/C50/C50test/\"\n",
    "\n",
    "def get_all_c50(c50_path):  # path to train or tests\n",
    "    all_texts = [] \n",
    "    all_labels = []\n",
    "    for author in authors:\n",
    "        author_texts = get_texts_for_author(c50_path, author)\n",
    "        all_texts += author_texts\n",
    "        all_labels += [author] * len(author_texts)\n",
    "        if len(author_texts) != 50:\n",
    "            print(author, \"not 50\")\n",
    "    return all_texts, all_labels\n",
    "\n",
    "c5)test_texts, test_labels = get_all_c50(c50_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# vectorization - chars to ints\n",
    "import string\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    \"\"\"Sample predictions from a probability array\"\"\"\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds + 1e-6) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def generate(model, diversity=0.5, text=\"\"):\n",
    "    \"\"\"Generate text from a model\"\"\"\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    for i in range(5000):\n",
    "        x = np.zeros((1, maxlen), dtype=np.int)\n",
    "        for t, char in enumerate(sentence):\n",
    "            try:\n",
    "                x[0, t] = char_indices[char]\n",
    "            except:\n",
    "                print(sentence)\n",
    "        preds = model.predict(x, verbose=0)[0][0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    return\n",
    "\n",
    "def vectorize(text):\n",
    "    \"\"\"Convert text into character sequences\"\"\"\n",
    "    step = 3\n",
    "    sentences = []\n",
    "    next_chars = []\n",
    "    for i in range(0, len(text) - maxlen, step):\n",
    "        sentences.append(text[i: i + maxlen])\n",
    "        next_chars.append(text[i + maxlen])\n",
    "    X = np.zeros((len(sentences), maxlen), dtype=np.int)\n",
    "    y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for t, char in enumerate(sentence):\n",
    "            X[i, t] = char_indices[char]\n",
    "        y[i, char_indices[next_chars[i]]] = 1\n",
    "    return X, y\n",
    "\n",
    "def clean_text(text, charset):\n",
    "    text = \" \".join(text.split())  # all white space is one space\n",
    "    text = \"\".join([x for x in text if x in charset])  # remove characters that we don't care about\n",
    "    return text\n",
    "\n",
    "def get_model(modelfile, freeze=False):\n",
    "    model = load_model(modelfile)\n",
    "    if freeze:\n",
    "        for layer in model.layers[:6]:\n",
    "            layer.trainable = False\n",
    "    return model\n",
    "\n",
    "chars = \" \" + string.ascii_letters + string.punctuation  # sorted to keep indices consistent\n",
    "charset = set(chars)  # for lookup\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "maxlen = 100  # must match length which generated model - the sequence length\n",
    "\n",
    "# load a pretrained language model\n",
    "modelfile = \"charlm2/model_middlemarch_cnn.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM, Input, Embedding, Conv1D, MaxPooling1D, BatchNormalization, GRU\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def get_gru_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(GRU(256))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(y.shape[1], activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'c50_test_texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-dfde6bbfb639>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mindicies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc50_test_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindicies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mindicies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindicies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mc50_test_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc50_test_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindicies\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'c50_test_texts' is not defined"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "indicies = list(range(len(c50_test_texts)))\n",
    "shuffle(indicies)\n",
    "indicies = np.array(indicies)\n",
    "c50_test_texts = np.array(c50_test_texts)[indicies]\n",
    "c50_test_labels = np.array(c50_test_labels)[indicies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['LydiaZajc', 'MureDickie', 'RobinSidel', 'MartinWolk',\n",
       "       'KirstinRidley', 'PierreTran', 'NickLouth', \"LynneO'Donnell\",\n",
       "       'BenjaminKangLim', 'FumikoFujisaki'],\n",
       "      dtype='<U17')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 182 words is quite short\n",
    "# Try to join 5 tests texts together\n",
    "longer_test_texts = get_chunks(test_texts, 5)\n",
    "longer_test_labels = get_chunks(test_labels, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all([len(set(x)) == 1 for x in longer_test_labels])  # Make sure that all combined labels are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "longer_test_texts = ['\\n'.join(chunk) for chunk in longer_test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "longer_test_labels = [chunk[0] for chunk in longer_test_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 10 .................................................. 0:01:38.061659\n",
      "1 / 10 .................................................. 0:00:44.293768\n",
      "2 / 10 .................................................. 0:01:25.260194\n",
      "3 / 10 .................................................. 0:00:59.792717\n",
      "4 / 10 .................................................. 0:00:59.716388\n",
      "5 / 10 .................................................. 0:00:49.332335\n",
      "6 / 10 .................................................. 0:00:37.487210\n",
      "7 / 10 .........................."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from random import shuffle\n",
    "from datetime import datetime\n",
    "\n",
    "def get_predictions(author_models, test_texts, test_labels):\n",
    "    \"\"\"Evaluate each text for each author_model and append first metric to predictions\"\"\"\n",
    "    indicies = list(range(len(test_texts)))\n",
    "\n",
    "    test_texts = np.array(test_texts)\n",
    "    test_labels = np.array(test_labels)\n",
    "\n",
    "    test_texts = test_texts[indicies]\n",
    "    test_labels = test_labels[indicies]\n",
    "\n",
    "    predictions = []\n",
    "    for i, text in enumerate(test_texts):\n",
    "        t1 = datetime.now()\n",
    "        print(\"{} / {}\".format(i, len(test_texts)), end=\" \")\n",
    "        X, y = vectorize(clean_text(text, charset))\n",
    "\n",
    "        losses = []\n",
    "        for am in author_models:\n",
    "            print(\".\", end=\"\")\n",
    "            model = am[0]\n",
    "            label = am[1]\n",
    "            loss = model.evaluate(X, y, verbose=0)\n",
    "            losses.append((loss, label))\n",
    "        print(\" {}\".format(datetime.now() - t1))\n",
    "        predictions.append(losses)\n",
    "    return predictions\n",
    "    \n",
    "\n",
    "predictions = get_predictions(author_models, c50_test_texts[:10], c50_test_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n"
     ]
    }
   ],
   "source": [
    "print(\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pred_is = []\n",
    "for pred in predictions_long:\n",
    "    pred_i = [p[0] for p in pred]\n",
    "    pred_is.append(pred_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pred_labs = [np.argmin(pred) for pred in pred_is]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94666666666666666"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(longer_test_labels, pred_labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_author_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_author_model.evaluatei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "len(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "word_vec = TfidfVectorizer(min_df=3, ngram_range=(1,2))\n",
    "char_vec = TfidfVectorizer(min_df=3, ngram_range=(2,5))\n",
    "\n",
    "fu = FeatureUnion([\n",
    "    ('word', word_vec),\n",
    "    ('char', char_vec)\n",
    "])\n",
    "\n",
    "\n",
    "X_train = fu.fit_transform(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_test = fu.transform(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "svm.fit(X_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "preds = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "accuracy_score(test_labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_test_longer = fu.transform(longer_test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "preds = svm.predict(X_test_longer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "accuracy_score(longer_test_labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X, y = vectorize(clean_text(train_texts[3], charset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_author_model.layers[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14725 samples, validate on 1637 samples\n",
      "Epoch 1/13\n",
      "14725/14725 [==============================] - 18s - loss: 3.1623 - val_loss: 3.9301\n",
      "Epoch 2/13\n",
      "14725/14725 [==============================] - 15s - loss: 2.4849 - val_loss: 3.2724\n",
      "Epoch 3/13\n",
      "14725/14725 [==============================] - 15s - loss: 2.2303 - val_loss: 2.7735\n",
      "Epoch 4/13\n",
      "14725/14725 [==============================] - 15s - loss: 2.0664 - val_loss: 2.3324\n",
      "Epoch 5/13\n",
      "14725/14725 [==============================] - 15s - loss: 1.9487 - val_loss: 2.0202\n",
      "Epoch 6/13\n",
      "14725/14725 [==============================] - 15s - loss: 1.8679 - val_loss: 1.9499\n",
      "Epoch 7/13\n",
      "14725/14725 [==============================] - 15s - loss: 1.7817 - val_loss: 1.9287\n",
      "Epoch 8/13\n",
      "14725/14725 [==============================] - 15s - loss: 1.6960 - val_loss: 1.9165\n",
      "Epoch 9/13\n",
      "14725/14725 [==============================] - 15s - loss: 1.6509 - val_loss: 1.9007\n",
      "Epoch 10/13\n",
      "14725/14725 [==============================] - 15s - loss: 1.5811 - val_loss: 1.8963\n",
      "Epoch 11/13\n",
      "14725/14725 [==============================] - 15s - loss: 1.5102 - val_loss: 1.8982\n",
      "Epoch 12/13\n",
      "14725/14725 [==============================] - 15s - loss: 1.4347 - val_loss: 1.9258\n",
      "Epoch 13/13\n",
      "14725/14725 [==============================] - 15s - loss: 1.3818 - val_loss: 1.9303\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nLSTM, BatchNorm\\nTrain on 14929 samples, validate on 1659 samples\\nEpoch 1/5\\n14929/14929 [==============================] - 75s - loss: 3.0403 - val_loss: 3.4757\\nEpoch 2/5\\n14929/14929 [==============================] - 60s - loss: 2.3156 - val_loss: 2.9687\\n\\nLSTM\\nTrain on 14929 samples, validate on 1659 samples\\nEpoch 1/5\\n14929/14929 [==============================] - 75s - loss: 3.1259 - val_loss: 2.7865\\nEpoch 2/5\\n14929/14929 [==============================] - 60s - loss: 2.6150 - val_loss: 2.3894\\n\\nCNN(5), LSTM  # faster, needs more epochs\\nTrain on 14929 samples, validate on 1659 samples\\nEpoch 1/5\\n14929/14929 [==============================] - 42s - loss: 3.1579 - val_loss: 2.9987\\nEpoch 2/5\\n14929/14929 [==============================] - 26s - loss: 2.8874 - val_loss: 2.6994\\nEpoch 3/5\\n14929/14929 [==============================] - 26s - loss: 2.6220 - val_loss: 2.4879\\nEpoch 4/5\\n14929/14929 [==============================] - 26s - loss: 2.4309 - val_loss: 2.3942\\nEpoch 5/5\\n14929/14929 [==============================] - 26s - loss: 2.2950 - val_loss: 2.2902\\n\\nCNN(5), CNN(3), LSTM doesn't drop below 3.0 in 5 epochs\\n\\n\\nEmbedding, BatchNorm, GRU, BatchNorm\\nTrain on 14725 samples, validate on 1637 samples\\nEpoch 1/20\\n14725/14725 [==============================] - 16s - loss: 2.9240 - val_loss: 3.9516\\nEpoch 2/20\\n14725/14725 [==============================] - 15s - loss: 2.2447 - val_loss: 3.3667\\nEpoch 3/20\\n14725/14725 [==============================] - 15s - loss: 2.0054 - val_loss: 2.8011\\nEpoch 4/20\\n14725/14725 [==============================] - 15s - loss: 1.8388 - val_loss: 2.3477\\nEpoch 5/20\\n14725/14725 [==============================] - 15s - loss: 1.7122 - val_loss: 2.0196\\nEpoch 6/20\\n14725/14725 [==============================] - 15s - loss: 1.6069 - val_loss: 1.9417\\nEpoch 7/20\\n14725/14725 [==============================] - 15s - loss: 1.5044 - val_loss: 1.9541\\nEpoch 8/20\\n14725/14725 [==============================] - 15s - loss: 1.3987 - val_loss: 1.9512\\nEpoch 9/20\\n14725/14725 [==============================] - 15s - loss: 1.2940 - val_loss: 1.9921\\nEpoch 10/20\\n14725/14725 [==============================] - 15s - loss: 1.1850 - val_loss: 2.0424\\n\\nTrain on 14725 samples, validate on 1637 samples\\nEpoch 1/20\\n14725/14725 [==============================] - 17s - loss: 3.2577 - val_loss: 3.8892\\nEpoch 2/20\\n14725/14725 [==============================] - 15s - loss: 2.5444 - val_loss: 3.2364\\nEpoch 3/20\\n14725/14725 [==============================] - 15s - loss: 2.2915 - val_loss: 2.7205\\nEpoch 4/20\\n14725/14725 [==============================] - 15s - loss: 2.1258 - val_loss: 2.3477\\nEpoch 5/20\\n14725/14725 [==============================] - 15s - loss: 2.0219 - val_loss: 2.0588\\nEpoch 6/20\\n14725/14725 [==============================] - 15s - loss: 1.9386 - val_loss: 1.9478\\nEpoch 7/20\\n14725/14725 [==============================] - 15s - loss: 1.8762 - val_loss: 1.9152\\nEpoch 8/20\\n14725/14725 [==============================] - 15s - loss: 1.7994 - val_loss: 1.9076\\nEpoch 9/20\\n14725/14725 [==============================] - 15s - loss: 1.7224 - val_loss: 1.8829\\nEpoch 10/20\\n14725/14725 [==============================] - 15s - loss: 1.6783 - val_loss: 1.9007\\nEpoch 11/20\\n14725/14725 [==============================] - 15s - loss: 1.6154 - val_loss: 1.8910\\nEpoch 12/20\\n14725/14725 [==============================] - 15s - loss: 1.5485 - val_loss: 1.8920\\nEpoch 13/20\\n14725/14725 [==============================] - 15s - loss: 1.4844 - val_loss: 1.9198\\nEpoch 14/20\\n14725/14725 [==============================] - 15s - loss: 1.4291 - val_loss: 1.9193\\nEpoch 15/20\\n14725/14725 [==============================] - 15s - loss: 1.3670 - val_loss: 1.9295\\nEpoch 16/20\\n14725/14725 [==============================] - 15s - loss: 1.3028 - val_loss: 1.9752\\nEpoch 17/20\\n14725/14725 [==============================] - 15s - loss: 1.2321 - val_loss: 1.9969\\nEpoch 18/20\\n14725/14725 [==============================] - 15s - loss: 1.2159 - val_loss: 2.0431\\nEpoch 19/20\\n14725/14725 [==============================] - 15s - loss: 1.1391 - val_loss: 2.0709\\nEpoch 20/20\\n14725/14725 [==============================] - 15s - loss: 1.1321 - val_loss: 2.1172\\n\\n\\nmodel = Sequential()\\nmodel.add(Embedding(input_dim=len(charset), output_dim=100))\\nmodel.add(BatchNormalization())\\nmodel.add(GRU(256))\\nmodel.add(Dropout(0.5))\\nmodel.add(BatchNormalization())\\nmodel.add(Dense(y.shape[1], activation='softmax'))\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\\nmodel.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\\nTrain on 14725 samples, validate on 1637 samples\\nEpoch 1/20\\n14725/14725 [==============================] - 17s - loss: 3.2577 - val_loss: 3.8892\\nEpoch 2/20\\n14725/14725 [==============================] - 15s - loss: 2.5444 - val_loss: 3.2364\\nEpoch 3/20\\n14725/14725 [==============================] - 15s - loss: 2.2915 - val_loss: 2.7205\\nEpoch 4/20\\n14725/14725 [==============================] - 15s - loss: 2.1258 - val_loss: 2.3477\\nEpoch 5/20\\n14725/14725 [==============================] - 15s - loss: 2.0219 - val_loss: 2.0588\\nEpoch 6/20\\n14725/14725 [==============================] - 15s - loss: 1.9386 - val_loss: 1.9478\\nEpoch 7/20\\n14725/14725 [==============================] - 15s - loss: 1.8762 - val_loss: 1.9152\\nEpoch 8/20\\n14725/14725 [==============================] - 15s - loss: 1.7994 - val_loss: 1.9076\\nEpoch 9/20\\n14725/14725 [==============================] - 15s - loss: 1.7224 - val_loss: 1.8829\\nEpoch 10/20\\n14725/14725 [==============================] - 15s - loss: 1.6783 - val_loss: 1.9007\\nEpoch 11/20\\n14725/14725 [==============================] - 15s - loss: 1.6154 - val_loss: 1.8910\\nEpoch 12/20\\n14725/14725 [==============================] - 15s - loss: 1.5485 - val_loss: 1.8920\\nEpoch 13/20\\n14725/14725 [==============================] - 15s - loss: 1.4844 - val_loss: 1.9198\\nEpoch 14/20\\n14725/14725 [==============================] - 15s - loss: 1.4291 - val_loss: 1.9193\\nEpoch 15/20\\n14725/14725 [==============================] - 15s - loss: 1.3670 - val_loss: 1.9295\\nEpoch 16/20\\n14725/14725 [==============================] - 15s - loss: 1.3028 - val_loss: 1.9752\\nEpoch 17/20\\n14725/14725 [==============================] - 15s - loss: 1.2321 - val_loss: 1.9969\\nEpoch 18/20\\n14725/14725 [==============================] - 15s - loss: 1.2159 - val_loss: 2.0431\\nEpoch 19/20\\n14725/14725 [==============================] - 15s - loss: 1.1391 - val_loss: 2.0709\\nEpoch 20/20\\n14725/14725 [==============================] - 15s - loss: 1.1321 - val_loss: 2.1172\\n\\n\\nmodel = Sequential()\\nmodel.add(Embedding(input_dim=len(charset), output_dim=100))\\nmodel.add(GRU(256))\\nmodel.add(BatchNormalization())\\nmodel.add(Dropout(0.5))\\nmodel.add(Dense(y.shape[1], activation='softmax'))\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\\nmodel.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\\nTrain on 14725 samples, validate on 1637 samples\\nEpoch 1/20\\n14725/14725 [==============================] - 16s - loss: 3.1731 - val_loss: 3.5648\\nEpoch 2/20\\n14725/14725 [==============================] - 14s - loss: 2.4964 - val_loss: 2.9875\\nEpoch 3/20\\n14725/14725 [==============================] - 14s - loss: 2.3446 - val_loss: 2.7695\\nEpoch 4/20\\n14725/14725 [==============================] - 14s - loss: 2.2928 - val_loss: 2.6010\\nEpoch 5/20\\n14725/14725 [==============================] - 14s - loss: 2.2642 - val_loss: 2.3900\\nEpoch 6/20\\n14725/14725 [==============================] - 14s - loss: 2.2373 - val_loss: 2.5023\\nEpoch 7/20\\n14725/14725 [==============================] - 14s - loss: 2.2186 - val_loss: 2.3780\\nEpoch 8/20\\n14725/14725 [==============================] - 14s - loss: 2.2029 - val_loss: 2.4928\\nEpoch 9/20\\n14725/14725 [==============================] - 14s - loss: 2.1852 - val_loss: 2.3480\\nEpoch 10/20\\n14725/14725 [==============================] - 14s - loss: 2.1745 - val_loss: 2.4801\\nEpoch 11/20\\n14725/14725 [==============================] - 14s - loss: 2.1563 - val_loss: 2.3951\\nEpoch 12/20\\n14725/14725 [==============================] - 14s - loss: 2.1391 - val_loss: 2.4133\\nEpoch 13/20\\n14725/14725 [==============================] - 14s - loss: 2.1192 - val_loss: 2.5896\\nEpoch 14/20\\n14725/14725 [==============================] - 14s - loss: 2.1020 - val_loss: 2.2692\\nEpoch 15/20\\n14725/14725 [==============================] - 14s - loss: 2.0770 - val_loss: 2.2179\\nEpoch 16/20\\n14725/14725 [==============================] - 14s - loss: 2.0643 - val_loss: 2.2822\\nEpoch 17/20\\n 6784/14725 [============>.................] - ETA: 7s - loss: 2.0302\\n\\n\\nmodel = Sequential()\\nmodel.add(Embedding(input_dim=len(charset), output_dim=100))\\nmodel.add(Dropout(0.2))\\nmodel.add(BatchNormalization())\\nmodel.add(GRU(256))\\nmodel.add(Dropout(0.2))\\nmodel.add(BatchNormalization())\\nmodel.add(Dense(y.shape[1], activation='softmax'))\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\\nmodel.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\\nTrain on 14725 samples, validate on 1637 samples\\nEpoch 1/20\\n14725/14725 [==============================] - 17s - loss: 3.1528 - val_loss: 3.9042\\nEpoch 2/20\\n14725/14725 [==============================] - 15s - loss: 2.4658 - val_loss: 3.2093\\nEpoch 3/20\\n14725/14725 [==============================] - 15s - loss: 2.2001 - val_loss: 2.6764\\nEpoch 4/20\\n14725/14725 [==============================] - 15s - loss: 2.0358 - val_loss: 2.2919\\nEpoch 5/20\\n14725/14725 [==============================] - 15s - loss: 1.9497 - val_loss: 2.0060\\nEpoch 6/20\\n14725/14725 [==============================] - 15s - loss: 1.8588 - val_loss: 1.9313\\nEpoch 7/20\\n14725/14725 [==============================] - 15s - loss: 1.7883 - val_loss: 1.9153\\nEpoch 8/20\\n14725/14725 [==============================] - 15s - loss: 1.7034 - val_loss: 1.9145\\nEpoch 9/20\\n14725/14725 [==============================] - 15s - loss: 1.6382 - val_loss: 1.8979\\nEpoch 10/20\\n14725/14725 [==============================] - 15s - loss: 1.5827 - val_loss: 1.8864\\nEpoch 11/20\\n14725/14725 [==============================] - 15s - loss: 1.5093 - val_loss: 1.8967\\nEpoch 12/20\\n14725/14725 [==============================] - 15s - loss: 1.4472 - val_loss: 1.9040\\nEpoch 13/20\\n14725/14725 [==============================] - 15s - loss: 1.3809 - val_loss: 1.9227\\nEpoch 14/20\\n14725/14725 [==============================] - 15s - loss: 1.3225 - val_loss: 1.9469\\nEpoch 15/20\\n14725/14725 [==============================] - 15s - loss: 1.2516 - val_loss: 1.9862\\nEpoch 16/20\\n14725/14725 [==============================] - 15s - loss: 1.2094 - val_loss: 1.9963\\nEpoch 17/20\\n14725/14725 [==============================] - 15s - loss: 1.1658 - val_loss: 2.0331\\nEpoch 18/20\\n14725/14725 [==============================] - 15s - loss: 1.0851 - val_loss: 2.0452\\nEpoch 19/20\\n14725/14725 [==============================] - 15s - loss: 1.0394 - val_loss: 2.0810\\nEpoch 20/20\\n14725/14725 [==============================] - 15s - loss: 0.9903 - val_loss: 2.1283\\n\\n\\nmodel = Sequential()\\nmodel.add(Embedding(input_dim=len(charset), output_dim=100))\\nmodel.add(Dropout(0.3))\\nmodel.add(BatchNormalization())\\nmodel.add(GRU(256))\\nmodel.add(Dropout(0.3))\\nmodel.add(BatchNormalization())\\nmodel.add(Dense(y.shape[1], activation='softmax'))\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\\nmodel.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\\nTrain on 14725 samples, validate on 1637 samples\\nEpoch 1/20\\n14725/14725 [==============================] - 17s - loss: 3.2991 - val_loss: 3.8902\\nEpoch 2/20\\n14725/14725 [==============================] - 15s - loss: 2.5672 - val_loss: 3.1627\\nEpoch 3/20\\n14725/14725 [==============================] - 15s - loss: 2.2731 - val_loss: 2.6340\\nEpoch 4/20\\n14725/14725 [==============================] - 15s - loss: 2.1316 - val_loss: 2.2594\\nEpoch 5/20\\n14725/14725 [==============================] - 15s - loss: 2.0249 - val_loss: 2.0159\\nEpoch 6/20\\n14725/14725 [==============================] - 15s - loss: 1.9571 - val_loss: 1.9456\\nEpoch 7/20\\n14725/14725 [==============================] - 15s - loss: 1.8789 - val_loss: 1.9213\\nEpoch 8/20\\n14725/14725 [==============================] - 15s - loss: 1.8233 - val_loss: 1.8924\\nEpoch 9/20\\n14725/14725 [==============================] - 15s - loss: 1.7575 - val_loss: 1.8987\\n\\n\\n\\nmodel = Sequential()\\nmodel.add(Embedding(input_dim=len(charset), output_dim=100))\\nmodel.add(Dropout(0.5))\\nmodel.add(BatchNormalization())\\nmodel.add(GRU(256))\\nmodel.add(Dropout(0.5))\\nmodel.add(BatchNormalization())\\nmodel.add(Dense(y.shape[1], activation='softmax'))\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\\nmodel.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\\nTrain on 14725 samples, validate on 1637 samples\\nEpoch 1/20\\n14725/14725 [==============================] - 17s - loss: 3.6036 - val_loss: 3.7924\\nEpoch 2/20\\n14725/14725 [==============================] - 15s - loss: 2.7765 - val_loss: 3.0022\\nEpoch 3/20\\n14725/14725 [==============================] - 15s - loss: 2.4773 - val_loss: 2.5697\\nEpoch 4/20\\n14725/14725 [==============================] - 15s - loss: 2.3218 - val_loss: 2.2606\\nEpoch 5/20\\n14725/14725 [==============================] - 15s - loss: 2.2328 - val_loss: 2.0832\\nEpoch 6/20\\n14725/14725 [==============================] - 15s - loss: 2.1748 - val_loss: 2.0248\\nEpoch 7/20\\n14725/14725 [==============================] - 15s - loss: 2.1174 - val_loss: 1.9865\\nEpoch 8/20\\n14725/14725 [==============================] - 15s - loss: 2.0617 - val_loss: 1.9640\\nEpoch 9/20\\n14725/14725 [==============================] - 15s - loss: 2.0206 - val_loss: 1.9461\\nEpoch 10/20\\n14725/14725 [==============================] - 15s - loss: 1.9758 - val_loss: 1.9334\\nEpoch 11/20\\n14725/14725 [==============================] - 15s - loss: 1.9546 - val_loss: 1.9148\\nEpoch 12/20\\n14725/14725 [==============================] - 15s - loss: 1.9045 - val_loss: 1.9121\\nEpoch 13/20\\n14725/14725 [==============================] - 15s - loss: 1.8757 - val_loss: 1.8888\\nEpoch 14/20\\n14725/14725 [==============================] - 15s - loss: 1.8437 - val_loss: 1.8874\\nEpoch 15/20\\n14725/14725 [==============================] - 15s - loss: 1.8145 - val_loss: 1.8822\\nEpoch 16/20\\n14725/14725 [==============================] - 15s - loss: 1.7805 - val_loss: 1.8785\\nEpoch 17/20\\n14725/14725 [==============================] - 15s - loss: 1.7558 - val_loss: 1.8868\\nEpoch 18/20\\n14725/14725 [==============================] - 15s - loss: 1.7218 - val_loss: 1.8670\\nEpoch 19/20\\n14725/14725 [==============================] - 15s - loss: 1.7032 - val_loss: 1.8759\\nEpoch 20/20\\n14725/14725 [==============================] - 15s - loss: 1.6832 - val_loss: 1.8834\\n\\n\\nmodel = Sequential()\\nmodel.add(Embedding(input_dim=len(charset), output_dim=100))\\nmodel.add(Dropout(0.5))\\nmodel.add(BatchNormalization())\\nmodel.add(GRU(256, return_sequences=True))\\nmodel.add(Dropout(0.5))\\nmodel.add(BatchNormalization())\\nmodel.add(GRU(256))\\nmodel.add(Dropout(0.5))\\nmodel.add(BatchNormalization())\\nmodel.add(Dense(y.shape[1], activation='softmax'))\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\\nmodel.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\\nTrain on 14725 samples, validate on 1637 samples\\nEpoch 1/20\\n14725/14725 [==============================] - 33s - loss: 3.7814 - val_loss: 3.7282\\nEpoch 2/20\\n14725/14725 [==============================] - 29s - loss: 2.8453 - val_loss: 2.8542\\nEpoch 3/20\\n14725/14725 [==============================] - 29s - loss: 2.5178 - val_loss: 2.4434\\nEpoch 4/20\\n14725/14725 [==============================] - 29s - loss: 2.3762 - val_loss: 2.1894\\nEpoch 5/20\\n14725/14725 [==============================] - 29s - loss: 2.2896 - val_loss: 2.0862\\nEpoch 6/20\\n14725/14725 [==============================] - 29s - loss: 2.2254 - val_loss: 2.0516\\nEpoch 7/20\\n14725/14725 [==============================] - 29s - loss: 2.1565 - val_loss: 2.0133\\nEpoch 8/20\\n14725/14725 [==============================] - 29s - loss: 2.1132 - val_loss: 1.9992\\nEpoch 9/20\\n14725/14725 [==============================] - 29s - loss: 2.0798 - val_loss: 1.9881\\nEpoch 10/20\\n14725/14725 [==============================] - 29s - loss: 2.0509 - val_loss: 1.9784\\nEpoch 11/20\\n14725/14725 [==============================] - 29s - loss: 2.0198 - val_loss: 1.9618\\nEpoch 12/20\\n14725/14725 [==============================] - 29s - loss: 1.9822 - val_loss: 1.9383\\nEpoch 13/20\\n14725/14725 [==============================] - 29s - loss: 1.9437 - val_loss: 1.9300\\nEpoch 14/20\\n14725/14725 [==============================] - 29s - loss: 1.9198 - val_loss: 1.9163\\nEpoch 15/20\\n14725/14725 [==============================] - 29s - loss: 1.8989 - val_loss: 1.9160\\nEpoch 16/20\\n14725/14725 [==============================] - 29s - loss: 1.8866 - val_loss: 1.9085\\nEpoch 17/20\\n14725/14725 [==============================] - 29s - loss: 1.8493 - val_loss: 1.8965\\nEpoch 18/20\\n14725/14725 [==============================] - 29s - loss: 1.8248 - val_loss: 1.8878\\nEpoch 19/20\\n14725/14725 [==============================] - 29s - loss: 1.8037 - val_loss: 1.8870\\nEpoch 20/20\\n14725/14725 [==============================] - 29s - loss: 1.7724 - val_loss: 1.8862\\n\\n\\nmodel = Sequential()\\nmodel.add(Embedding(input_dim=len(charset), output_dim=100))\\nmodel.add(Dropout(0.2))\\nmodel.add(BatchNormalization())\\nmodel.add(GRU(256, return_sequences=True))\\nmodel.add(Dropout(0.2))\\nmodel.add(BatchNormalization())\\nmodel.add(GRU(256))\\nmodel.add(Dropout(0.2))\\nmodel.add(BatchNormalization())\\nmodel.add(Dense(y.shape[1], activation='softmax'))\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\\nmodel.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\\nTrain on 14725 samples, validate on 1637 samples\\nEpoch 1/20\\n14725/14725 [==============================] - 32s - loss: 3.2809 - val_loss: 3.7595\\nEpoch 2/20\\n14725/14725 [==============================] - 29s - loss: 2.4379 - val_loss: 2.9869\\nEpoch 3/20\\n14725/14725 [==============================] - 29s - loss: 2.1504 - val_loss: 2.5361\\nEpoch 4/20\\n14725/14725 [==============================] - 29s - loss: 1.9887 - val_loss: 2.1294\\nEpoch 5/20\\n14725/14725 [==============================] - 29s - loss: 1.8984 - val_loss: 1.9727\\nEpoch 6/20\\n14725/14725 [==============================] - 29s - loss: 1.7892 - val_loss: 1.9264\\nEpoch 7/20\\n14725/14725 [==============================] - 29s - loss: 1.7172 - val_loss: 1.9100\\nEpoch 8/20\\n14725/14725 [==============================] - 29s - loss: 1.6361 - val_loss: 1.9124\\nEpoch 9/20\\n14725/14725 [==============================] - 29s - loss: 1.5621 - val_loss: 1.9122\\nEpoch 10/20\\n14725/14725 [==============================] - 29s - loss: 1.4863 - val_loss: 1.9045\\nEpoch 11/20\\n14725/14725 [==============================] - 29s - loss: 1.4150 - val_loss: 1.9278\\nEpoch 12/20\\n14725/14725 [==============================] - 29s - loss: 1.3691 - val_loss: 1.9181\\nEpoch 13/20\\n14725/14725 [==============================] - 29s - loss: 1.2970 - val_loss: 1.9414\\nEpoch 14/20\\n 1536/14725 [==>...........................] - ETA: 25s - loss: 1.1475\\n \\n \\n \\nmodel = Sequential()\\nmodel.add(Embedding(input_dim=len(charset), output_dim=100))\\nmodel.add(BatchNormalization())\\nmodel.add(GRU(256))\\nmodel.add(Dropout(0.4))\\nmodel.add(BatchNormalization())\\nmodel.add(Dense(y.shape[1], activation='softmax'))\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\\nmodel.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\\n Train on 14725 samples, validate on 1637 samples\\nEpoch 1/20\\n14725/14725 [==============================] - 17s - loss: 3.1787 - val_loss: 3.9282\\nEpoch 2/20\\n14725/14725 [==============================] - 15s - loss: 2.5070 - val_loss: 3.2479\\nEpoch 3/20\\n14725/14725 [==============================] - 15s - loss: 2.2191 - val_loss: 2.7522\\nEpoch 4/20\\n14725/14725 [==============================] - 15s - loss: 2.0637 - val_loss: 2.3331\\nEpoch 5/20\\n14725/14725 [==============================] - 15s - loss: 1.9539 - val_loss: 2.0326\\nEpoch 6/20\\n14725/14725 [==============================] - 15s - loss: 1.8622 - val_loss: 1.9440\\nEpoch 7/20\\n14725/14725 [==============================] - 15s - loss: 1.7821 - val_loss: 1.9166\\nEpoch 8/20\\n14725/14725 [==============================] - 15s - loss: 1.7169 - val_loss: 1.8996\\nEpoch 9/20\\n14725/14725 [==============================] - 15s - loss: 1.6561 - val_loss: 1.8849\\nEpoch 10/20\\n14725/14725 [==============================] - 15s - loss: 1.5910 - val_loss: 1.9032\\nEpoch 11/20\\n14725/14725 [==============================] - 15s - loss: 1.5082 - val_loss: 1.8878\\nEpoch 12/20\\n14725/14725 [==============================] - 15s - loss: 1.4513 - val_loss: 1.9252\\nEpoch 13/20\\n 7552/14725 [==============>...............] - ETA: 7s - loss: 1.3534\\n \\n \\nmodel = Sequential()\\nmodel.add(Embedding(input_dim=len(charset), output_dim=100))\\nmodel.add(BatchNormalization())\\nmodel.add(GRU(512))\\nmodel.add(Dropout(0.4))\\nmodel.add(BatchNormalization())\\nmodel.add(Dense(y.shape[1], activation='softmax'))\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\\nmodel.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\\n Train on 14725 samples, validate on 1637 samples\\nEpoch 1/20\\n14725/14725 [==============================] - 23s - loss: 3.1173 - val_loss: 3.8415\\nEpoch 2/20\\n14725/14725 [==============================] - 20s - loss: 2.4202 - val_loss: 3.1512\\nEpoch 3/20\\n14725/14725 [==============================] - 20s - loss: 2.1302 - val_loss: 2.7191\\nEpoch 4/20\\n14725/14725 [==============================] - 20s - loss: 1.9435 - val_loss: 2.3341\\nEpoch 5/20\\n14725/14725 [==============================] - 20s - loss: 1.7966 - val_loss: 1.9877\\nEpoch 6/20\\n14725/14725 [==============================] - 20s - loss: 1.6621 - val_loss: 1.9349\\nEpoch 7/20\\n14725/14725 [==============================] - 20s - loss: 1.5190 - val_loss: 1.9632\\nEpoch 8/20\\n14725/14725 [==============================] - 20s - loss: 1.3925 - val_loss: 1.9735\\n\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM, Input, Embedding, Conv1D, MaxPooling1D, BatchNormalization, GRU\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# load ascii text and covert to lowercase\n",
    "\n",
    "# cnn = Dropout(0.2)(embedded)\n",
    "# cnn = Conv1D(128, 5, activation='relu')(cnn)\n",
    "# cnn = MaxPooling1D(pool_size=4)(cnn)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(256))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=13, batch_size=128, validation_split=0.1)\n",
    "\n",
    "\"\"\"\n",
    "LSTM, BatchNorm\n",
    "Train on 14929 samples, validate on 1659 samples\n",
    "Epoch 1/5\n",
    "14929/14929 [==============================] - 75s - loss: 3.0403 - val_loss: 3.4757\n",
    "Epoch 2/5\n",
    "14929/14929 [==============================] - 60s - loss: 2.3156 - val_loss: 2.9687\n",
    "\n",
    "LSTM\n",
    "Train on 14929 samples, validate on 1659 samples\n",
    "Epoch 1/5\n",
    "14929/14929 [==============================] - 75s - loss: 3.1259 - val_loss: 2.7865\n",
    "Epoch 2/5\n",
    "14929/14929 [==============================] - 60s - loss: 2.6150 - val_loss: 2.3894\n",
    "\n",
    "CNN(5), LSTM  # faster, needs more epochs\n",
    "Train on 14929 samples, validate on 1659 samples\n",
    "Epoch 1/5\n",
    "14929/14929 [==============================] - 42s - loss: 3.1579 - val_loss: 2.9987\n",
    "Epoch 2/5\n",
    "14929/14929 [==============================] - 26s - loss: 2.8874 - val_loss: 2.6994\n",
    "Epoch 3/5\n",
    "14929/14929 [==============================] - 26s - loss: 2.6220 - val_loss: 2.4879\n",
    "Epoch 4/5\n",
    "14929/14929 [==============================] - 26s - loss: 2.4309 - val_loss: 2.3942\n",
    "Epoch 5/5\n",
    "14929/14929 [==============================] - 26s - loss: 2.2950 - val_loss: 2.2902\n",
    "\n",
    "CNN(5), CNN(3), LSTM doesn't drop below 3.0 in 5 epochs\n",
    "\n",
    "\n",
    "Embedding, BatchNorm, GRU, BatchNorm\n",
    "Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 16s - loss: 2.9240 - val_loss: 3.9516\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.2447 - val_loss: 3.3667\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.0054 - val_loss: 2.8011\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.8388 - val_loss: 2.3477\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7122 - val_loss: 2.0196\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.6069 - val_loss: 1.9417\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.5044 - val_loss: 1.9541\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.3987 - val_loss: 1.9512\n",
    "Epoch 9/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.2940 - val_loss: 1.9921\n",
    "Epoch 10/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.1850 - val_loss: 2.0424\n",
    "\n",
    "Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 17s - loss: 3.2577 - val_loss: 3.8892\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.5444 - val_loss: 3.2364\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.2915 - val_loss: 2.7205\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.1258 - val_loss: 2.3477\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.0219 - val_loss: 2.0588\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.9386 - val_loss: 1.9478\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.8762 - val_loss: 1.9152\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7994 - val_loss: 1.9076\n",
    "Epoch 9/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7224 - val_loss: 1.8829\n",
    "Epoch 10/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.6783 - val_loss: 1.9007\n",
    "Epoch 11/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.6154 - val_loss: 1.8910\n",
    "Epoch 12/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.5485 - val_loss: 1.8920\n",
    "Epoch 13/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.4844 - val_loss: 1.9198\n",
    "Epoch 14/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.4291 - val_loss: 1.9193\n",
    "Epoch 15/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.3670 - val_loss: 1.9295\n",
    "Epoch 16/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.3028 - val_loss: 1.9752\n",
    "Epoch 17/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.2321 - val_loss: 1.9969\n",
    "Epoch 18/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.2159 - val_loss: 2.0431\n",
    "Epoch 19/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.1391 - val_loss: 2.0709\n",
    "Epoch 20/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.1321 - val_loss: 2.1172\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(256))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\n",
    "Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 17s - loss: 3.2577 - val_loss: 3.8892\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.5444 - val_loss: 3.2364\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.2915 - val_loss: 2.7205\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.1258 - val_loss: 2.3477\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.0219 - val_loss: 2.0588\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.9386 - val_loss: 1.9478\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.8762 - val_loss: 1.9152\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7994 - val_loss: 1.9076\n",
    "Epoch 9/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7224 - val_loss: 1.8829\n",
    "Epoch 10/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.6783 - val_loss: 1.9007\n",
    "Epoch 11/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.6154 - val_loss: 1.8910\n",
    "Epoch 12/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.5485 - val_loss: 1.8920\n",
    "Epoch 13/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.4844 - val_loss: 1.9198\n",
    "Epoch 14/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.4291 - val_loss: 1.9193\n",
    "Epoch 15/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.3670 - val_loss: 1.9295\n",
    "Epoch 16/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.3028 - val_loss: 1.9752\n",
    "Epoch 17/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.2321 - val_loss: 1.9969\n",
    "Epoch 18/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.2159 - val_loss: 2.0431\n",
    "Epoch 19/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.1391 - val_loss: 2.0709\n",
    "Epoch 20/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.1321 - val_loss: 2.1172\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "model.add(GRU(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\n",
    "Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 16s - loss: 3.1731 - val_loss: 3.5648\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.4964 - val_loss: 2.9875\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.3446 - val_loss: 2.7695\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.2928 - val_loss: 2.6010\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.2642 - val_loss: 2.3900\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.2373 - val_loss: 2.5023\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.2186 - val_loss: 2.3780\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.2029 - val_loss: 2.4928\n",
    "Epoch 9/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.1852 - val_loss: 2.3480\n",
    "Epoch 10/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.1745 - val_loss: 2.4801\n",
    "Epoch 11/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.1563 - val_loss: 2.3951\n",
    "Epoch 12/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.1391 - val_loss: 2.4133\n",
    "Epoch 13/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.1192 - val_loss: 2.5896\n",
    "Epoch 14/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.1020 - val_loss: 2.2692\n",
    "Epoch 15/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.0770 - val_loss: 2.2179\n",
    "Epoch 16/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.0643 - val_loss: 2.2822\n",
    "Epoch 17/20\n",
    " 6784/14725 [============>.................] - ETA: 7s - loss: 2.0302\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\n",
    "Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 17s - loss: 3.1528 - val_loss: 3.9042\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.4658 - val_loss: 3.2093\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.2001 - val_loss: 2.6764\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.0358 - val_loss: 2.2919\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.9497 - val_loss: 2.0060\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.8588 - val_loss: 1.9313\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7883 - val_loss: 1.9153\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7034 - val_loss: 1.9145\n",
    "Epoch 9/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.6382 - val_loss: 1.8979\n",
    "Epoch 10/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.5827 - val_loss: 1.8864\n",
    "Epoch 11/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.5093 - val_loss: 1.8967\n",
    "Epoch 12/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.4472 - val_loss: 1.9040\n",
    "Epoch 13/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.3809 - val_loss: 1.9227\n",
    "Epoch 14/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.3225 - val_loss: 1.9469\n",
    "Epoch 15/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.2516 - val_loss: 1.9862\n",
    "Epoch 16/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.2094 - val_loss: 1.9963\n",
    "Epoch 17/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.1658 - val_loss: 2.0331\n",
    "Epoch 18/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.0851 - val_loss: 2.0452\n",
    "Epoch 19/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.0394 - val_loss: 2.0810\n",
    "Epoch 20/20\n",
    "14725/14725 [==============================] - 15s - loss: 0.9903 - val_loss: 2.1283\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(256))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\n",
    "Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 17s - loss: 3.2991 - val_loss: 3.8902\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.5672 - val_loss: 3.1627\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.2731 - val_loss: 2.6340\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.1316 - val_loss: 2.2594\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.0249 - val_loss: 2.0159\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.9571 - val_loss: 1.9456\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.8789 - val_loss: 1.9213\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.8233 - val_loss: 1.8924\n",
    "Epoch 9/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7575 - val_loss: 1.8987\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(256))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\n",
    "Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 17s - loss: 3.6036 - val_loss: 3.7924\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.7765 - val_loss: 3.0022\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.4773 - val_loss: 2.5697\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.3218 - val_loss: 2.2606\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.2328 - val_loss: 2.0832\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.1748 - val_loss: 2.0248\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.1174 - val_loss: 1.9865\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.0617 - val_loss: 1.9640\n",
    "Epoch 9/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.0206 - val_loss: 1.9461\n",
    "Epoch 10/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.9758 - val_loss: 1.9334\n",
    "Epoch 11/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.9546 - val_loss: 1.9148\n",
    "Epoch 12/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.9045 - val_loss: 1.9121\n",
    "Epoch 13/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.8757 - val_loss: 1.8888\n",
    "Epoch 14/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.8437 - val_loss: 1.8874\n",
    "Epoch 15/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.8145 - val_loss: 1.8822\n",
    "Epoch 16/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7805 - val_loss: 1.8785\n",
    "Epoch 17/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7558 - val_loss: 1.8868\n",
    "Epoch 18/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7218 - val_loss: 1.8670\n",
    "Epoch 19/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7032 - val_loss: 1.8759\n",
    "Epoch 20/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.6832 - val_loss: 1.8834\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(256, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(256))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\n",
    "Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 33s - loss: 3.7814 - val_loss: 3.7282\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.8453 - val_loss: 2.8542\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.5178 - val_loss: 2.4434\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.3762 - val_loss: 2.1894\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.2896 - val_loss: 2.0862\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.2254 - val_loss: 2.0516\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.1565 - val_loss: 2.0133\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.1132 - val_loss: 1.9992\n",
    "Epoch 9/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.0798 - val_loss: 1.9881\n",
    "Epoch 10/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.0509 - val_loss: 1.9784\n",
    "Epoch 11/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.0198 - val_loss: 1.9618\n",
    "Epoch 12/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.9822 - val_loss: 1.9383\n",
    "Epoch 13/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.9437 - val_loss: 1.9300\n",
    "Epoch 14/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.9198 - val_loss: 1.9163\n",
    "Epoch 15/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.8989 - val_loss: 1.9160\n",
    "Epoch 16/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.8866 - val_loss: 1.9085\n",
    "Epoch 17/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.8493 - val_loss: 1.8965\n",
    "Epoch 18/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.8248 - val_loss: 1.8878\n",
    "Epoch 19/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.8037 - val_loss: 1.8870\n",
    "Epoch 20/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.7724 - val_loss: 1.8862\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\n",
    "Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 32s - loss: 3.2809 - val_loss: 3.7595\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.4379 - val_loss: 2.9869\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.1504 - val_loss: 2.5361\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.9887 - val_loss: 2.1294\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.8984 - val_loss: 1.9727\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.7892 - val_loss: 1.9264\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.7172 - val_loss: 1.9100\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.6361 - val_loss: 1.9124\n",
    "Epoch 9/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.5621 - val_loss: 1.9122\n",
    "Epoch 10/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.4863 - val_loss: 1.9045\n",
    "Epoch 11/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.4150 - val_loss: 1.9278\n",
    "Epoch 12/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.3691 - val_loss: 1.9181\n",
    "Epoch 13/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.2970 - val_loss: 1.9414\n",
    "Epoch 14/20\n",
    " 1536/14725 [==>...........................] - ETA: 25s - loss: 1.1475\n",
    " \n",
    " \n",
    " \n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(256))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\n",
    " Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 17s - loss: 3.1787 - val_loss: 3.9282\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.5070 - val_loss: 3.2479\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.2191 - val_loss: 2.7522\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.0637 - val_loss: 2.3331\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.9539 - val_loss: 2.0326\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.8622 - val_loss: 1.9440\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7821 - val_loss: 1.9166\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7169 - val_loss: 1.8996\n",
    "Epoch 9/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.6561 - val_loss: 1.8849\n",
    "Epoch 10/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.5910 - val_loss: 1.9032\n",
    "Epoch 11/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.5082 - val_loss: 1.8878\n",
    "Epoch 12/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.4513 - val_loss: 1.9252\n",
    "Epoch 13/20\n",
    " 7552/14725 [==============>...............] - ETA: 7s - loss: 1.3534\n",
    " \n",
    " \n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(512))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\n",
    " Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 23s - loss: 3.1173 - val_loss: 3.8415\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 20s - loss: 2.4202 - val_loss: 3.1512\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 20s - loss: 2.1302 - val_loss: 2.7191\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 20s - loss: 1.9435 - val_loss: 2.3341\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 20s - loss: 1.7966 - val_loss: 1.9877\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 20s - loss: 1.6621 - val_loss: 1.9349\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 20s - loss: 1.5190 - val_loss: 1.9632\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 20s - loss: 1.3925 - val_loss: 1.9735\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.fit(X, y, epochs=5, batch_size=128, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_author_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Generating with seed: \"oes it really matter what it says this is some test text does it really matter what it says this is \"\n",
      "oes it really matter what it says this is some test text does it really matter what it says this is "
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.float64' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-af9aa51fe5bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"this is some test text does it really matter what it says \"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-16c3c1fabb4f>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(model, diversity, text)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mnext_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mnext_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mgenerated\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnext_char\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-16c3c1fabb4f>\u001b[0m in \u001b[0;36msample\u001b[0;34m(preds, temperature)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mexp_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_preds\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mprobas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.multinomial (numpy/random/mtrand/mtrand.c:37721)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'numpy.float64' has no len()"
     ]
    }
   ],
   "source": [
    "generate(model, diversity=0.7, text=\"this is some test text does it really matter what it says \" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generate(model, diversity=0.5, text=\"\"):\n",
    "    \"\"\"Generate text from a model\"\"\"\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    for i in range(5000):\n",
    "        x = np.zeros((1, maxlen), dtype=np.int)\n",
    "        for t, char in enumerate(sentence):\n",
    "            try:\n",
    "                x[0, t] = char_indices[char]\n",
    "            except:\n",
    "                print(sentence)\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Generating with seed: \" what it says this is some test text does it really matter what it says this is some test text does \"\n",
      " what it says this is some test text does it really matter what it says this is some test text does the chan store if the store in for Vis a singic and the places I nor for the store in find store is a gitter beat to the trist and the counders here to find. The sele the store with the storit and the chansic I was next the back the truck out. This is a git and the courder the trust and the ching store with the me to good and the next the countret to the this store is little back to the clourder the selsection for the stration's and my for probebles to have the store if friends out befould the chan net store with the park is can sele, I was not longer casteral friends. The store store is little this is this lace in findst and the inclure the selmen store in finds a befter the beet the car this store if the chan strater and closed to the counders. The strail also here in find store with the store with me and selestanding and the strorthors. So I had things a back to the mant I was not look in fried. The store in finds. The mest to the had a leng so this shore sprob food. The lent for the store for the store with the mant and the pricked out a find the chings realing and the mettreer smore with the store is later with got and the counders here for the stort and consorthing. The store store store is littely to get the onlite of the been to this shorth and I was trinks a fead a feas and my next the car the store in find store better here and class. The me of the only convenienter the the cound store. The store store is little but the only Nortar Crip and my ore also head with the things. The me so this shores and class store screet and the fings and long sometime store store is little also this is a back is this is a bittle this was small with the metr seet and my rear to was my sor the counder fead and was counders. The line for the strort and the Chrack is bak in friends and the chan in finds. The strail was come to this store is little bas in finds on the mettranter but the me store is little any the fincers and land store in findlest your for the stort office all park in things around findss. The ment store is fried was converite back the park in finds. The stores store with the counderth and check is this is a good sement and the boxt of the best thing to hand a fead and the me sell this stort of the one of the straf a feat and the chises with the only got for the stroirs and the mark is a firle a feed and the only metter slow with the stricket and my could my dere for real peating. The me something the counders to but the chan store is a Still a was a glass in finds that was conser this store shore for the strorth only regulay same but if my ere fur nite before the bands. The lane I was to gas here and the counder for the store is this is this is a bask in finds and the curont real for the store, The store in findst. The start in friends which when I gound the plation for Crome was strort and a fead all peat for your for the store and the chisies with the manity look in friends. The last in firl also but hive the was next the coure fent and the plattor for your for the store is little back to be. And the me of the stor"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-49291dfdfef3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"this is some test text does it really matter what it says \"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-9c3c76047201>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(model, diversity, text)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mnext_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mnext_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.4/site-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 902\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.4/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1583\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m         return self._predict_loop(f, ins,\n\u001b[0;32m-> 1585\u001b[0;31m                                   batch_size=batch_size, verbose=verbose)\n\u001b[0m\u001b[1;32m   1586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1587\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/.local/lib/python3.4/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1210\u001b[0m                 \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1212\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1213\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.4/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2227\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2228\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2229\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generate(model, diversity=0.5, text=\"this is some test text does it really matter what it says \" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
