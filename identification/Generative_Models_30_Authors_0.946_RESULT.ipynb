{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n"
     ]
    }
   ],
   "source": [
    "print(\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'yelp_academic_dataset_review.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3aefc1e4ba66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\nimport json\\nreviews = []\\nwith open(\"yelp_academic_dataset_review.json\") as f:\\n    index = 0\\n    for line in f:\\n        if index % 1000000 == 0:\\n            print(index)\\n        index += 1\\n        reviews.append(json.loads(line))'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'yelp_academic_dataset_review.json'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "reviews = []\n",
    "with open(\"yelp_academic_dataset_review.json\") as f:\n",
    "    index = 0\n",
    "    for line in f:\n",
    "        if index % 1000000 == 0:\n",
    "            print(index)\n",
    "        index += 1\n",
    "        reviews.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "prolific_reviewers = Counter([review['user_id'] for review in reviews]).most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "keep_ids = {pr[0] : 0 for pr in prolific_reviewers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "by_author = {} # author : \"review 1\\n review 2\\n review 3\"\n",
    "for review in reviews:\n",
    "    uid = review['user_id']\n",
    "    if uid in keep_ids:\n",
    "        uid = review['user_id']\n",
    "        if uid in by_author:\n",
    "            by_author[uid] += \"\\n{}\".format(review['text'])\n",
    "        else:\n",
    "            by_author[uid] = \"{}\".format(review['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "len(by_author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# check that we have at least 200000 characters for each author\n",
    "sorted([(len(by_author[key]), key) for key in by_author])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_chunks(l, n):\n",
    "    n = max(1, n)\n",
    "    return [l[i:i+n] for i in range(0, len(l), n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train_texts = []  # the first 100 000 chars from each author\n",
    "train_labels = [] # each author\n",
    "test_texts = []   # 100 texts of 1000 characters each (second 100 000 chars of each author)\n",
    "test_labels = []  # each author * 100\n",
    "\n",
    "author_int = {author: i for i,author in enumerate(by_author)}\n",
    "int_author = {author_int[author]: author for author in author_int}\n",
    "\n",
    "for author in by_author:\n",
    "    train_text = by_author[author][:50000]\n",
    "    train_label = author_int[author]\n",
    "    train_texts.append(train_text)\n",
    "    train_labels.append(train_label)\n",
    "    \n",
    "    short_texts = get_chunks(by_author[author][50000:100000], 1000)\n",
    "    for text in short_texts:\n",
    "        test_texts.append(text)\n",
    "        test_labels.append(author_int[author])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"train_texts\": train_texts,\n",
    "    \"train_labels\": train_labels,\n",
    "    \"test_texts\": test_texts,\n",
    "    \"test_labels\": test_labels\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"train_test_texts_labels.pickle\", \"wb\") as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"train_test_texts_labels.pickle\", \"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_texts = data['train_texts']\n",
    "train_labels = data['train_labels']\n",
    "test_texts = data['test_texts']\n",
    "test_labels = data['test_labels']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 1500\n"
     ]
    }
   ],
   "source": [
    "print(len(train_texts), len(test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# vectorization - chars to ints\n",
    "import string\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    \"\"\"Sample predictions from a probability array\"\"\"\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds + 1e-6) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def generate(model, diversity=0.5, text=\"\"):\n",
    "    \"\"\"Generate text from a model\"\"\"\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    for i in range(5000):\n",
    "        x = np.zeros((1, maxlen), dtype=np.int)\n",
    "        for t, char in enumerate(sentence):\n",
    "            try:\n",
    "                x[0, t] = char_indices[char]\n",
    "            except:\n",
    "                print(sentence)\n",
    "        preds = model.predict(x, verbose=0)[0][0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    return\n",
    "\n",
    "def vectorize(text):\n",
    "    \"\"\"Convert text into character sequences\"\"\"\n",
    "    step = 3\n",
    "    sentences = []\n",
    "    next_chars = []\n",
    "    for i in range(0, len(text) - maxlen, step):\n",
    "        sentences.append(text[i: i + maxlen])\n",
    "        next_chars.append(text[i + maxlen])\n",
    "    X = np.zeros((len(sentences), maxlen), dtype=np.int)\n",
    "    y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for t, char in enumerate(sentence):\n",
    "            X[i, t] = char_indices[char]\n",
    "        y[i, char_indices[next_chars[i]]] = 1\n",
    "    return X, y\n",
    "\n",
    "def clean_text(text, charset):\n",
    "    text = \" \".join(text.split())  # all white space is one space\n",
    "    text = \"\".join([x for x in text if x in charset])  # remove characters that we don't care about\n",
    "    return text\n",
    "\n",
    "def get_model(modelfile, freeze=False):\n",
    "    model = load_model(modelfile)\n",
    "    if freeze:\n",
    "        for layer in model.layers[:6]:\n",
    "            layer.trainable = False\n",
    "    return model\n",
    "\n",
    "chars = \" \" + string.ascii_letters + string.punctuation  # sorted to keep indices consistent\n",
    "charset = set(chars)  # for lookup\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "maxlen = 100  # must match length which generated model - the sequence length\n",
    "\n",
    "# load a pretrained language model\n",
    "modelfile = \"charlm2/model_middlemarch_cnn.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# test train an author specific model\n",
    "test_author_text = clean_text(train_texts[0], charset)\n",
    "test_author_model = get_model(modelfile, freeze=False)\n",
    "X, y = vectorize(test_author_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14725 samples, validate on 1637 samples\n",
      "Epoch 1/5\n",
      "14725/14725 [==============================] - 31s - loss: 2.4285 - main_out_loss: 2.0032 - aux_out_loss: 2.1265 - val_loss: 2.3065 - val_main_out_loss: 1.8932 - val_aux_out_loss: 2.0666\n",
      "Epoch 2/5\n",
      "14725/14725 [==============================] - 3s - loss: 1.9441 - main_out_loss: 1.5285 - aux_out_loss: 2.0778 - val_loss: 2.2461 - val_main_out_loss: 1.8360 - val_aux_out_loss: 2.0503\n",
      "Epoch 3/5\n",
      "14725/14725 [==============================] - 3s - loss: 1.7531 - main_out_loss: 1.3430 - aux_out_loss: 2.0509 - val_loss: 2.2212 - val_main_out_loss: 1.8128 - val_aux_out_loss: 2.0421\n",
      "Epoch 4/5\n",
      "14725/14725 [==============================] - 3s - loss: 1.6006 - main_out_loss: 1.1958 - aux_out_loss: 2.0238 - val_loss: 2.2239 - val_main_out_loss: 1.8170 - val_aux_out_loss: 2.0348\n",
      "Epoch 5/5\n",
      "14725/14725 [==============================] - 3s - loss: 1.4972 - main_out_loss: 1.0974 - aux_out_loss: 1.9989 - val_loss: 2.2472 - val_main_out_loss: 1.8414 - val_aux_out_loss: 2.0294\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6e3798ad30>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_author_model = get_model(modelfile, freeze=True)\n",
    "test_author_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", loss_weights=[1., 0.2])\n",
    "test_author_model.fit(X, [y, y], epochs=5, batch_size=128, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_author_model = get_model(modelfile, freeze=True)\n",
    "test_author_model.fit(X, [y, y], epochs=10, batch_size=128, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "fresh_model = model_from_json(test_author_model.to_json())\n",
    "fresh_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", loss_weights=[1.0, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_author_model.fit(X, [y, y], epochs=5, batch_size=128, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_author_model.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "print(mean(scores[:50]))\n",
    "print(mean(scores[50:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.boxplot(get_chunks(scores, 50)[:20]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "scores[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_labels[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "generate(test_author_model, diversity=0.7, text=\"this is some test text does it really matter what it says \"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_gru_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(GRU(256))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(y.shape[1], activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 30\n",
      "1 / 30\n",
      "2 / 30\n",
      "3 / 30\n",
      "4 / 30\n",
      "5 / 30\n",
      "6 / 30\n",
      "7 / 30\n",
      "8 / 30\n",
      "9 / 30\n",
      "10 / 30\n",
      "11 / 30\n",
      "12 / 30\n",
      "13 / 30\n",
      "14 / 30\n",
      "15 / 30\n",
      "16 / 30\n",
      "17 / 30\n",
      "18 / 30\n",
      "19 / 30\n",
      "20 / 30\n",
      "21 / 30\n",
      "22 / 30\n",
      "23 / 30\n",
      "24 / 30\n",
      "25 / 30\n",
      "26 / 30\n",
      "27 / 30\n",
      "28 / 30\n",
      "29 / 30\n",
      "CPU times: user 2h 7min 41s, sys: 9min 55s, total: 2h 17min 36s\n",
      "Wall time: 1h 25min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "author_models = []  # [(author_model, author_id), (author_model, author_id), ...] - ids are ints\n",
    "for i, train_text in enumerate(train_texts):\n",
    "    print(\"{} / {}\".format(i, len(train_texts)))\n",
    "    ct = clean_text(train_text, charset)\n",
    "    am = get_gru_model()\n",
    "    X, y = vectorize(ct)\n",
    "    am.fit(X, y, epochs=10, batch_size=128, verbose=0)\n",
    "    author_models.append((am, train_labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "1500\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "print(len(author_models))\n",
    "print(len(test_texts))\n",
    "print(len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182.57733333333334"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statistics import mean\n",
    "word_counts = [text.count(\" \") for text in test_texts]\n",
    "mean(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 182 words is quite short\n",
    "# Try to join 5 tests texts together\n",
    "longer_test_texts = get_chunks(test_texts, 5)\n",
    "longer_test_labels = get_chunks(test_labels, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all([len(set(x)) == 1 for x in longer_test_labels])  # Make sure that all combined labels are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "longer_test_texts = ['\\n'.join(chunk) for chunk in longer_test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "longer_test_labels = [chunk[0] for chunk in longer_test_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 300 .............................. 0:02:31.288444\n",
      "1 / 300 .............................. 0:00:51.179490\n",
      "2 / 300 .............................. 0:00:51.173313\n",
      "3 / 300 .............................. 0:00:51.177723\n",
      "4 / 300 .............................. 0:00:51.178888\n",
      "5 / 300 .............................. 0:00:51.174609\n",
      "6 / 300 .............................. 0:00:51.174918\n",
      "7 / 300 .............................. 0:00:51.175020\n",
      "8 / 300 .............................. 0:00:50.174255\n",
      "9 / 300 .............................. 0:00:51.175629\n",
      "10 / 300 .............................. 0:00:51.182774\n",
      "11 / 300 .............................. 0:00:51.173909\n",
      "12 / 300 .............................. 0:00:51.175227\n",
      "13 / 300 .............................. 0:00:51.176712\n",
      "14 / 300 .............................. 0:00:51.175450\n",
      "15 / 300 .............................. 0:00:51.176448\n",
      "16 / 300 .............................. 0:00:51.177725\n",
      "17 / 300 .............................. 0:00:51.172266\n",
      "18 / 300 .............................. 0:00:51.177899\n",
      "19 / 300 .............................. 0:00:51.178655\n",
      "20 / 300 .............................. 0:00:51.174676\n",
      "21 / 300 .............................. 0:00:51.173906\n",
      "22 / 300 .............................. 0:00:51.178488\n",
      "23 / 300 .............................. 0:00:51.176605\n",
      "24 / 300 .............................. 0:00:51.173529\n",
      "25 / 300 .............................. 0:00:51.177339\n",
      "26 / 300 .............................. 0:00:51.173687\n",
      "27 / 300 .............................. 0:00:51.177176\n",
      "28 / 300 .............................. 0:00:51.173846\n",
      "29 / 300 .............................. 0:00:51.178405\n",
      "30 / 300 .............................. 0:00:51.176994\n",
      "31 / 300 .............................. 0:00:51.175449\n",
      "32 / 300 .............................. 0:00:51.173769\n",
      "33 / 300 .............................. 0:00:51.174005\n",
      "34 / 300 .............................. 0:00:51.187461\n",
      "35 / 300 .............................. 0:00:51.164710\n",
      "36 / 300 .............................. 0:00:51.175437\n",
      "37 / 300 .............................. 0:00:51.177640\n",
      "38 / 300 .............................. 0:00:51.178936\n",
      "39 / 300 .............................. 0:00:51.184834\n",
      "40 / 300 .............................. 0:00:51.166280\n",
      "41 / 300 .............................. 0:00:50.190406\n",
      "42 / 300 .............................. 0:00:50.159403\n",
      "43 / 300 .............................. 0:00:50.172236\n",
      "44 / 300 .............................. 0:00:51.176120\n",
      "45 / 300 .............................. 0:00:50.174531\n",
      "46 / 300 .............................. 0:00:50.174458\n",
      "47 / 300 .............................. 0:00:50.175400\n",
      "48 / 300 .............................. 0:00:50.173298\n",
      "49 / 300 .............................. 0:00:50.172585\n",
      "50 / 300 .............................. 0:00:51.174413\n",
      "51 / 300 .............................. 0:00:51.177084\n",
      "52 / 300 .............................. 0:00:51.173970\n",
      "53 / 300 .............................. 0:00:51.177699\n",
      "54 / 300 .............................. 0:00:51.175063\n",
      "55 / 300 .............................. 0:00:51.175038\n",
      "56 / 300 .............................. 0:00:51.177727\n",
      "57 / 300 .............................. 0:00:51.172340\n",
      "58 / 300 .............................. 0:00:51.176488\n",
      "59 / 300 .............................. 0:00:51.176547\n",
      "60 / 300 .............................. 0:00:51.177139\n",
      "61 / 300 .............................. 0:00:51.174188\n",
      "62 / 300 .............................. 0:00:51.177321\n",
      "63 / 300 .............................. 0:00:51.173959\n",
      "64 / 300 .............................. 0:00:51.175347\n",
      "65 / 300 .............................. 0:00:51.176311\n",
      "66 / 300 .............................. 0:00:51.178032\n",
      "67 / 300 .............................. 0:00:51.174054\n",
      "68 / 300 .............................. 0:00:51.173692\n",
      "69 / 300 .............................. 0:00:51.179541\n",
      "70 / 300 .............................. 0:00:51.173901\n",
      "71 / 300 .............................. 0:00:51.176023\n",
      "72 / 300 .............................. 0:00:51.173525\n",
      "73 / 300 .............................. 0:00:51.178886\n",
      "74 / 300 .............................. 0:00:51.174936\n",
      "75 / 300 .............................. 0:00:50.187735\n",
      "76 / 300 .............................. 0:00:51.160999\n",
      "77 / 300 .............................. 0:00:51.176618\n",
      "78 / 300 .............................. 0:00:51.174879\n",
      "79 / 300 .............................. 0:00:51.175107\n",
      "80 / 300 .............................. 0:00:51.179415\n",
      "81 / 300 .............................. 0:00:51.177348\n",
      "82 / 300 .............................. 0:00:51.177245\n",
      "83 / 300 .............................. 0:00:51.175215\n",
      "84 / 300 .............................. 0:00:51.175957\n",
      "85 / 300 .............................. 0:00:51.174958\n",
      "86 / 300 .............................. 0:00:51.176613\n",
      "87 / 300 .............................. 0:00:51.176952\n",
      "88 / 300 .............................. 0:00:51.175690\n",
      "89 / 300 .............................. 0:00:51.175939\n",
      "90 / 300 .............................. 0:00:51.174457\n",
      "91 / 300 .............................. 0:00:51.179062\n",
      "92 / 300 .............................. 0:00:51.174775\n",
      "93 / 300 .............................. 0:00:51.175980\n",
      "94 / 300 .............................. 0:00:51.178920\n",
      "95 / 300 .............................. 0:00:51.179795\n",
      "96 / 300 .............................. 0:00:51.172393\n",
      "97 / 300 .............................. 0:00:51.174516\n",
      "98 / 300 .............................. 0:00:51.170542\n",
      "99 / 300 .............................. 0:00:51.178915\n",
      "100 / 300 .............................. 0:00:51.180038\n",
      "101 / 300 .............................. 0:00:51.175015\n",
      "102 / 300 .............................. 0:00:51.174311\n",
      "103 / 300 .............................. 0:00:51.177418\n",
      "104 / 300 .............................. 0:00:51.176920\n",
      "105 / 300 .............................. 0:00:51.173515\n",
      "106 / 300 .............................. 0:00:51.181038\n",
      "107 / 300 .............................. 0:00:51.202853\n",
      "108 / 300 .............................. 0:00:51.184644\n",
      "109 / 300 .............................. 0:00:51.167898\n",
      "110 / 300 .............................. 0:00:51.215967\n",
      "111 / 300 .............................. 0:00:51.168481\n",
      "112 / 300 .............................. 0:00:51.175537\n",
      "113 / 300 .............................. 0:00:51.177061\n",
      "114 / 300 .............................. 0:00:51.179034\n",
      "115 / 300 .............................. 0:00:51.175228\n",
      "116 / 300 .............................. 0:00:51.186413\n",
      "117 / 300 .............................. 0:00:51.165929\n",
      "118 / 300 .............................. 0:00:51.174603\n",
      "119 / 300 .............................. 0:00:51.177619\n",
      "120 / 300 .............................. 0:00:51.176494\n",
      "121 / 300 .............................. 0:00:51.181079\n",
      "122 / 300 .............................. 0:00:51.171276\n",
      "123 / 300 .............................. 0:00:51.176211\n",
      "124 / 300 .............................. 0:00:51.175394\n",
      "125 / 300 .............................. 0:00:51.177352\n",
      "126 / 300 .............................. 0:00:51.178771\n",
      "127 / 300 .............................. 0:00:51.170244\n",
      "128 / 300 .............................. 0:00:51.180673\n",
      "129 / 300 .............................. 0:00:51.167658\n",
      "130 / 300 .............................. 0:00:50.176917\n",
      "131 / 300 .............................. 0:00:51.174888\n",
      "132 / 300 .............................. 0:00:50.180936\n",
      "133 / 300 .............................. 0:00:51.173252\n",
      "134 / 300 .............................. 0:00:50.178311\n",
      "135 / 300 .............................. 0:00:50.175516\n",
      "136 / 300 .............................. 0:00:51.175793\n",
      "137 / 300 .............................. 0:00:51.174186\n",
      "138 / 300 .............................. 0:00:51.178260\n",
      "139 / 300 .............................. 0:00:51.175079\n",
      "140 / 300 .............................. 0:00:51.210531\n",
      "141 / 300 .............................. 0:00:51.177633\n",
      "142 / 300 .............................. 0:00:51.188711\n",
      "143 / 300 .............................. 0:00:51.158583\n",
      "144 / 300 .............................. 0:00:52.175554\n",
      "145 / 300 .............................. 0:00:51.214515\n",
      "146 / 300 .............................. 0:00:51.169833\n",
      "147 / 300 .............................. 0:00:51.178752\n",
      "148 / 300 .............................. 0:00:51.173903\n",
      "149 / 300 .............................. 0:00:51.176158\n",
      "150 / 300 .............................. 0:00:51.170403\n",
      "151 / 300 .............................. 0:00:51.175187\n",
      "152 / 300 .............................. 0:00:51.177604\n",
      "153 / 300 .............................. 0:00:51.188106\n",
      "154 / 300 .............................. 0:00:51.165934\n",
      "155 / 300 .............................. 0:00:51.175957\n",
      "156 / 300 .............................. 0:00:51.178379\n",
      "157 / 300 .............................. 0:00:51.178692\n",
      "158 / 300 .............................. 0:00:51.174804\n",
      "159 / 300 .............................. 0:00:51.176067\n",
      "160 / 300 .............................. 0:00:51.175623\n",
      "161 / 300 .............................. 0:00:51.178304\n",
      "162 / 300 .............................. 0:00:51.179223\n",
      "163 / 300 .............................. 0:00:51.176224\n",
      "164 / 300 .............................. 0:00:51.174375\n",
      "165 / 300 .............................. 0:00:51.178115\n",
      "166 / 300 .............................. 0:00:51.176905\n",
      "167 / 300 .............................. 0:00:51.176677\n",
      "168 / 300 .............................. 0:00:51.174560\n",
      "169 / 300 .............................. 0:00:51.177824\n",
      "170 / 300 .............................. 0:00:51.176433\n",
      "171 / 300 .............................. 0:00:51.175933\n",
      "172 / 300 .............................. 0:00:51.177404\n",
      "173 / 300 .............................. 0:00:51.170946\n",
      "174 / 300 .............................. 0:00:51.180193\n",
      "175 / 300 .............................. 0:00:51.176225\n",
      "176 / 300 .............................. 0:00:51.177783\n",
      "177 / 300 .............................. 0:00:51.171523\n",
      "178 / 300 .............................. 0:00:51.176259\n",
      "179 / 300 .............................. 0:00:51.176882\n",
      "180 / 300 .............................. 0:00:51.182130\n",
      "181 / 300 .............................. 0:00:51.168177\n",
      "182 / 300 .............................. 0:00:51.175426\n",
      "183 / 300 .............................. 0:00:51.176105\n",
      "184 / 300 .............................. 0:00:51.173707\n",
      "185 / 300 .............................. 0:00:51.174955\n",
      "186 / 300 .............................. 0:00:51.177609\n",
      "187 / 300 .............................. 0:00:51.175490\n",
      "188 / 300 .............................. 0:00:51.175783\n",
      "189 / 300 .............................. 0:00:51.175582\n",
      "190 / 300 .............................. 0:00:51.182514\n",
      "191 / 300 .............................. 0:00:51.170709\n",
      "192 / 300 .............................. 0:00:51.178122\n",
      "193 / 300 .............................. 0:00:51.175523\n",
      "194 / 300 .............................. 0:00:51.173464\n",
      "195 / 300 .............................. 0:00:51.175323\n",
      "196 / 300 .............................. 0:00:51.179480\n",
      "197 / 300 .............................. 0:00:51.173107\n",
      "198 / 300 .............................. 0:00:51.177853\n",
      "199 / 300 .............................. 0:00:51.174999\n",
      "200 / 300 .............................. 0:00:51.177122\n",
      "201 / 300 .............................. 0:00:51.172105\n",
      "202 / 300 .............................. 0:00:51.179793\n",
      "203 / 300 .............................. 0:00:51.182110\n",
      "204 / 300 .............................. 0:00:51.167497\n",
      "205 / 300 .............................. 0:00:51.177999\n",
      "206 / 300 .............................. 0:00:51.176153\n",
      "207 / 300 .............................. 0:00:51.174878\n",
      "208 / 300 .............................. 0:00:51.182296\n",
      "209 / 300 .............................. 0:00:51.177290\n",
      "210 / 300 .............................. 0:00:51.183812\n",
      "211 / 300 .............................. 0:00:51.166394\n",
      "212 / 300 .............................. 0:00:51.177871\n",
      "213 / 300 .............................. 0:00:51.178013\n",
      "214 / 300 .............................. 0:00:51.173409\n",
      "215 / 300 .............................. 0:00:51.184343\n",
      "216 / 300 .............................. 0:00:51.168667\n",
      "217 / 300 .............................. 0:00:51.175379\n",
      "218 / 300 .............................. 0:00:51.174167\n",
      "219 / 300 .............................. 0:00:51.177045\n",
      "220 / 300 .............................. 0:00:51.176220\n",
      "221 / 300 .............................. 0:00:51.173788\n",
      "222 / 300 .............................. 0:00:51.181806\n",
      "223 / 300 .............................. 0:00:51.167492\n",
      "224 / 300 .............................. 0:00:51.173918\n",
      "225 / 300 .............................. 0:00:51.174079\n",
      "226 / 300 .............................. 0:00:51.176899\n",
      "227 / 300 .............................. 0:00:51.173055\n",
      "228 / 300 .............................. 0:00:51.175070\n",
      "229 / 300 .............................. 0:00:51.175152\n",
      "230 / 300 .............................. 0:00:51.176124\n",
      "231 / 300 .............................. 0:00:51.177691\n",
      "232 / 300 .............................. 0:00:51.176015\n",
      "233 / 300 .............................. 0:00:51.178970\n",
      "234 / 300 .............................. 0:00:51.178220\n",
      "235 / 300 .............................. 0:00:51.178071\n",
      "236 / 300 .............................. 0:00:51.174515\n",
      "237 / 300 .............................. 0:00:51.179429\n",
      "238 / 300 .............................. 0:00:51.172675\n",
      "239 / 300 .............................. 0:00:51.175497\n",
      "240 / 300 .............................. 0:00:50.176414\n",
      "241 / 300 .............................. 0:00:50.172010\n",
      "242 / 300 .............................. 0:00:50.174945\n",
      "243 / 300 .............................. 0:00:51.172951\n",
      "244 / 300 .............................. 0:00:50.182899\n",
      "245 / 300 .............................. 0:00:50.164026\n",
      "246 / 300 .............................. 0:00:50.177313\n",
      "247 / 300 .............................. 0:00:51.168450\n",
      "248 / 300 .............................. 0:00:50.173934\n",
      "249 / 300 .............................. 0:00:50.178016\n",
      "250 / 300 .............................. 0:00:51.176614\n",
      "251 / 300 .............................. 0:00:51.209867\n",
      "252 / 300 .............................. 0:00:51.175213\n",
      "253 / 300 .............................. 0:00:51.174671\n",
      "254 / 300 .............................. 0:00:51.177726\n",
      "255 / 300 .............................. 0:00:51.174787\n",
      "256 / 300 .............................. 0:00:51.179391\n",
      "257 / 300 .............................. 0:00:51.174993\n",
      "258 / 300 .............................. 0:00:51.174716\n",
      "259 / 300 .............................. 0:00:51.174931\n",
      "260 / 300 .............................. 0:00:51.177169\n",
      "261 / 300 .............................. 0:00:51.181541\n",
      "262 / 300 .............................. 0:00:51.175039\n",
      "263 / 300 .............................. 0:00:51.185943\n",
      "264 / 300 .............................. 0:00:51.179661\n",
      "265 / 300 .............................. 0:00:51.208737\n",
      "266 / 300 .............................. 0:00:51.195666\n",
      "267 / 300 .............................. 0:00:51.182876\n",
      "268 / 300 .............................. 0:00:51.205462\n",
      "269 / 300 .............................. 0:00:51.231702\n",
      "270 / 300 .............................. 0:00:51.186111\n",
      "271 / 300 .............................. 0:00:51.178185\n",
      "272 / 300 .............................. 0:00:51.178324\n",
      "273 / 300 .............................. 0:00:51.175690\n",
      "274 / 300 .............................. 0:00:51.174728\n",
      "275 / 300 .............................. 0:00:51.175940\n",
      "276 / 300 .............................. 0:00:51.179890\n",
      "277 / 300 .............................. 0:00:51.172051\n",
      "278 / 300 .............................. 0:00:51.173613\n",
      "279 / 300 .............................. 0:00:51.177032\n",
      "280 / 300 .............................. 0:00:51.177012\n",
      "281 / 300 .............................. 0:00:50.207732\n",
      "282 / 300 .............................. 0:00:50.205408\n",
      "283 / 300 .............................. 0:00:50.170071\n",
      "284 / 300 .............................. 0:00:51.176302\n",
      "285 / 300 .............................. 0:00:51.178650\n",
      "286 / 300 .............................. 0:00:50.240493\n",
      "287 / 300 .............................. 0:00:51.206116\n",
      "288 / 300 .............................. 0:00:50.245740\n",
      "289 / 300 .............................. 0:00:51.168110\n",
      "290 / 300 .............................. 0:00:51.177129\n",
      "291 / 300 .............................. 0:00:51.178068\n",
      "292 / 300 .............................. 0:00:50.211047\n",
      "293 / 300 .............................. 0:00:50.207350\n",
      "294 / 300 .............................. 0:00:51.173662\n",
      "295 / 300 .............................. 0:00:50.203746\n",
      "296 / 300 .............................. 0:00:50.251217\n",
      "297 / 300 .............................. 0:00:50.213163\n",
      "298 / 300 .............................. 0:00:50.263549\n",
      "299 / 300 .............................. 0:00:51.171803\n",
      "CPU times: user 3h 47min 55s, sys: 0 ns, total: 3h 47min 55s\n",
      "Wall time: 4h 17min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from random import shuffle\n",
    "from datetime import datetime\n",
    "\n",
    "def get_predictions(author_models, test_texts, test_labels):\n",
    "    \"\"\"Evaluate each text for each author_model and append first metric to predictions\"\"\"\n",
    "    indicies = list(range(len(test_texts)))\n",
    "\n",
    "    test_texts = np.array(test_texts)\n",
    "    test_labels = np.array(test_labels)\n",
    "\n",
    "    test_texts = test_texts[indicies]\n",
    "    test_labels = test_labels[indicies]\n",
    "\n",
    "    predictions = []\n",
    "    for i, text in enumerate(test_texts):\n",
    "        t1 = datetime.now()\n",
    "        print(\"{} / {}\".format(i, len(test_texts)), end=\" \")\n",
    "        X, y = vectorize(clean_text(text, charset))\n",
    "\n",
    "        losses = []\n",
    "        for am in author_models:\n",
    "            print(\".\", end=\"\")\n",
    "            model = am[0]\n",
    "            label = am[1]\n",
    "            loss = model.evaluate(X, y, verbose=0)\n",
    "            losses.append((loss, label))\n",
    "        print(\" {}\".format(datetime.now() - t1))\n",
    "        predictions.append(losses)\n",
    "    return predictions\n",
    "    \n",
    "\n",
    "predictions_long = get_predictions(author_models, longer_test_texts, longer_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n"
     ]
    }
   ],
   "source": [
    "print(\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pred_is = []\n",
    "for pred in predictions_long:\n",
    "    pred_i = [p[0] for p in pred]\n",
    "    pred_is.append(pred_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pred_labs = [np.argmin(pred) for pred in pred_is]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94666666666666666"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(longer_test_labels, pred_labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_author_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_author_model.evaluatei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "len(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "word_vec = TfidfVectorizer(min_df=3, ngram_range=(1,2))\n",
    "char_vec = TfidfVectorizer(min_df=3, ngram_range=(2,5))\n",
    "\n",
    "fu = FeatureUnion([\n",
    "    ('word', word_vec),\n",
    "    ('char', char_vec)\n",
    "])\n",
    "\n",
    "\n",
    "X_train = fu.fit_transform(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_test = fu.transform(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "svm.fit(X_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "preds = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "accuracy_score(test_labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_test_longer = fu.transform(longer_test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "preds = svm.predict(X_test_longer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "accuracy_score(longer_test_labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X, y = vectorize(clean_text(train_texts[3], charset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_author_model.layers[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14725 samples, validate on 1637 samples\n",
      "Epoch 1/13\n",
      "14725/14725 [==============================] - 18s - loss: 3.1623 - val_loss: 3.9301\n",
      "Epoch 2/13\n",
      "14725/14725 [==============================] - 15s - loss: 2.4849 - val_loss: 3.2724\n",
      "Epoch 3/13\n",
      "14725/14725 [==============================] - 15s - loss: 2.2303 - val_loss: 2.7735\n",
      "Epoch 4/13\n",
      "14725/14725 [==============================] - 15s - loss: 2.0664 - val_loss: 2.3324\n",
      "Epoch 5/13\n",
      "14725/14725 [==============================] - 15s - loss: 1.9487 - val_loss: 2.0202\n",
      "Epoch 6/13\n",
      "14725/14725 [==============================] - 15s - loss: 1.8679 - val_loss: 1.9499\n",
      "Epoch 7/13\n",
      "14725/14725 [==============================] - 15s - loss: 1.7817 - val_loss: 1.9287\n",
      "Epoch 8/13\n",
      "14725/14725 [==============================] - 15s - loss: 1.6960 - val_loss: 1.9165\n",
      "Epoch 9/13\n",
      "14725/14725 [==============================] - 15s - loss: 1.6509 - val_loss: 1.9007\n",
      "Epoch 10/13\n",
      "14725/14725 [==============================] - 15s - loss: 1.5811 - val_loss: 1.8963\n",
      "Epoch 11/13\n",
      "14725/14725 [==============================] - 15s - loss: 1.5102 - val_loss: 1.8982\n",
      "Epoch 12/13\n",
      "14725/14725 [==============================] - 15s - loss: 1.4347 - val_loss: 1.9258\n",
      "Epoch 13/13\n",
      "14725/14725 [==============================] - 15s - loss: 1.3818 - val_loss: 1.9303\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nLSTM, BatchNorm\\nTrain on 14929 samples, validate on 1659 samples\\nEpoch 1/5\\n14929/14929 [==============================] - 75s - loss: 3.0403 - val_loss: 3.4757\\nEpoch 2/5\\n14929/14929 [==============================] - 60s - loss: 2.3156 - val_loss: 2.9687\\n\\nLSTM\\nTrain on 14929 samples, validate on 1659 samples\\nEpoch 1/5\\n14929/14929 [==============================] - 75s - loss: 3.1259 - val_loss: 2.7865\\nEpoch 2/5\\n14929/14929 [==============================] - 60s - loss: 2.6150 - val_loss: 2.3894\\n\\nCNN(5), LSTM  # faster, needs more epochs\\nTrain on 14929 samples, validate on 1659 samples\\nEpoch 1/5\\n14929/14929 [==============================] - 42s - loss: 3.1579 - val_loss: 2.9987\\nEpoch 2/5\\n14929/14929 [==============================] - 26s - loss: 2.8874 - val_loss: 2.6994\\nEpoch 3/5\\n14929/14929 [==============================] - 26s - loss: 2.6220 - val_loss: 2.4879\\nEpoch 4/5\\n14929/14929 [==============================] - 26s - loss: 2.4309 - val_loss: 2.3942\\nEpoch 5/5\\n14929/14929 [==============================] - 26s - loss: 2.2950 - val_loss: 2.2902\\n\\nCNN(5), CNN(3), LSTM doesn't drop below 3.0 in 5 epochs\\n\\n\\nEmbedding, BatchNorm, GRU, BatchNorm\\nTrain on 14725 samples, validate on 1637 samples\\nEpoch 1/20\\n14725/14725 [==============================] - 16s - loss: 2.9240 - val_loss: 3.9516\\nEpoch 2/20\\n14725/14725 [==============================] - 15s - loss: 2.2447 - val_loss: 3.3667\\nEpoch 3/20\\n14725/14725 [==============================] - 15s - loss: 2.0054 - val_loss: 2.8011\\nEpoch 4/20\\n14725/14725 [==============================] - 15s - loss: 1.8388 - val_loss: 2.3477\\nEpoch 5/20\\n14725/14725 [==============================] - 15s - loss: 1.7122 - val_loss: 2.0196\\nEpoch 6/20\\n14725/14725 [==============================] - 15s - loss: 1.6069 - val_loss: 1.9417\\nEpoch 7/20\\n14725/14725 [==============================] - 15s - loss: 1.5044 - val_loss: 1.9541\\nEpoch 8/20\\n14725/14725 [==============================] - 15s - loss: 1.3987 - val_loss: 1.9512\\nEpoch 9/20\\n14725/14725 [==============================] - 15s - loss: 1.2940 - val_loss: 1.9921\\nEpoch 10/20\\n14725/14725 [==============================] - 15s - loss: 1.1850 - val_loss: 2.0424\\n\\nTrain on 14725 samples, validate on 1637 samples\\nEpoch 1/20\\n14725/14725 [==============================] - 17s - loss: 3.2577 - val_loss: 3.8892\\nEpoch 2/20\\n14725/14725 [==============================] - 15s - loss: 2.5444 - val_loss: 3.2364\\nEpoch 3/20\\n14725/14725 [==============================] - 15s - loss: 2.2915 - val_loss: 2.7205\\nEpoch 4/20\\n14725/14725 [==============================] - 15s - loss: 2.1258 - val_loss: 2.3477\\nEpoch 5/20\\n14725/14725 [==============================] - 15s - loss: 2.0219 - val_loss: 2.0588\\nEpoch 6/20\\n14725/14725 [==============================] - 15s - loss: 1.9386 - val_loss: 1.9478\\nEpoch 7/20\\n14725/14725 [==============================] - 15s - loss: 1.8762 - val_loss: 1.9152\\nEpoch 8/20\\n14725/14725 [==============================] - 15s - loss: 1.7994 - val_loss: 1.9076\\nEpoch 9/20\\n14725/14725 [==============================] - 15s - loss: 1.7224 - val_loss: 1.8829\\nEpoch 10/20\\n14725/14725 [==============================] - 15s - loss: 1.6783 - val_loss: 1.9007\\nEpoch 11/20\\n14725/14725 [==============================] - 15s - loss: 1.6154 - val_loss: 1.8910\\nEpoch 12/20\\n14725/14725 [==============================] - 15s - loss: 1.5485 - val_loss: 1.8920\\nEpoch 13/20\\n14725/14725 [==============================] - 15s - loss: 1.4844 - val_loss: 1.9198\\nEpoch 14/20\\n14725/14725 [==============================] - 15s - loss: 1.4291 - val_loss: 1.9193\\nEpoch 15/20\\n14725/14725 [==============================] - 15s - loss: 1.3670 - val_loss: 1.9295\\nEpoch 16/20\\n14725/14725 [==============================] - 15s - loss: 1.3028 - val_loss: 1.9752\\nEpoch 17/20\\n14725/14725 [==============================] - 15s - loss: 1.2321 - val_loss: 1.9969\\nEpoch 18/20\\n14725/14725 [==============================] - 15s - loss: 1.2159 - val_loss: 2.0431\\nEpoch 19/20\\n14725/14725 [==============================] - 15s - loss: 1.1391 - val_loss: 2.0709\\nEpoch 20/20\\n14725/14725 [==============================] - 15s - loss: 1.1321 - val_loss: 2.1172\\n\\n\\nmodel = Sequential()\\nmodel.add(Embedding(input_dim=len(charset), output_dim=100))\\nmodel.add(BatchNormalization())\\nmodel.add(GRU(256))\\nmodel.add(Dropout(0.5))\\nmodel.add(BatchNormalization())\\nmodel.add(Dense(y.shape[1], activation='softmax'))\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\\nmodel.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\\nTrain on 14725 samples, validate on 1637 samples\\nEpoch 1/20\\n14725/14725 [==============================] - 17s - loss: 3.2577 - val_loss: 3.8892\\nEpoch 2/20\\n14725/14725 [==============================] - 15s - loss: 2.5444 - val_loss: 3.2364\\nEpoch 3/20\\n14725/14725 [==============================] - 15s - loss: 2.2915 - val_loss: 2.7205\\nEpoch 4/20\\n14725/14725 [==============================] - 15s - loss: 2.1258 - val_loss: 2.3477\\nEpoch 5/20\\n14725/14725 [==============================] - 15s - loss: 2.0219 - val_loss: 2.0588\\nEpoch 6/20\\n14725/14725 [==============================] - 15s - loss: 1.9386 - val_loss: 1.9478\\nEpoch 7/20\\n14725/14725 [==============================] - 15s - loss: 1.8762 - val_loss: 1.9152\\nEpoch 8/20\\n14725/14725 [==============================] - 15s - loss: 1.7994 - val_loss: 1.9076\\nEpoch 9/20\\n14725/14725 [==============================] - 15s - loss: 1.7224 - val_loss: 1.8829\\nEpoch 10/20\\n14725/14725 [==============================] - 15s - loss: 1.6783 - val_loss: 1.9007\\nEpoch 11/20\\n14725/14725 [==============================] - 15s - loss: 1.6154 - val_loss: 1.8910\\nEpoch 12/20\\n14725/14725 [==============================] - 15s - loss: 1.5485 - val_loss: 1.8920\\nEpoch 13/20\\n14725/14725 [==============================] - 15s - loss: 1.4844 - val_loss: 1.9198\\nEpoch 14/20\\n14725/14725 [==============================] - 15s - loss: 1.4291 - val_loss: 1.9193\\nEpoch 15/20\\n14725/14725 [==============================] - 15s - loss: 1.3670 - val_loss: 1.9295\\nEpoch 16/20\\n14725/14725 [==============================] - 15s - loss: 1.3028 - val_loss: 1.9752\\nEpoch 17/20\\n14725/14725 [==============================] - 15s - loss: 1.2321 - val_loss: 1.9969\\nEpoch 18/20\\n14725/14725 [==============================] - 15s - loss: 1.2159 - val_loss: 2.0431\\nEpoch 19/20\\n14725/14725 [==============================] - 15s - loss: 1.1391 - val_loss: 2.0709\\nEpoch 20/20\\n14725/14725 [==============================] - 15s - loss: 1.1321 - val_loss: 2.1172\\n\\n\\nmodel = Sequential()\\nmodel.add(Embedding(input_dim=len(charset), output_dim=100))\\nmodel.add(GRU(256))\\nmodel.add(BatchNormalization())\\nmodel.add(Dropout(0.5))\\nmodel.add(Dense(y.shape[1], activation='softmax'))\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\\nmodel.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\\nTrain on 14725 samples, validate on 1637 samples\\nEpoch 1/20\\n14725/14725 [==============================] - 16s - loss: 3.1731 - val_loss: 3.5648\\nEpoch 2/20\\n14725/14725 [==============================] - 14s - loss: 2.4964 - val_loss: 2.9875\\nEpoch 3/20\\n14725/14725 [==============================] - 14s - loss: 2.3446 - val_loss: 2.7695\\nEpoch 4/20\\n14725/14725 [==============================] - 14s - loss: 2.2928 - val_loss: 2.6010\\nEpoch 5/20\\n14725/14725 [==============================] - 14s - loss: 2.2642 - val_loss: 2.3900\\nEpoch 6/20\\n14725/14725 [==============================] - 14s - loss: 2.2373 - val_loss: 2.5023\\nEpoch 7/20\\n14725/14725 [==============================] - 14s - loss: 2.2186 - val_loss: 2.3780\\nEpoch 8/20\\n14725/14725 [==============================] - 14s - loss: 2.2029 - val_loss: 2.4928\\nEpoch 9/20\\n14725/14725 [==============================] - 14s - loss: 2.1852 - val_loss: 2.3480\\nEpoch 10/20\\n14725/14725 [==============================] - 14s - loss: 2.1745 - val_loss: 2.4801\\nEpoch 11/20\\n14725/14725 [==============================] - 14s - loss: 2.1563 - val_loss: 2.3951\\nEpoch 12/20\\n14725/14725 [==============================] - 14s - loss: 2.1391 - val_loss: 2.4133\\nEpoch 13/20\\n14725/14725 [==============================] - 14s - loss: 2.1192 - val_loss: 2.5896\\nEpoch 14/20\\n14725/14725 [==============================] - 14s - loss: 2.1020 - val_loss: 2.2692\\nEpoch 15/20\\n14725/14725 [==============================] - 14s - loss: 2.0770 - val_loss: 2.2179\\nEpoch 16/20\\n14725/14725 [==============================] - 14s - loss: 2.0643 - val_loss: 2.2822\\nEpoch 17/20\\n 6784/14725 [============>.................] - ETA: 7s - loss: 2.0302\\n\\n\\nmodel = Sequential()\\nmodel.add(Embedding(input_dim=len(charset), output_dim=100))\\nmodel.add(Dropout(0.2))\\nmodel.add(BatchNormalization())\\nmodel.add(GRU(256))\\nmodel.add(Dropout(0.2))\\nmodel.add(BatchNormalization())\\nmodel.add(Dense(y.shape[1], activation='softmax'))\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\\nmodel.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\\nTrain on 14725 samples, validate on 1637 samples\\nEpoch 1/20\\n14725/14725 [==============================] - 17s - loss: 3.1528 - val_loss: 3.9042\\nEpoch 2/20\\n14725/14725 [==============================] - 15s - loss: 2.4658 - val_loss: 3.2093\\nEpoch 3/20\\n14725/14725 [==============================] - 15s - loss: 2.2001 - val_loss: 2.6764\\nEpoch 4/20\\n14725/14725 [==============================] - 15s - loss: 2.0358 - val_loss: 2.2919\\nEpoch 5/20\\n14725/14725 [==============================] - 15s - loss: 1.9497 - val_loss: 2.0060\\nEpoch 6/20\\n14725/14725 [==============================] - 15s - loss: 1.8588 - val_loss: 1.9313\\nEpoch 7/20\\n14725/14725 [==============================] - 15s - loss: 1.7883 - val_loss: 1.9153\\nEpoch 8/20\\n14725/14725 [==============================] - 15s - loss: 1.7034 - val_loss: 1.9145\\nEpoch 9/20\\n14725/14725 [==============================] - 15s - loss: 1.6382 - val_loss: 1.8979\\nEpoch 10/20\\n14725/14725 [==============================] - 15s - loss: 1.5827 - val_loss: 1.8864\\nEpoch 11/20\\n14725/14725 [==============================] - 15s - loss: 1.5093 - val_loss: 1.8967\\nEpoch 12/20\\n14725/14725 [==============================] - 15s - loss: 1.4472 - val_loss: 1.9040\\nEpoch 13/20\\n14725/14725 [==============================] - 15s - loss: 1.3809 - val_loss: 1.9227\\nEpoch 14/20\\n14725/14725 [==============================] - 15s - loss: 1.3225 - val_loss: 1.9469\\nEpoch 15/20\\n14725/14725 [==============================] - 15s - loss: 1.2516 - val_loss: 1.9862\\nEpoch 16/20\\n14725/14725 [==============================] - 15s - loss: 1.2094 - val_loss: 1.9963\\nEpoch 17/20\\n14725/14725 [==============================] - 15s - loss: 1.1658 - val_loss: 2.0331\\nEpoch 18/20\\n14725/14725 [==============================] - 15s - loss: 1.0851 - val_loss: 2.0452\\nEpoch 19/20\\n14725/14725 [==============================] - 15s - loss: 1.0394 - val_loss: 2.0810\\nEpoch 20/20\\n14725/14725 [==============================] - 15s - loss: 0.9903 - val_loss: 2.1283\\n\\n\\nmodel = Sequential()\\nmodel.add(Embedding(input_dim=len(charset), output_dim=100))\\nmodel.add(Dropout(0.3))\\nmodel.add(BatchNormalization())\\nmodel.add(GRU(256))\\nmodel.add(Dropout(0.3))\\nmodel.add(BatchNormalization())\\nmodel.add(Dense(y.shape[1], activation='softmax'))\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\\nmodel.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\\nTrain on 14725 samples, validate on 1637 samples\\nEpoch 1/20\\n14725/14725 [==============================] - 17s - loss: 3.2991 - val_loss: 3.8902\\nEpoch 2/20\\n14725/14725 [==============================] - 15s - loss: 2.5672 - val_loss: 3.1627\\nEpoch 3/20\\n14725/14725 [==============================] - 15s - loss: 2.2731 - val_loss: 2.6340\\nEpoch 4/20\\n14725/14725 [==============================] - 15s - loss: 2.1316 - val_loss: 2.2594\\nEpoch 5/20\\n14725/14725 [==============================] - 15s - loss: 2.0249 - val_loss: 2.0159\\nEpoch 6/20\\n14725/14725 [==============================] - 15s - loss: 1.9571 - val_loss: 1.9456\\nEpoch 7/20\\n14725/14725 [==============================] - 15s - loss: 1.8789 - val_loss: 1.9213\\nEpoch 8/20\\n14725/14725 [==============================] - 15s - loss: 1.8233 - val_loss: 1.8924\\nEpoch 9/20\\n14725/14725 [==============================] - 15s - loss: 1.7575 - val_loss: 1.8987\\n\\n\\n\\nmodel = Sequential()\\nmodel.add(Embedding(input_dim=len(charset), output_dim=100))\\nmodel.add(Dropout(0.5))\\nmodel.add(BatchNormalization())\\nmodel.add(GRU(256))\\nmodel.add(Dropout(0.5))\\nmodel.add(BatchNormalization())\\nmodel.add(Dense(y.shape[1], activation='softmax'))\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\\nmodel.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\\nTrain on 14725 samples, validate on 1637 samples\\nEpoch 1/20\\n14725/14725 [==============================] - 17s - loss: 3.6036 - val_loss: 3.7924\\nEpoch 2/20\\n14725/14725 [==============================] - 15s - loss: 2.7765 - val_loss: 3.0022\\nEpoch 3/20\\n14725/14725 [==============================] - 15s - loss: 2.4773 - val_loss: 2.5697\\nEpoch 4/20\\n14725/14725 [==============================] - 15s - loss: 2.3218 - val_loss: 2.2606\\nEpoch 5/20\\n14725/14725 [==============================] - 15s - loss: 2.2328 - val_loss: 2.0832\\nEpoch 6/20\\n14725/14725 [==============================] - 15s - loss: 2.1748 - val_loss: 2.0248\\nEpoch 7/20\\n14725/14725 [==============================] - 15s - loss: 2.1174 - val_loss: 1.9865\\nEpoch 8/20\\n14725/14725 [==============================] - 15s - loss: 2.0617 - val_loss: 1.9640\\nEpoch 9/20\\n14725/14725 [==============================] - 15s - loss: 2.0206 - val_loss: 1.9461\\nEpoch 10/20\\n14725/14725 [==============================] - 15s - loss: 1.9758 - val_loss: 1.9334\\nEpoch 11/20\\n14725/14725 [==============================] - 15s - loss: 1.9546 - val_loss: 1.9148\\nEpoch 12/20\\n14725/14725 [==============================] - 15s - loss: 1.9045 - val_loss: 1.9121\\nEpoch 13/20\\n14725/14725 [==============================] - 15s - loss: 1.8757 - val_loss: 1.8888\\nEpoch 14/20\\n14725/14725 [==============================] - 15s - loss: 1.8437 - val_loss: 1.8874\\nEpoch 15/20\\n14725/14725 [==============================] - 15s - loss: 1.8145 - val_loss: 1.8822\\nEpoch 16/20\\n14725/14725 [==============================] - 15s - loss: 1.7805 - val_loss: 1.8785\\nEpoch 17/20\\n14725/14725 [==============================] - 15s - loss: 1.7558 - val_loss: 1.8868\\nEpoch 18/20\\n14725/14725 [==============================] - 15s - loss: 1.7218 - val_loss: 1.8670\\nEpoch 19/20\\n14725/14725 [==============================] - 15s - loss: 1.7032 - val_loss: 1.8759\\nEpoch 20/20\\n14725/14725 [==============================] - 15s - loss: 1.6832 - val_loss: 1.8834\\n\\n\\nmodel = Sequential()\\nmodel.add(Embedding(input_dim=len(charset), output_dim=100))\\nmodel.add(Dropout(0.5))\\nmodel.add(BatchNormalization())\\nmodel.add(GRU(256, return_sequences=True))\\nmodel.add(Dropout(0.5))\\nmodel.add(BatchNormalization())\\nmodel.add(GRU(256))\\nmodel.add(Dropout(0.5))\\nmodel.add(BatchNormalization())\\nmodel.add(Dense(y.shape[1], activation='softmax'))\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\\nmodel.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\\nTrain on 14725 samples, validate on 1637 samples\\nEpoch 1/20\\n14725/14725 [==============================] - 33s - loss: 3.7814 - val_loss: 3.7282\\nEpoch 2/20\\n14725/14725 [==============================] - 29s - loss: 2.8453 - val_loss: 2.8542\\nEpoch 3/20\\n14725/14725 [==============================] - 29s - loss: 2.5178 - val_loss: 2.4434\\nEpoch 4/20\\n14725/14725 [==============================] - 29s - loss: 2.3762 - val_loss: 2.1894\\nEpoch 5/20\\n14725/14725 [==============================] - 29s - loss: 2.2896 - val_loss: 2.0862\\nEpoch 6/20\\n14725/14725 [==============================] - 29s - loss: 2.2254 - val_loss: 2.0516\\nEpoch 7/20\\n14725/14725 [==============================] - 29s - loss: 2.1565 - val_loss: 2.0133\\nEpoch 8/20\\n14725/14725 [==============================] - 29s - loss: 2.1132 - val_loss: 1.9992\\nEpoch 9/20\\n14725/14725 [==============================] - 29s - loss: 2.0798 - val_loss: 1.9881\\nEpoch 10/20\\n14725/14725 [==============================] - 29s - loss: 2.0509 - val_loss: 1.9784\\nEpoch 11/20\\n14725/14725 [==============================] - 29s - loss: 2.0198 - val_loss: 1.9618\\nEpoch 12/20\\n14725/14725 [==============================] - 29s - loss: 1.9822 - val_loss: 1.9383\\nEpoch 13/20\\n14725/14725 [==============================] - 29s - loss: 1.9437 - val_loss: 1.9300\\nEpoch 14/20\\n14725/14725 [==============================] - 29s - loss: 1.9198 - val_loss: 1.9163\\nEpoch 15/20\\n14725/14725 [==============================] - 29s - loss: 1.8989 - val_loss: 1.9160\\nEpoch 16/20\\n14725/14725 [==============================] - 29s - loss: 1.8866 - val_loss: 1.9085\\nEpoch 17/20\\n14725/14725 [==============================] - 29s - loss: 1.8493 - val_loss: 1.8965\\nEpoch 18/20\\n14725/14725 [==============================] - 29s - loss: 1.8248 - val_loss: 1.8878\\nEpoch 19/20\\n14725/14725 [==============================] - 29s - loss: 1.8037 - val_loss: 1.8870\\nEpoch 20/20\\n14725/14725 [==============================] - 29s - loss: 1.7724 - val_loss: 1.8862\\n\\n\\nmodel = Sequential()\\nmodel.add(Embedding(input_dim=len(charset), output_dim=100))\\nmodel.add(Dropout(0.2))\\nmodel.add(BatchNormalization())\\nmodel.add(GRU(256, return_sequences=True))\\nmodel.add(Dropout(0.2))\\nmodel.add(BatchNormalization())\\nmodel.add(GRU(256))\\nmodel.add(Dropout(0.2))\\nmodel.add(BatchNormalization())\\nmodel.add(Dense(y.shape[1], activation='softmax'))\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\\nmodel.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\\nTrain on 14725 samples, validate on 1637 samples\\nEpoch 1/20\\n14725/14725 [==============================] - 32s - loss: 3.2809 - val_loss: 3.7595\\nEpoch 2/20\\n14725/14725 [==============================] - 29s - loss: 2.4379 - val_loss: 2.9869\\nEpoch 3/20\\n14725/14725 [==============================] - 29s - loss: 2.1504 - val_loss: 2.5361\\nEpoch 4/20\\n14725/14725 [==============================] - 29s - loss: 1.9887 - val_loss: 2.1294\\nEpoch 5/20\\n14725/14725 [==============================] - 29s - loss: 1.8984 - val_loss: 1.9727\\nEpoch 6/20\\n14725/14725 [==============================] - 29s - loss: 1.7892 - val_loss: 1.9264\\nEpoch 7/20\\n14725/14725 [==============================] - 29s - loss: 1.7172 - val_loss: 1.9100\\nEpoch 8/20\\n14725/14725 [==============================] - 29s - loss: 1.6361 - val_loss: 1.9124\\nEpoch 9/20\\n14725/14725 [==============================] - 29s - loss: 1.5621 - val_loss: 1.9122\\nEpoch 10/20\\n14725/14725 [==============================] - 29s - loss: 1.4863 - val_loss: 1.9045\\nEpoch 11/20\\n14725/14725 [==============================] - 29s - loss: 1.4150 - val_loss: 1.9278\\nEpoch 12/20\\n14725/14725 [==============================] - 29s - loss: 1.3691 - val_loss: 1.9181\\nEpoch 13/20\\n14725/14725 [==============================] - 29s - loss: 1.2970 - val_loss: 1.9414\\nEpoch 14/20\\n 1536/14725 [==>...........................] - ETA: 25s - loss: 1.1475\\n \\n \\n \\nmodel = Sequential()\\nmodel.add(Embedding(input_dim=len(charset), output_dim=100))\\nmodel.add(BatchNormalization())\\nmodel.add(GRU(256))\\nmodel.add(Dropout(0.4))\\nmodel.add(BatchNormalization())\\nmodel.add(Dense(y.shape[1], activation='softmax'))\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\\nmodel.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\\n Train on 14725 samples, validate on 1637 samples\\nEpoch 1/20\\n14725/14725 [==============================] - 17s - loss: 3.1787 - val_loss: 3.9282\\nEpoch 2/20\\n14725/14725 [==============================] - 15s - loss: 2.5070 - val_loss: 3.2479\\nEpoch 3/20\\n14725/14725 [==============================] - 15s - loss: 2.2191 - val_loss: 2.7522\\nEpoch 4/20\\n14725/14725 [==============================] - 15s - loss: 2.0637 - val_loss: 2.3331\\nEpoch 5/20\\n14725/14725 [==============================] - 15s - loss: 1.9539 - val_loss: 2.0326\\nEpoch 6/20\\n14725/14725 [==============================] - 15s - loss: 1.8622 - val_loss: 1.9440\\nEpoch 7/20\\n14725/14725 [==============================] - 15s - loss: 1.7821 - val_loss: 1.9166\\nEpoch 8/20\\n14725/14725 [==============================] - 15s - loss: 1.7169 - val_loss: 1.8996\\nEpoch 9/20\\n14725/14725 [==============================] - 15s - loss: 1.6561 - val_loss: 1.8849\\nEpoch 10/20\\n14725/14725 [==============================] - 15s - loss: 1.5910 - val_loss: 1.9032\\nEpoch 11/20\\n14725/14725 [==============================] - 15s - loss: 1.5082 - val_loss: 1.8878\\nEpoch 12/20\\n14725/14725 [==============================] - 15s - loss: 1.4513 - val_loss: 1.9252\\nEpoch 13/20\\n 7552/14725 [==============>...............] - ETA: 7s - loss: 1.3534\\n \\n \\nmodel = Sequential()\\nmodel.add(Embedding(input_dim=len(charset), output_dim=100))\\nmodel.add(BatchNormalization())\\nmodel.add(GRU(512))\\nmodel.add(Dropout(0.4))\\nmodel.add(BatchNormalization())\\nmodel.add(Dense(y.shape[1], activation='softmax'))\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\\nmodel.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\\n Train on 14725 samples, validate on 1637 samples\\nEpoch 1/20\\n14725/14725 [==============================] - 23s - loss: 3.1173 - val_loss: 3.8415\\nEpoch 2/20\\n14725/14725 [==============================] - 20s - loss: 2.4202 - val_loss: 3.1512\\nEpoch 3/20\\n14725/14725 [==============================] - 20s - loss: 2.1302 - val_loss: 2.7191\\nEpoch 4/20\\n14725/14725 [==============================] - 20s - loss: 1.9435 - val_loss: 2.3341\\nEpoch 5/20\\n14725/14725 [==============================] - 20s - loss: 1.7966 - val_loss: 1.9877\\nEpoch 6/20\\n14725/14725 [==============================] - 20s - loss: 1.6621 - val_loss: 1.9349\\nEpoch 7/20\\n14725/14725 [==============================] - 20s - loss: 1.5190 - val_loss: 1.9632\\nEpoch 8/20\\n14725/14725 [==============================] - 20s - loss: 1.3925 - val_loss: 1.9735\\n\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM, Input, Embedding, Conv1D, MaxPooling1D, BatchNormalization, GRU\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# load ascii text and covert to lowercase\n",
    "\n",
    "# cnn = Dropout(0.2)(embedded)\n",
    "# cnn = Conv1D(128, 5, activation='relu')(cnn)\n",
    "# cnn = MaxPooling1D(pool_size=4)(cnn)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(256))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=13, batch_size=128, validation_split=0.1)\n",
    "\n",
    "\"\"\"\n",
    "LSTM, BatchNorm\n",
    "Train on 14929 samples, validate on 1659 samples\n",
    "Epoch 1/5\n",
    "14929/14929 [==============================] - 75s - loss: 3.0403 - val_loss: 3.4757\n",
    "Epoch 2/5\n",
    "14929/14929 [==============================] - 60s - loss: 2.3156 - val_loss: 2.9687\n",
    "\n",
    "LSTM\n",
    "Train on 14929 samples, validate on 1659 samples\n",
    "Epoch 1/5\n",
    "14929/14929 [==============================] - 75s - loss: 3.1259 - val_loss: 2.7865\n",
    "Epoch 2/5\n",
    "14929/14929 [==============================] - 60s - loss: 2.6150 - val_loss: 2.3894\n",
    "\n",
    "CNN(5), LSTM  # faster, needs more epochs\n",
    "Train on 14929 samples, validate on 1659 samples\n",
    "Epoch 1/5\n",
    "14929/14929 [==============================] - 42s - loss: 3.1579 - val_loss: 2.9987\n",
    "Epoch 2/5\n",
    "14929/14929 [==============================] - 26s - loss: 2.8874 - val_loss: 2.6994\n",
    "Epoch 3/5\n",
    "14929/14929 [==============================] - 26s - loss: 2.6220 - val_loss: 2.4879\n",
    "Epoch 4/5\n",
    "14929/14929 [==============================] - 26s - loss: 2.4309 - val_loss: 2.3942\n",
    "Epoch 5/5\n",
    "14929/14929 [==============================] - 26s - loss: 2.2950 - val_loss: 2.2902\n",
    "\n",
    "CNN(5), CNN(3), LSTM doesn't drop below 3.0 in 5 epochs\n",
    "\n",
    "\n",
    "Embedding, BatchNorm, GRU, BatchNorm\n",
    "Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 16s - loss: 2.9240 - val_loss: 3.9516\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.2447 - val_loss: 3.3667\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.0054 - val_loss: 2.8011\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.8388 - val_loss: 2.3477\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7122 - val_loss: 2.0196\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.6069 - val_loss: 1.9417\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.5044 - val_loss: 1.9541\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.3987 - val_loss: 1.9512\n",
    "Epoch 9/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.2940 - val_loss: 1.9921\n",
    "Epoch 10/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.1850 - val_loss: 2.0424\n",
    "\n",
    "Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 17s - loss: 3.2577 - val_loss: 3.8892\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.5444 - val_loss: 3.2364\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.2915 - val_loss: 2.7205\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.1258 - val_loss: 2.3477\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.0219 - val_loss: 2.0588\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.9386 - val_loss: 1.9478\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.8762 - val_loss: 1.9152\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7994 - val_loss: 1.9076\n",
    "Epoch 9/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7224 - val_loss: 1.8829\n",
    "Epoch 10/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.6783 - val_loss: 1.9007\n",
    "Epoch 11/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.6154 - val_loss: 1.8910\n",
    "Epoch 12/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.5485 - val_loss: 1.8920\n",
    "Epoch 13/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.4844 - val_loss: 1.9198\n",
    "Epoch 14/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.4291 - val_loss: 1.9193\n",
    "Epoch 15/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.3670 - val_loss: 1.9295\n",
    "Epoch 16/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.3028 - val_loss: 1.9752\n",
    "Epoch 17/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.2321 - val_loss: 1.9969\n",
    "Epoch 18/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.2159 - val_loss: 2.0431\n",
    "Epoch 19/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.1391 - val_loss: 2.0709\n",
    "Epoch 20/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.1321 - val_loss: 2.1172\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(256))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\n",
    "Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 17s - loss: 3.2577 - val_loss: 3.8892\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.5444 - val_loss: 3.2364\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.2915 - val_loss: 2.7205\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.1258 - val_loss: 2.3477\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.0219 - val_loss: 2.0588\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.9386 - val_loss: 1.9478\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.8762 - val_loss: 1.9152\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7994 - val_loss: 1.9076\n",
    "Epoch 9/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7224 - val_loss: 1.8829\n",
    "Epoch 10/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.6783 - val_loss: 1.9007\n",
    "Epoch 11/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.6154 - val_loss: 1.8910\n",
    "Epoch 12/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.5485 - val_loss: 1.8920\n",
    "Epoch 13/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.4844 - val_loss: 1.9198\n",
    "Epoch 14/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.4291 - val_loss: 1.9193\n",
    "Epoch 15/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.3670 - val_loss: 1.9295\n",
    "Epoch 16/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.3028 - val_loss: 1.9752\n",
    "Epoch 17/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.2321 - val_loss: 1.9969\n",
    "Epoch 18/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.2159 - val_loss: 2.0431\n",
    "Epoch 19/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.1391 - val_loss: 2.0709\n",
    "Epoch 20/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.1321 - val_loss: 2.1172\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "model.add(GRU(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\n",
    "Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 16s - loss: 3.1731 - val_loss: 3.5648\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.4964 - val_loss: 2.9875\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.3446 - val_loss: 2.7695\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.2928 - val_loss: 2.6010\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.2642 - val_loss: 2.3900\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.2373 - val_loss: 2.5023\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.2186 - val_loss: 2.3780\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.2029 - val_loss: 2.4928\n",
    "Epoch 9/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.1852 - val_loss: 2.3480\n",
    "Epoch 10/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.1745 - val_loss: 2.4801\n",
    "Epoch 11/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.1563 - val_loss: 2.3951\n",
    "Epoch 12/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.1391 - val_loss: 2.4133\n",
    "Epoch 13/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.1192 - val_loss: 2.5896\n",
    "Epoch 14/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.1020 - val_loss: 2.2692\n",
    "Epoch 15/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.0770 - val_loss: 2.2179\n",
    "Epoch 16/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.0643 - val_loss: 2.2822\n",
    "Epoch 17/20\n",
    " 6784/14725 [============>.................] - ETA: 7s - loss: 2.0302\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\n",
    "Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 17s - loss: 3.1528 - val_loss: 3.9042\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.4658 - val_loss: 3.2093\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.2001 - val_loss: 2.6764\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.0358 - val_loss: 2.2919\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.9497 - val_loss: 2.0060\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.8588 - val_loss: 1.9313\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7883 - val_loss: 1.9153\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7034 - val_loss: 1.9145\n",
    "Epoch 9/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.6382 - val_loss: 1.8979\n",
    "Epoch 10/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.5827 - val_loss: 1.8864\n",
    "Epoch 11/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.5093 - val_loss: 1.8967\n",
    "Epoch 12/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.4472 - val_loss: 1.9040\n",
    "Epoch 13/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.3809 - val_loss: 1.9227\n",
    "Epoch 14/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.3225 - val_loss: 1.9469\n",
    "Epoch 15/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.2516 - val_loss: 1.9862\n",
    "Epoch 16/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.2094 - val_loss: 1.9963\n",
    "Epoch 17/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.1658 - val_loss: 2.0331\n",
    "Epoch 18/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.0851 - val_loss: 2.0452\n",
    "Epoch 19/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.0394 - val_loss: 2.0810\n",
    "Epoch 20/20\n",
    "14725/14725 [==============================] - 15s - loss: 0.9903 - val_loss: 2.1283\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(256))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\n",
    "Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 17s - loss: 3.2991 - val_loss: 3.8902\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.5672 - val_loss: 3.1627\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.2731 - val_loss: 2.6340\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.1316 - val_loss: 2.2594\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.0249 - val_loss: 2.0159\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.9571 - val_loss: 1.9456\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.8789 - val_loss: 1.9213\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.8233 - val_loss: 1.8924\n",
    "Epoch 9/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7575 - val_loss: 1.8987\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(256))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\n",
    "Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 17s - loss: 3.6036 - val_loss: 3.7924\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.7765 - val_loss: 3.0022\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.4773 - val_loss: 2.5697\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.3218 - val_loss: 2.2606\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.2328 - val_loss: 2.0832\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.1748 - val_loss: 2.0248\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.1174 - val_loss: 1.9865\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.0617 - val_loss: 1.9640\n",
    "Epoch 9/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.0206 - val_loss: 1.9461\n",
    "Epoch 10/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.9758 - val_loss: 1.9334\n",
    "Epoch 11/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.9546 - val_loss: 1.9148\n",
    "Epoch 12/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.9045 - val_loss: 1.9121\n",
    "Epoch 13/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.8757 - val_loss: 1.8888\n",
    "Epoch 14/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.8437 - val_loss: 1.8874\n",
    "Epoch 15/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.8145 - val_loss: 1.8822\n",
    "Epoch 16/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7805 - val_loss: 1.8785\n",
    "Epoch 17/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7558 - val_loss: 1.8868\n",
    "Epoch 18/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7218 - val_loss: 1.8670\n",
    "Epoch 19/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7032 - val_loss: 1.8759\n",
    "Epoch 20/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.6832 - val_loss: 1.8834\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(256, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(256))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\n",
    "Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 33s - loss: 3.7814 - val_loss: 3.7282\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.8453 - val_loss: 2.8542\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.5178 - val_loss: 2.4434\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.3762 - val_loss: 2.1894\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.2896 - val_loss: 2.0862\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.2254 - val_loss: 2.0516\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.1565 - val_loss: 2.0133\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.1132 - val_loss: 1.9992\n",
    "Epoch 9/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.0798 - val_loss: 1.9881\n",
    "Epoch 10/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.0509 - val_loss: 1.9784\n",
    "Epoch 11/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.0198 - val_loss: 1.9618\n",
    "Epoch 12/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.9822 - val_loss: 1.9383\n",
    "Epoch 13/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.9437 - val_loss: 1.9300\n",
    "Epoch 14/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.9198 - val_loss: 1.9163\n",
    "Epoch 15/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.8989 - val_loss: 1.9160\n",
    "Epoch 16/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.8866 - val_loss: 1.9085\n",
    "Epoch 17/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.8493 - val_loss: 1.8965\n",
    "Epoch 18/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.8248 - val_loss: 1.8878\n",
    "Epoch 19/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.8037 - val_loss: 1.8870\n",
    "Epoch 20/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.7724 - val_loss: 1.8862\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\n",
    "Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 32s - loss: 3.2809 - val_loss: 3.7595\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.4379 - val_loss: 2.9869\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.1504 - val_loss: 2.5361\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.9887 - val_loss: 2.1294\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.8984 - val_loss: 1.9727\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.7892 - val_loss: 1.9264\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.7172 - val_loss: 1.9100\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.6361 - val_loss: 1.9124\n",
    "Epoch 9/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.5621 - val_loss: 1.9122\n",
    "Epoch 10/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.4863 - val_loss: 1.9045\n",
    "Epoch 11/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.4150 - val_loss: 1.9278\n",
    "Epoch 12/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.3691 - val_loss: 1.9181\n",
    "Epoch 13/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.2970 - val_loss: 1.9414\n",
    "Epoch 14/20\n",
    " 1536/14725 [==>...........................] - ETA: 25s - loss: 1.1475\n",
    " \n",
    " \n",
    " \n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(256))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\n",
    " Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 17s - loss: 3.1787 - val_loss: 3.9282\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.5070 - val_loss: 3.2479\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.2191 - val_loss: 2.7522\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.0637 - val_loss: 2.3331\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.9539 - val_loss: 2.0326\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.8622 - val_loss: 1.9440\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7821 - val_loss: 1.9166\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7169 - val_loss: 1.8996\n",
    "Epoch 9/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.6561 - val_loss: 1.8849\n",
    "Epoch 10/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.5910 - val_loss: 1.9032\n",
    "Epoch 11/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.5082 - val_loss: 1.8878\n",
    "Epoch 12/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.4513 - val_loss: 1.9252\n",
    "Epoch 13/20\n",
    " 7552/14725 [==============>...............] - ETA: 7s - loss: 1.3534\n",
    " \n",
    " \n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(512))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\n",
    " Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 23s - loss: 3.1173 - val_loss: 3.8415\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 20s - loss: 2.4202 - val_loss: 3.1512\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 20s - loss: 2.1302 - val_loss: 2.7191\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 20s - loss: 1.9435 - val_loss: 2.3341\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 20s - loss: 1.7966 - val_loss: 1.9877\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 20s - loss: 1.6621 - val_loss: 1.9349\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 20s - loss: 1.5190 - val_loss: 1.9632\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 20s - loss: 1.3925 - val_loss: 1.9735\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.fit(X, y, epochs=5, batch_size=128, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_author_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Generating with seed: \"oes it really matter what it says this is some test text does it really matter what it says this is \"\n",
      "oes it really matter what it says this is some test text does it really matter what it says this is "
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.float64' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-af9aa51fe5bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"this is some test text does it really matter what it says \"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-16c3c1fabb4f>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(model, diversity, text)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mnext_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mnext_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mgenerated\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnext_char\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-16c3c1fabb4f>\u001b[0m in \u001b[0;36msample\u001b[0;34m(preds, temperature)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mexp_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_preds\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mprobas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.multinomial (numpy/random/mtrand/mtrand.c:37721)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'numpy.float64' has no len()"
     ]
    }
   ],
   "source": [
    "generate(model, diversity=0.7, text=\"this is some test text does it really matter what it says \" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generate(model, diversity=0.5, text=\"\"):\n",
    "    \"\"\"Generate text from a model\"\"\"\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    for i in range(5000):\n",
    "        x = np.zeros((1, maxlen), dtype=np.int)\n",
    "        for t, char in enumerate(sentence):\n",
    "            try:\n",
    "                x[0, t] = char_indices[char]\n",
    "            except:\n",
    "                print(sentence)\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Generating with seed: \" what it says this is some test text does it really matter what it says this is some test text does \"\n",
      " what it says this is some test text does it really matter what it says this is some test text does the chan store if the store in for Vis a singic and the places I nor for the store in find store is a gitter beat to the trist and the counders here to find. The sele the store with the storit and the chansic I was next the back the truck out. This is a git and the courder the trust and the ching store with the me to good and the next the countret to the this store is little back to the clourder the selsection for the stration's and my for probebles to have the store if friends out befould the chan net store with the park is can sele, I was not longer casteral friends. The store store is little this is this lace in findst and the inclure the selmen store in finds a befter the beet the car this store if the chan strater and closed to the counders. The strail also here in find store with the store with me and selestanding and the strorthors. So I had things a back to the mant I was not look in fried. The store in finds. The mest to the had a leng so this shore sprob food. The lent for the store for the store with the mant and the pricked out a find the chings realing and the mettreer smore with the store is later with got and the counders here for the stort and consorthing. The store store store is littely to get the onlite of the been to this shorth and I was trinks a fead a feas and my next the car the store in find store better here and class. The me of the only convenienter the the cound store. The store store is little but the only Nortar Crip and my ore also head with the things. The me so this shores and class store screet and the fings and long sometime store store is little also this is a back is this is a bittle this was small with the metr seet and my rear to was my sor the counder fead and was counders. The line for the strort and the Chrack is bak in friends and the chan in finds. The strail was come to this store is little bas in finds on the mettranter but the me store is little any the fincers and land store in findlest your for the stort office all park in things around findss. The ment store is fried was converite back the park in finds. The stores store with the counderth and check is this is a good sement and the boxt of the best thing to hand a fead and the me sell this stort of the one of the straf a feat and the chises with the only got for the stroirs and the mark is a firle a feed and the only metter slow with the stricket and my could my dere for real peating. The me something the counders to but the chan store is a Still a was a glass in finds that was conser this store shore for the strorth only regulay same but if my ere fur nite before the bands. The lane I was to gas here and the counder for the store is this is this is a bask in finds and the curont real for the store, The store in findst. The start in friends which when I gound the plation for Crome was strort and a fead all peat for your for the store and the chisies with the manity look in friends. The last in firl also but hive the was next the coure fent and the plattor for your for the store is little back to be. And the me of the stor"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-49291dfdfef3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"this is some test text does it really matter what it says \"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-9c3c76047201>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(model, diversity, text)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mnext_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mnext_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.4/site-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 902\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.4/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1583\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m         return self._predict_loop(f, ins,\n\u001b[0;32m-> 1585\u001b[0;31m                                   batch_size=batch_size, verbose=verbose)\n\u001b[0m\u001b[1;32m   1586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1587\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/.local/lib/python3.4/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1210\u001b[0m                 \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1212\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1213\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.4/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2227\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2228\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2229\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generate(model, diversity=0.5, text=\"this is some test text does it really matter what it says \" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
