{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n"
     ]
    }
   ],
   "source": [
    "print(\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "CPU times: user 45.4 s, sys: 5.21 s, total: 50.6 s\n",
      "Wall time: 12min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import json\n",
    "reviews = []\n",
    "with open(\"yelp_academic_dataset_review.json\") as f:\n",
    "    index = 0\n",
    "    for line in f:\n",
    "        if index % 1000000 == 0:\n",
    "            print(index)\n",
    "        index += 1\n",
    "        reviews.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'funny': 0, 'text': \"If you enjoy service by someone who is as competent as he is personable, I would recommend Corey Kaplan highly. The time he has spent here has been very productive and working with him educational and enjoyable. I hope not to need him again (though this is highly unlikely) but knowing he is there if I do is very nice. By the way, I'm not from El Centro, CA. but Scottsdale, AZ.\", 'useful': 0, 'review_id': 'NxL8SIC5yqOdnlXCg18IBg', 'user_id': 'KpkOkG6RIf4Ra25Lhhxf1A', 'date': '2011-10-10', 'cool': 0, 'business_id': '2aFiy99vNLklCx3T_tGS9A', 'type': 'review', 'stars': 5}\n"
     ]
    }
   ],
   "source": [
    "print(reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "prolific_reviewers = Counter([review['user_id'] for review in reviews]).most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_ids = {pr[0] : 0 for pr in prolific_reviewers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "by_author = {} # author : \"review 1\\n review 2\\n review 3\"\n",
    "for review in reviews:\n",
    "    uid = review['user_id']\n",
    "    if uid in keep_ids:\n",
    "        uid = review['user_id']\n",
    "        if uid in by_author:\n",
    "            by_author[uid] += \"\\n{}\".format(review['text'])\n",
    "        else:\n",
    "            by_author[uid] = \"{}\".format(review['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(by_author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(276136, 'ffPY_bHX8vLebHu8LBEqfg'),\n",
       " (278427, 'PeLGa5vUR8_mcsn-fn42Jg'),\n",
       " (351311, 'cMEtAiW60I5wE_vLfTxoJQ'),\n",
       " (370129, 'iDlkZO2iILS8Jwfdy7DP9A'),\n",
       " (461333, 'dt9IHwfuZs9D9LOH7gjNew')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that we have at least 200000 characters for each author\n",
    "sorted([(len(by_author[key]), key) for key in by_author])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_chunks(l, n):\n",
    "    n = max(1, n)\n",
    "    return [l[i:i+n] for i in range(0, len(l), n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train_texts = []  # the first 100 000 chars from each author\n",
    "train_labels = [] # each author\n",
    "test_texts = []   # 100 texts of 1000 characters each (second 100 000 chars of each author)\n",
    "test_labels = []  # each author * 100\n",
    "\n",
    "author_int = {author: i for i,author in enumerate(by_author)}\n",
    "int_author = {author_int[author]: author for author in author_int}\n",
    "\n",
    "for author in by_author:\n",
    "    train_text = by_author[author][:5000]\n",
    "    train_label = author_int[author]\n",
    "    train_texts.append(train_text)\n",
    "    train_labels.append(train_label)\n",
    "    \n",
    "    short_texts = get_chunks(by_author[author][5000:10000], 1000)\n",
    "    for text in short_texts:\n",
    "        test_texts.append(text)\n",
    "        test_labels.append(author_int[author])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 150\n"
     ]
    }
   ],
   "source": [
    "print(len(train_texts), len(test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# vectorization - chars to ints\n",
    "import string\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    \"\"\"Sample predictions from a probability array\"\"\"\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds + 1e-6) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def generate(model, diversity=0.5, text=\"\"):\n",
    "    \"\"\"Generate text from a model\"\"\"\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    for i in range(5000):\n",
    "        x = np.zeros((1, maxlen), dtype=np.int)\n",
    "        for t, char in enumerate(sentence):\n",
    "            try:\n",
    "                x[0, t] = char_indices[char]\n",
    "            except:\n",
    "                print(sentence)\n",
    "        preds = model.predict(x, verbose=0)[0][0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    return\n",
    "\n",
    "def vectorize(text):\n",
    "    \"\"\"Convert text into character sequences\"\"\"\n",
    "    step = 3\n",
    "    sentences = []\n",
    "    next_chars = []\n",
    "    for i in range(0, len(text) - maxlen, step):\n",
    "        sentences.append(text[i: i + maxlen])\n",
    "        next_chars.append(text[i + maxlen])\n",
    "    X = np.zeros((len(sentences), maxlen), dtype=np.int)\n",
    "    y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for t, char in enumerate(sentence):\n",
    "            X[i, t] = char_indices[char]\n",
    "        y[i, char_indices[next_chars[i]]] = 1\n",
    "    return X, y\n",
    "\n",
    "def clean_text(text, charset):\n",
    "    text = \" \".join(text.split())  # all white space is one space\n",
    "    text = \"\".join([x for x in text if x in charset])  # remove characters that we don't care about\n",
    "    return text\n",
    "\n",
    "def get_model(modelfile, freeze=False):\n",
    "    model = load_model(modelfile)\n",
    "    if freeze:\n",
    "        for layer in model.layers[:6]:\n",
    "            layer.trainable = False\n",
    "    return model\n",
    "\n",
    "chars = \" \" + string.ascii_letters + string.punctuation  # sorted to keep indices consistent\n",
    "charset = set(chars)  # for lookup\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "maxlen = 100  # must match length which generated model - the sequence length\n",
    "\n",
    "# load a pretrained language model\n",
    "modelfile = \"charlm2/model_middlemarch_cnn.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test train an author specific model\n",
    "# test_author_text = clean_text(train_texts[0], charset)\n",
    "# test_author_model = get_model(modelfile, freeze=False)\n",
    "X, y = vectorize(test_author_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (Unable to open file: name = 'charlm2/model_middlemarch_cnn.hdf5', errno = 2, error message = 'no such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-708c4d2e36f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_author_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_author_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"categorical_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_author_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-16c3c1fabb4f>\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(modelfile, freeze)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfreeze\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.4/site-packages/keras/models.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;31m# instantiate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/h5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/tmp/pip-tcce551v-build/h5py/_objects.c:2840)\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mneeded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m \u001b[0macquires\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnotifies\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfirst\u001b[0m \u001b[0mthread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mto\u001b[0m \u001b[0mrelease\u001b[0m \u001b[0mit\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mThis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mall\u001b[0m \u001b[0mmade\u001b[0m \u001b[0mpossible\u001b[0m \u001b[0mby\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mwonderful\u001b[0m \u001b[0mGIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \"\"\"\n\u001b[1;32m     56\u001b[0m     \u001b[0mcdef\u001b[0m \u001b[0mpythread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyThread_type_lock\u001b[0m \u001b[0m_real_lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/h5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/tmp/pip-tcce551v-build/h5py/_objects.c:2798)\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mto\u001b[0m \u001b[0mrelease\u001b[0m \u001b[0mit\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mThis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mall\u001b[0m \u001b[0mmade\u001b[0m \u001b[0mpossible\u001b[0m \u001b[0mby\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mwonderful\u001b[0m \u001b[0mGIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \"\"\"\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mcdef\u001b[0m \u001b[0mpythread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyThread_type_lock\u001b[0m \u001b[0m_real_lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mcdef\u001b[0m \u001b[0mlong\u001b[0m \u001b[0m_owner\u001b[0m            \u001b[0;31m# ID of thread owning the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/h5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open (/tmp/pip-tcce551v-build/h5py/h5f.c:2117)\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m     \"\"\"(STRING name, INT flags=ACC_TRUNC, PropFCID fcpl=None,\n\u001b[1;32m     77\u001b[0m     PropFAID fapl=None) => FileID\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0mCreate\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnew\u001b[0m \u001b[0mHDF5\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mKeyword\u001b[0m \u001b[0;34m\"flags\"\u001b[0m \u001b[0mmay\u001b[0m \u001b[0mbe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (Unable to open file: name = 'charlm2/model_middlemarch_cnn.hdf5', errno = 2, error message = 'no such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "test_author_model = get_model(modelfile, freeze=True)\n",
    "test_author_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", loss_weights=[1., 0.2])\n",
    "test_author_model.fit(X, [y, y], epochs=5, batch_size=128, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_author_model = get_model(modelfile, freeze=True)\n",
    "test_author_model.fit(X, [y, y], epochs=10, batch_size=128, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "fresh_model = model_from_json(test_author_model.to_json())\n",
    "fresh_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", loss_weights=[1.0, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_author_model.fit(X, [y, y], epochs=5, batch_size=128, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_author_model.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "print(mean(scores[:50]))\n",
    "print(mean(scores[50:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.boxplot(get_chunks(scores, 50)[:20]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_labels[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generate(test_author_model, diversity=0.7, text=\"this is some test text does it really matter what it says \"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM, Input, Embedding, Conv1D, MaxPooling1D, BatchNormalization, GRU\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def get_gru_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(GRU(256))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(y.shape[1], activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 30\n",
      "1 / 30\n",
      "2 / 30\n",
      "3 / 30\n",
      "4 / 30\n",
      "5 / 30\n",
      "6 / 30\n",
      "7 / 30\n",
      "8 / 30\n",
      "9 / 30\n",
      "10 / 30\n",
      "11 / 30\n",
      "12 / 30\n",
      "13 / 30\n",
      "14 / 30\n",
      "15 / 30\n",
      "16 / 30\n",
      "17 / 30\n",
      "18 / 30\n",
      "19 / 30\n",
      "20 / 30\n",
      "21 / 30\n",
      "22 / 30\n",
      "23 / 30\n",
      "24 / 30\n",
      "25 / 30\n",
      "26 / 30\n",
      "27 / 30\n",
      "28 / 30\n",
      "29 / 30\n",
      "CPU times: user 9min 15s, sys: 45.4 s, total: 10min 1s\n",
      "Wall time: 7min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "author_models = []  # [(author_model, author_id), (author_model, author_id), ...] - ids are ints\n",
    "for i, train_text in enumerate(train_texts):\n",
    "    print(\"{} / {}\".format(i, len(train_texts)))\n",
    "    ct = clean_text(train_text, charset)\n",
    "    am = get_gru_model()\n",
    "    X, y = vectorize(ct)\n",
    "    am.fit(X, y, epochs=5, batch_size=128, verbose=0)\n",
    "    author_models.append((am, train_labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1446 samples, validate on 161 samples\n",
      "Epoch 1/40\n",
      "1446/1446 [==============================] - 5s - loss: 4.4259 - val_loss: 4.3729\n",
      "Epoch 2/40\n",
      "1446/1446 [==============================] - 1s - loss: 3.4265 - val_loss: 4.3142\n",
      "Epoch 3/40\n",
      "1446/1446 [==============================] - 1s - loss: 3.0410 - val_loss: 4.2668\n",
      "Epoch 4/40\n",
      "1446/1446 [==============================] - 1s - loss: 2.7695 - val_loss: 4.2206\n",
      "Epoch 5/40\n",
      "1446/1446 [==============================] - 1s - loss: 2.6066 - val_loss: 4.1777\n",
      "Epoch 6/40\n",
      "1446/1446 [==============================] - 1s - loss: 2.4397 - val_loss: 4.1267\n",
      "Epoch 7/40\n",
      "1446/1446 [==============================] - 1s - loss: 2.3169 - val_loss: 4.0819\n",
      "Epoch 8/40\n",
      "1446/1446 [==============================] - 1s - loss: 2.2029 - val_loss: 4.0310\n",
      "Epoch 9/40\n",
      "1446/1446 [==============================] - 1s - loss: 2.0937 - val_loss: 3.9768\n",
      "Epoch 10/40\n",
      "1446/1446 [==============================] - 1s - loss: 2.0027 - val_loss: 3.9192\n",
      "Epoch 11/40\n",
      "1446/1446 [==============================] - 1s - loss: 1.8858 - val_loss: 3.8526\n",
      "Epoch 12/40\n",
      "1446/1446 [==============================] - 1s - loss: 1.7861 - val_loss: 3.8031\n",
      "Epoch 13/40\n",
      "1446/1446 [==============================] - 1s - loss: 1.6861 - val_loss: 3.7371\n",
      "Epoch 14/40\n",
      "1446/1446 [==============================] - 1s - loss: 1.6301 - val_loss: 3.6745\n",
      "Epoch 15/40\n",
      "1446/1446 [==============================] - 1s - loss: 1.5184 - val_loss: 3.6161\n",
      "Epoch 16/40\n",
      "1446/1446 [==============================] - 1s - loss: 1.4255 - val_loss: 3.5737\n",
      "Epoch 17/40\n",
      "1446/1446 [==============================] - 1s - loss: 1.3267 - val_loss: 3.5185\n",
      "Epoch 18/40\n",
      "1446/1446 [==============================] - 1s - loss: 1.1930 - val_loss: 3.4780\n",
      "Epoch 19/40\n",
      "1446/1446 [==============================] - 1s - loss: 1.1022 - val_loss: 3.4248\n",
      "Epoch 20/40\n",
      "1446/1446 [==============================] - 1s - loss: 1.0621 - val_loss: 3.3687\n",
      "Epoch 21/40\n",
      "1446/1446 [==============================] - 1s - loss: 0.9367 - val_loss: 3.3485\n",
      "Epoch 22/40\n",
      "1446/1446 [==============================] - 1s - loss: 0.8554 - val_loss: 3.3133\n",
      "Epoch 23/40\n",
      "1446/1446 [==============================] - 1s - loss: 0.7709 - val_loss: 3.2500\n",
      "Epoch 24/40\n",
      "1446/1446 [==============================] - 1s - loss: 0.6978 - val_loss: 3.2753\n",
      "Epoch 25/40\n",
      "1446/1446 [==============================] - 1s - loss: 0.6334 - val_loss: 3.3876\n",
      "Epoch 26/40\n",
      "1446/1446 [==============================] - 1s - loss: 0.5587 - val_loss: 3.3133\n",
      "Epoch 27/40\n",
      "1446/1446 [==============================] - 1s - loss: 0.4931 - val_loss: 3.2625\n",
      "Epoch 28/40\n",
      "1446/1446 [==============================] - 1s - loss: 0.4433 - val_loss: 3.2434\n",
      "Epoch 29/40\n",
      "1446/1446 [==============================] - 1s - loss: 0.4135 - val_loss: 3.1697\n",
      "Epoch 30/40\n",
      "1446/1446 [==============================] - 1s - loss: 0.3472 - val_loss: 3.1775\n",
      "Epoch 31/40\n",
      "1446/1446 [==============================] - 1s - loss: 0.2985 - val_loss: 3.2696\n",
      "Epoch 32/40\n",
      "1446/1446 [==============================] - 1s - loss: 0.2793 - val_loss: 3.1078\n",
      "Epoch 33/40\n",
      "1446/1446 [==============================] - 1s - loss: 0.2546 - val_loss: 3.1116\n",
      "Epoch 34/40\n",
      "1446/1446 [==============================] - 1s - loss: 0.2262 - val_loss: 3.2152\n",
      "Epoch 35/40\n",
      "1446/1446 [==============================] - 1s - loss: 0.1933 - val_loss: 3.0777\n",
      "Epoch 36/40\n",
      "1446/1446 [==============================] - 1s - loss: 0.1832 - val_loss: 3.0919\n",
      "Epoch 37/40\n",
      "1446/1446 [==============================] - 1s - loss: 0.1500 - val_loss: 3.2803\n",
      "Epoch 38/40\n",
      "1446/1446 [==============================] - 1s - loss: 0.1463 - val_loss: 3.2938\n",
      "Epoch 39/40\n",
      "1446/1446 [==============================] - 1s - loss: 0.1319 - val_loss: 3.3846\n",
      "Epoch 40/40\n",
      "1446/1446 [==============================] - 1s - loss: 0.1260 - val_loss: 3.2557\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb32235f240>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "am = get_gru_model()\n",
    "X, y = vectorize(ct)\n",
    "am.fit(X, y, epochs=40, batch_size=128, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The place is called The Millvale Diner now. The ownership is not the same, however. Curiously enough, a webpage for the restaurant has a quote from my Yelp review. http://www.millvalepa.com/businesses/millvale-diner This is a technocrat\\'s pizza parlor. If you order online, you are taken to a website called foodler.com. You place your order just like you would order a book on amazon.com, and you get a total and everything. They even calculate a tip for you ( or  percent) that you can put on the card, send you an email confirmation, and even estimate a delivery time. The food came ten minutes early. Pretty nice. Was it good? Oh my my, oh hell yes, got to put on your party dress! Or um...pants if you\\'re um...not that there\\'s anything wrong with, I mean, Roddy Piper wore a kilt... While they\\'ve embraced the future in terms of placing orders, they get medieval when it comes to food prep. Stone oven folks. That\\'s what they say anyway, and I\\'m inclined to believe \\'em since this was crispy and chewy where it needed to be with a strong mozzarella blend, a nicely sweet \\'n somewhat chunky sauce, and those uncanned mushrooms that get burnt when the pie comes out. The Aprimo Steak has it all and as a result is magnificently sloppy. If you ate it standing up you\\'d be doing the Philly Stoop, and you wouldn\\'t care who was staring since what you\\'re eating would be too good to allow for an awareness of your surroundings. \"Mozzarella & provolone, mushrooms, onions, green peppers, hot peppers, mayonnaise, lettuce, tomatoes & Italian dressing,\" they sez. What is it with mayo on steak hoagies these days around Pittsburgh, and why didn\\'t people start doing this ten years ago? It\\'s not the greatest steak hoagie ever, but it doesn\\'t back down at all from any challenger. The Italian Hoagie was rather good too. The bread on both sandwiches wasn\\'t baked so much that it gets rock-like. It was toasted just enough to maintain pliancy and didn\\'t skimp on the meat at all. How many years has it been since we got delivery from anywhere in Mt. Oliver? Jeez. They have something to boast of now. Eras are ending daily in our fair city of late. The local news media is reporting that Benkovitz Seafoods has gone dark. Its Leviathan fish sandwiches are no more... http://triblive.com/news/adminpage/-/benkovitz-seafood-district#axzzMinHfXss The  + year-old business will certainly leave a large void in the wake of its death. I\\'m sure something trendy and expensive will take its place. I\\'m really surprised at how many CVS stores there are within this - mile radius and am surprised all the more that they all seem to stay in business. Kay needed some Excedrin en route back from The Galleria, and we just happened upon this CVS that is clean, well-stocked, and full of helpful employees including a middle-aged cashier with a lovely British accent. Yes, a British accent. At a CVS in the South suburbs of Pittsburgh. Our world gets smaller still. Oddly enough, there is a Dollar General across the road, but it didn\\'t occur to me to stop in one until we saw another Dollar General about a mile away on McFarland Road. Hmm. The other night, Kay and I did something...risky. She didn\\'t feel like cooking, so we decided to order in. Fortunately there\\'s the internet and the Pennysaver. In the Pennysaver, a local paper version of craigslist for those who don\\'t know, we found a pamphlet menu for Sushi Three, yet another place I\\'d been meaning to try for many years. \"Kay, why don\\'t we just order pizza? They probably don\\'t even deliver here.\" \"But I. Want. Asian!!!\" Besides, did you call them?\" \"No.\" \"Well...\" So I called, and by God they did deliver to Beechview after all. \"But Kay, it\\'s Japanese food. It may not hold up well.\" \"But what about that takeout from Pan Asia you had your parents get you for your birthday? You raved about that on Yelp.\" My Baby Doll coulda been a lawyer. Alas... The shumai was rubbery and had flavor that was blubbery. Why? The  pieces of sushi were a tad mushy and left me anything but gushy. The chicken bulgogi was gravely soggy, and its lack of true essence of BBQ made me go \"Eeew.\" The Miso, in terms of taste, was a flat zero. However, the salad with , Island dressing was a small blessing. Kay had something called Shrimp Fantasy, and one bite confirmed for me that naming it such was heresy. And I think that\\'s all we had. My Etrigan-styled antics aside, I\\'m not sure if our dinner would have been better had we chosen to dine-in at Sushi Three instead of having it delivered. Because of that, I might be willing to try them again. Emphasis on \"might.\" We did finish everything we ordered, but none of what we had carried with it any sense of culinary flair. In that sense, this meal came off as generic Asian food. For example, the aforementioned bulgogi just seemed like it could have been any Americanized Asian chicken dish, one that came from a Reader\\'s Digest recipe or the'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "150\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "print(len(author_models))\n",
    "print(len(test_texts))\n",
    "print(len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182.18666666666667"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statistics import mean\n",
    "word_counts = [text.count(\" \") for text in test_texts]\n",
    "mean(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 182 words is quite short\n",
    "# Try to join 5 tests texts together\n",
    "longer_test_texts = get_chunks(test_texts, 5)\n",
    "longer_test_labels = get_chunks(test_labels, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all([len(set(x)) == 1 for x in longer_test_labels])  # Make sure that all combined labels are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "longer_test_texts = ['\\n'.join(chunk) for chunk in longer_test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "longer_test_labels = [chunk[0] for chunk in longer_test_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 150 .............................. 0:01:12.658216\n",
      "1 / 150 .............................. 0:00:10.014997\n",
      "2 / 150 .............................. 0:00:10.005684\n",
      "3 / 150 .............................. 0:00:09.998496\n",
      "4 / 150 .............................. 0:00:10.015908\n",
      "5 / 150 .............................. 0:00:09.996850\n",
      "6 / 150 .............................. 0:00:10.033142\n",
      "7 / 150 .............................. 0:00:09.995770\n",
      "8 / 150 .............................. 0:00:09.987757\n",
      "9 / 150 .............................. 0:00:09.999000\n",
      "10 / 150 .............................. 0:00:10.002491\n",
      "11 / 150 .............................. 0:00:10.008417\n",
      "12 / 150 .............................. 0:00:09.989792\n",
      "13 / 150 .............................. 0:00:09.986971\n",
      "14 / 150 .............................. 0:00:09.992458\n",
      "15 / 150 .............................. 0:00:10.026915\n",
      "16 / 150 .............................. 0:00:10.005073\n",
      "17 / 150 .............................. 0:00:10.016418\n",
      "18 / 150 .............................. 0:00:10.028552\n",
      "19 / 150 .............................. 0:00:09.996037\n",
      "20 / 150 .............................. 0:00:10.002553\n",
      "21 / 150 .............................. 0:00:09.993454\n",
      "22 / 150 .............................. 0:00:09.982133\n",
      "23 / 150 .............................. 0:00:09.995952\n",
      "24 / 150 .............................. 0:00:09.994128\n",
      "25 / 150 .............................. 0:00:10.024169\n",
      "26 / 150 .............................. 0:00:10.004989\n",
      "27 / 150 .............................. 0:00:10.020346\n",
      "28 / 150 .............................. 0:00:10.012594\n",
      "29 / 150 .............................. 0:00:09.990148\n",
      "30 / 150 .............................. 0:00:10.014439\n",
      "31 / 150 .............................. 0:00:09.995338\n",
      "32 / 150 .............................. 0:00:09.991592\n",
      "33 / 150 .............................. 0:00:09.993517\n",
      "34 / 150 .............................. 0:00:09.999053\n",
      "35 / 150 .............................. 0:00:09.999805\n",
      "36 / 150 .............................. 0:00:09.991354\n",
      "37 / 150 .............................. 0:00:09.992545\n",
      "38 / 150 .............................. 0:00:09.994511\n",
      "39 / 150 .............................. 0:00:09.981454\n",
      "40 / 150 .............................. 0:00:10.001552\n",
      "41 / 150 .............................. 0:00:10.020842\n",
      "42 / 150 .............................. 0:00:10.003249\n",
      "43 / 150 .............................. 0:00:10.000312\n",
      "44 / 150 .............................. 0:00:09.993504\n",
      "45 / 150 .............................. 0:00:10.013825\n",
      "46 / 150 .............................. 0:00:10.008423\n",
      "47 / 150 .............................. 0:00:10.007963\n",
      "48 / 150 .............................. 0:00:10.009525\n",
      "49 / 150 .............................. 0:00:10.031519\n",
      "50 / 150 .............................. 0:00:09.992341\n",
      "51 / 150 .............................. 0:00:09.997443\n",
      "52 / 150 .............................. 0:00:10.005824\n",
      "53 / 150 .............................. 0:00:10.011875\n",
      "54 / 150 .............................. 0:00:10.017766\n",
      "55 / 150 .............................. 0:00:10.008686\n",
      "56 / 150 .............................. 0:00:09.993993\n",
      "57 / 150 .............................. 0:00:10.025293\n",
      "58 / 150 .............................. 0:00:10.023859\n",
      "59 / 150 .............................. 0:00:10.034939\n",
      "60 / 150 .............................. 0:00:09.973600\n",
      "61 / 150 .............................. 0:00:09.983280\n",
      "62 / 150 .............................. 0:00:09.996041\n",
      "63 / 150 .............................. 0:00:10.016211\n",
      "64 / 150 .............................. 0:00:09.972734\n",
      "65 / 150 .............................. 0:00:10.020890\n",
      "66 / 150 .............................. 0:00:09.998132\n",
      "67 / 150 .............................. 0:00:10.001549\n",
      "68 / 150 .............................. 0:00:10.013466\n",
      "69 / 150 .............................. 0:00:10.031166\n",
      "70 / 150 .............................. 0:00:10.001246\n",
      "71 / 150 .............................. 0:00:10.022622\n",
      "72 / 150 .............................. 0:00:10.000570\n",
      "73 / 150 .............................. 0:00:10.018770\n",
      "74 / 150 .............................. 0:00:09.994028\n",
      "75 / 150 .............................. 0:00:10.025249\n",
      "76 / 150 .............................. 0:00:10.013539\n",
      "77 / 150 .............................. 0:00:09.996867\n",
      "78 / 150 .............................. 0:00:10.004695\n",
      "79 / 150 .............................. 0:00:09.973735\n",
      "80 / 150 .............................. 0:00:09.999693\n",
      "81 / 150 .............................. 0:00:10.015398\n",
      "82 / 150 .............................. 0:00:10.026511\n",
      "83 / 150 .............................. 0:00:10.028678\n",
      "84 / 150 .............................. 0:00:10.016627\n",
      "85 / 150 .............................. 0:00:09.987858\n",
      "86 / 150 .............................. 0:00:10.010366\n",
      "87 / 150 .............................. 0:00:10.032042\n",
      "88 / 150 .............................. 0:00:10.032556\n",
      "89 / 150 .............................. 0:00:10.021436\n",
      "90 / 150 .............................. 0:00:09.998224\n",
      "91 / 150 .............................. 0:00:09.999353\n",
      "92 / 150 .............................. 0:00:09.988317\n",
      "93 / 150 .............................. 0:00:09.992093\n",
      "94 / 150 .............................. 0:00:10.101537\n",
      "95 / 150 .............................. 0:00:10.021999\n",
      "96 / 150 .............................. 0:00:10.023920\n",
      "97 / 150 .............................. 0:00:09.995546\n",
      "98 / 150 .............................. 0:00:09.996885\n",
      "99 / 150 .............................. 0:00:09.991604\n",
      "100 / 150 .............................. 0:00:09.996447\n",
      "101 / 150 .............................. 0:00:10.002760\n",
      "102 / 150 .............................. 0:00:10.021785\n",
      "103 / 150 .............................. 0:00:09.992208\n",
      "104 / 150 .............................. 0:00:10.021747\n",
      "105 / 150 .............................. 0:00:10.002357\n",
      "106 / 150 .............................. 0:00:09.995977\n",
      "107 / 150 .............................. 0:00:09.996176\n",
      "108 / 150 .............................. 0:00:10.024116\n",
      "109 / 150 .............................. 0:00:10.028221\n",
      "110 / 150 .............................. 0:00:09.995925\n",
      "111 / 150 .............................. 0:00:10.014736\n",
      "112 / 150 .............................. 0:00:10.006170\n",
      "113 / 150 .............................. 0:00:10.000916\n",
      "114 / 150 .............................. 0:00:10.009521\n",
      "115 / 150 .............................. 0:00:09.979476\n",
      "116 / 150 .............................. 0:00:10.013757\n",
      "117 / 150 .............................. 0:00:09.986416\n",
      "118 / 150 .............................. 0:00:09.982209\n",
      "119 / 150 .............................. 0:00:09.993673\n",
      "120 / 150 .............................. 0:00:09.027922\n",
      "121 / 150 .............................. 0:00:09.986804\n",
      "122 / 150 .............................. 0:00:09.980192\n",
      "123 / 150 .............................. 0:00:09.969940\n",
      "124 / 150 .............................. 0:00:09.971390\n",
      "125 / 150 .............................. 0:00:10.024395\n",
      "126 / 150 .............................. 0:00:09.998778\n",
      "127 / 150 .............................. 0:00:09.999093\n",
      "128 / 150 .............................. 0:00:09.991532\n",
      "129 / 150 .............................. 0:00:09.995098\n",
      "130 / 150 .............................. 0:00:10.005388\n",
      "131 / 150 .............................. 0:00:09.990031\n",
      "132 / 150 .............................. 0:00:10.003172\n",
      "133 / 150 .............................. 0:00:09.991853\n",
      "134 / 150 .............................. 0:00:10.008898\n",
      "135 / 150 .............................. 0:00:09.983023\n",
      "136 / 150 .............................. 0:00:09.965567\n",
      "137 / 150 .............................. 0:00:09.967128\n",
      "138 / 150 .............................. 0:00:10.104407\n",
      "139 / 150 .............................. 0:00:09.997548\n",
      "140 / 150 .............................. 0:00:09.028956\n",
      "141 / 150 .............................. 0:00:10.018065\n",
      "142 / 150 .............................. 0:00:09.035821\n",
      "143 / 150 .............................. 0:00:09.986514\n",
      "144 / 150 .............................. 0:00:10.015662\n",
      "145 / 150 .............................. 0:00:10.002679\n",
      "146 / 150 .............................. 0:00:10.019923\n",
      "147 / 150 .............................. 0:00:10.006345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148 / 150 .............................. 0:00:09.999169\n",
      "149 / 150 .............................. 0:00:10.023478\n",
      "CPU times: user 23min 11s, sys: 6.78 s, total: 23min 17s\n",
      "Wall time: 26min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from random import shuffle\n",
    "from datetime import datetime\n",
    "\n",
    "def get_predictions(author_models, test_texts, test_labels):\n",
    "    \"\"\"Evaluate each text for each author_model and append first metric to predictions\"\"\"\n",
    "    indicies = list(range(len(test_texts)))\n",
    "\n",
    "    test_texts = np.array(test_texts)\n",
    "    test_labels = np.array(test_labels)\n",
    "\n",
    "    test_texts = test_texts[indicies]\n",
    "    test_labels = test_labels[indicies]\n",
    "\n",
    "    predictions = []\n",
    "    for i, text in enumerate(test_texts):\n",
    "        t1 = datetime.now()\n",
    "        print(\"{} / {}\".format(i, len(test_texts)), end=\" \")\n",
    "        X, y = vectorize(clean_text(text, charset))\n",
    "\n",
    "        losses = []\n",
    "        for am in author_models:\n",
    "            print(\".\", end=\"\")\n",
    "            model = am[0]\n",
    "            label = am[1]\n",
    "            loss = model.evaluate(X, y, verbose=0)\n",
    "            losses.append((loss, label))\n",
    "        print(\" {}\".format(datetime.now() - t1))\n",
    "        predictions.append(losses)\n",
    "    return predictions\n",
    "    \n",
    "\n",
    "predictions_long = get_predictions(author_models, test_texts, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n"
     ]
    }
   ],
   "source": [
    "print(\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_is = []\n",
    "for pred in predictions_long:\n",
    "    pred_i = [p[0] for p in pred]\n",
    "    pred_is.append(pred_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labs = [np.argmin(pred) for pred in pred_is]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.053333333333333337"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_labels, pred_labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 28,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_author_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_author_model.evaluatei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "word_vec = TfidfVectorizer(min_df=3, ngram_range=(1,2))\n",
    "char_vec = TfidfVectorizer(min_df=3, ngram_range=(2,5))\n",
    "\n",
    "fu = FeatureUnion([\n",
    "    ('word', word_vec),\n",
    "    ('char', char_vec)\n",
    "])\n",
    "\n",
    "\n",
    "X_train = fu.fit_transform(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = fu.transform(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm.fit(X_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy_score(test_labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_longer = fu.transform(longer_test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = svm.predict(X_test_longer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy_score(longer_test_labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = vectorize(clean_text(train_texts[3], charset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_author_model.layers[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-fd869bc6686c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM, Input, Embedding, Conv1D, MaxPooling1D, BatchNormalization, GRU\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# load ascii text and covert to lowercase\n",
    "\n",
    "# cnn = Dropout(0.2)(embedded)\n",
    "# cnn = Conv1D(128, 5, activation='relu')(cnn)\n",
    "# cnn = MaxPooling1D(pool_size=4)(cnn)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(256))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=13, batch_size=128, validation_split=0.1)\n",
    "\n",
    "\"\"\"\n",
    "LSTM, BatchNorm\n",
    "Train on 14929 samples, validate on 1659 samples\n",
    "Epoch 1/5\n",
    "14929/14929 [==============================] - 75s - loss: 3.0403 - val_loss: 3.4757\n",
    "Epoch 2/5\n",
    "14929/14929 [==============================] - 60s - loss: 2.3156 - val_loss: 2.9687\n",
    "\n",
    "LSTM\n",
    "Train on 14929 samples, validate on 1659 samples\n",
    "Epoch 1/5\n",
    "14929/14929 [==============================] - 75s - loss: 3.1259 - val_loss: 2.7865\n",
    "Epoch 2/5\n",
    "14929/14929 [==============================] - 60s - loss: 2.6150 - val_loss: 2.3894\n",
    "\n",
    "CNN(5), LSTM  # faster, needs more epochs\n",
    "Train on 14929 samples, validate on 1659 samples\n",
    "Epoch 1/5\n",
    "14929/14929 [==============================] - 42s - loss: 3.1579 - val_loss: 2.9987\n",
    "Epoch 2/5\n",
    "14929/14929 [==============================] - 26s - loss: 2.8874 - val_loss: 2.6994\n",
    "Epoch 3/5\n",
    "14929/14929 [==============================] - 26s - loss: 2.6220 - val_loss: 2.4879\n",
    "Epoch 4/5\n",
    "14929/14929 [==============================] - 26s - loss: 2.4309 - val_loss: 2.3942\n",
    "Epoch 5/5\n",
    "14929/14929 [==============================] - 26s - loss: 2.2950 - val_loss: 2.2902\n",
    "\n",
    "CNN(5), CNN(3), LSTM doesn't drop below 3.0 in 5 epochs\n",
    "\n",
    "\n",
    "Embedding, BatchNorm, GRU, BatchNorm\n",
    "Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 16s - loss: 2.9240 - val_loss: 3.9516\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.2447 - val_loss: 3.3667\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.0054 - val_loss: 2.8011\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.8388 - val_loss: 2.3477\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7122 - val_loss: 2.0196\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.6069 - val_loss: 1.9417\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.5044 - val_loss: 1.9541\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.3987 - val_loss: 1.9512\n",
    "Epoch 9/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.2940 - val_loss: 1.9921\n",
    "Epoch 10/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.1850 - val_loss: 2.0424\n",
    "\n",
    "Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 17s - loss: 3.2577 - val_loss: 3.8892\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.5444 - val_loss: 3.2364\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.2915 - val_loss: 2.7205\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.1258 - val_loss: 2.3477\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.0219 - val_loss: 2.0588\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.9386 - val_loss: 1.9478\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.8762 - val_loss: 1.9152\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7994 - val_loss: 1.9076\n",
    "Epoch 9/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7224 - val_loss: 1.8829\n",
    "Epoch 10/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.6783 - val_loss: 1.9007\n",
    "Epoch 11/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.6154 - val_loss: 1.8910\n",
    "Epoch 12/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.5485 - val_loss: 1.8920\n",
    "Epoch 13/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.4844 - val_loss: 1.9198\n",
    "Epoch 14/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.4291 - val_loss: 1.9193\n",
    "Epoch 15/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.3670 - val_loss: 1.9295\n",
    "Epoch 16/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.3028 - val_loss: 1.9752\n",
    "Epoch 17/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.2321 - val_loss: 1.9969\n",
    "Epoch 18/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.2159 - val_loss: 2.0431\n",
    "Epoch 19/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.1391 - val_loss: 2.0709\n",
    "Epoch 20/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.1321 - val_loss: 2.1172\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(256))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\n",
    "Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 17s - loss: 3.2577 - val_loss: 3.8892\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.5444 - val_loss: 3.2364\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.2915 - val_loss: 2.7205\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.1258 - val_loss: 2.3477\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.0219 - val_loss: 2.0588\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.9386 - val_loss: 1.9478\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.8762 - val_loss: 1.9152\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7994 - val_loss: 1.9076\n",
    "Epoch 9/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7224 - val_loss: 1.8829\n",
    "Epoch 10/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.6783 - val_loss: 1.9007\n",
    "Epoch 11/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.6154 - val_loss: 1.8910\n",
    "Epoch 12/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.5485 - val_loss: 1.8920\n",
    "Epoch 13/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.4844 - val_loss: 1.9198\n",
    "Epoch 14/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.4291 - val_loss: 1.9193\n",
    "Epoch 15/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.3670 - val_loss: 1.9295\n",
    "Epoch 16/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.3028 - val_loss: 1.9752\n",
    "Epoch 17/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.2321 - val_loss: 1.9969\n",
    "Epoch 18/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.2159 - val_loss: 2.0431\n",
    "Epoch 19/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.1391 - val_loss: 2.0709\n",
    "Epoch 20/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.1321 - val_loss: 2.1172\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "model.add(GRU(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\n",
    "Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 16s - loss: 3.1731 - val_loss: 3.5648\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.4964 - val_loss: 2.9875\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.3446 - val_loss: 2.7695\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.2928 - val_loss: 2.6010\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.2642 - val_loss: 2.3900\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.2373 - val_loss: 2.5023\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.2186 - val_loss: 2.3780\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.2029 - val_loss: 2.4928\n",
    "Epoch 9/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.1852 - val_loss: 2.3480\n",
    "Epoch 10/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.1745 - val_loss: 2.4801\n",
    "Epoch 11/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.1563 - val_loss: 2.3951\n",
    "Epoch 12/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.1391 - val_loss: 2.4133\n",
    "Epoch 13/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.1192 - val_loss: 2.5896\n",
    "Epoch 14/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.1020 - val_loss: 2.2692\n",
    "Epoch 15/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.0770 - val_loss: 2.2179\n",
    "Epoch 16/20\n",
    "14725/14725 [==============================] - 14s - loss: 2.0643 - val_loss: 2.2822\n",
    "Epoch 17/20\n",
    " 6784/14725 [============>.................] - ETA: 7s - loss: 2.0302\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\n",
    "Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 17s - loss: 3.1528 - val_loss: 3.9042\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.4658 - val_loss: 3.2093\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.2001 - val_loss: 2.6764\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.0358 - val_loss: 2.2919\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.9497 - val_loss: 2.0060\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.8588 - val_loss: 1.9313\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7883 - val_loss: 1.9153\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7034 - val_loss: 1.9145\n",
    "Epoch 9/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.6382 - val_loss: 1.8979\n",
    "Epoch 10/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.5827 - val_loss: 1.8864\n",
    "Epoch 11/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.5093 - val_loss: 1.8967\n",
    "Epoch 12/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.4472 - val_loss: 1.9040\n",
    "Epoch 13/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.3809 - val_loss: 1.9227\n",
    "Epoch 14/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.3225 - val_loss: 1.9469\n",
    "Epoch 15/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.2516 - val_loss: 1.9862\n",
    "Epoch 16/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.2094 - val_loss: 1.9963\n",
    "Epoch 17/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.1658 - val_loss: 2.0331\n",
    "Epoch 18/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.0851 - val_loss: 2.0452\n",
    "Epoch 19/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.0394 - val_loss: 2.0810\n",
    "Epoch 20/20\n",
    "14725/14725 [==============================] - 15s - loss: 0.9903 - val_loss: 2.1283\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(256))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\n",
    "Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 17s - loss: 3.2991 - val_loss: 3.8902\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.5672 - val_loss: 3.1627\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.2731 - val_loss: 2.6340\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.1316 - val_loss: 2.2594\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.0249 - val_loss: 2.0159\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.9571 - val_loss: 1.9456\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.8789 - val_loss: 1.9213\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.8233 - val_loss: 1.8924\n",
    "Epoch 9/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7575 - val_loss: 1.8987\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(256))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\n",
    "Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 17s - loss: 3.6036 - val_loss: 3.7924\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.7765 - val_loss: 3.0022\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.4773 - val_loss: 2.5697\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.3218 - val_loss: 2.2606\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.2328 - val_loss: 2.0832\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.1748 - val_loss: 2.0248\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.1174 - val_loss: 1.9865\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.0617 - val_loss: 1.9640\n",
    "Epoch 9/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.0206 - val_loss: 1.9461\n",
    "Epoch 10/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.9758 - val_loss: 1.9334\n",
    "Epoch 11/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.9546 - val_loss: 1.9148\n",
    "Epoch 12/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.9045 - val_loss: 1.9121\n",
    "Epoch 13/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.8757 - val_loss: 1.8888\n",
    "Epoch 14/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.8437 - val_loss: 1.8874\n",
    "Epoch 15/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.8145 - val_loss: 1.8822\n",
    "Epoch 16/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7805 - val_loss: 1.8785\n",
    "Epoch 17/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7558 - val_loss: 1.8868\n",
    "Epoch 18/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7218 - val_loss: 1.8670\n",
    "Epoch 19/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7032 - val_loss: 1.8759\n",
    "Epoch 20/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.6832 - val_loss: 1.8834\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(256, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(256))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\n",
    "Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 33s - loss: 3.7814 - val_loss: 3.7282\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.8453 - val_loss: 2.8542\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.5178 - val_loss: 2.4434\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.3762 - val_loss: 2.1894\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.2896 - val_loss: 2.0862\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.2254 - val_loss: 2.0516\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.1565 - val_loss: 2.0133\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.1132 - val_loss: 1.9992\n",
    "Epoch 9/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.0798 - val_loss: 1.9881\n",
    "Epoch 10/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.0509 - val_loss: 1.9784\n",
    "Epoch 11/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.0198 - val_loss: 1.9618\n",
    "Epoch 12/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.9822 - val_loss: 1.9383\n",
    "Epoch 13/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.9437 - val_loss: 1.9300\n",
    "Epoch 14/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.9198 - val_loss: 1.9163\n",
    "Epoch 15/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.8989 - val_loss: 1.9160\n",
    "Epoch 16/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.8866 - val_loss: 1.9085\n",
    "Epoch 17/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.8493 - val_loss: 1.8965\n",
    "Epoch 18/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.8248 - val_loss: 1.8878\n",
    "Epoch 19/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.8037 - val_loss: 1.8870\n",
    "Epoch 20/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.7724 - val_loss: 1.8862\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\n",
    "Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 32s - loss: 3.2809 - val_loss: 3.7595\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.4379 - val_loss: 2.9869\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 29s - loss: 2.1504 - val_loss: 2.5361\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.9887 - val_loss: 2.1294\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.8984 - val_loss: 1.9727\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.7892 - val_loss: 1.9264\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.7172 - val_loss: 1.9100\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.6361 - val_loss: 1.9124\n",
    "Epoch 9/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.5621 - val_loss: 1.9122\n",
    "Epoch 10/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.4863 - val_loss: 1.9045\n",
    "Epoch 11/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.4150 - val_loss: 1.9278\n",
    "Epoch 12/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.3691 - val_loss: 1.9181\n",
    "Epoch 13/20\n",
    "14725/14725 [==============================] - 29s - loss: 1.2970 - val_loss: 1.9414\n",
    "Epoch 14/20\n",
    " 1536/14725 [==>...........................] - ETA: 25s - loss: 1.1475\n",
    " \n",
    " \n",
    " \n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(256))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\n",
    " Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 17s - loss: 3.1787 - val_loss: 3.9282\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.5070 - val_loss: 3.2479\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.2191 - val_loss: 2.7522\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 15s - loss: 2.0637 - val_loss: 2.3331\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.9539 - val_loss: 2.0326\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.8622 - val_loss: 1.9440\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7821 - val_loss: 1.9166\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.7169 - val_loss: 1.8996\n",
    "Epoch 9/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.6561 - val_loss: 1.8849\n",
    "Epoch 10/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.5910 - val_loss: 1.9032\n",
    "Epoch 11/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.5082 - val_loss: 1.8878\n",
    "Epoch 12/20\n",
    "14725/14725 [==============================] - 15s - loss: 1.4513 - val_loss: 1.9252\n",
    "Epoch 13/20\n",
    " 7552/14725 [==============>...............] - ETA: 7s - loss: 1.3534\n",
    " \n",
    " \n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(charset), output_dim=100))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(512))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=20, batch_size=128, validation_split=0.1)\n",
    " Train on 14725 samples, validate on 1637 samples\n",
    "Epoch 1/20\n",
    "14725/14725 [==============================] - 23s - loss: 3.1173 - val_loss: 3.8415\n",
    "Epoch 2/20\n",
    "14725/14725 [==============================] - 20s - loss: 2.4202 - val_loss: 3.1512\n",
    "Epoch 3/20\n",
    "14725/14725 [==============================] - 20s - loss: 2.1302 - val_loss: 2.7191\n",
    "Epoch 4/20\n",
    "14725/14725 [==============================] - 20s - loss: 1.9435 - val_loss: 2.3341\n",
    "Epoch 5/20\n",
    "14725/14725 [==============================] - 20s - loss: 1.7966 - val_loss: 1.9877\n",
    "Epoch 6/20\n",
    "14725/14725 [==============================] - 20s - loss: 1.6621 - val_loss: 1.9349\n",
    "Epoch 7/20\n",
    "14725/14725 [==============================] - 20s - loss: 1.5190 - val_loss: 1.9632\n",
    "Epoch 8/20\n",
    "14725/14725 [==============================] - 20s - loss: 1.3925 - val_loss: 1.9735\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(X, y, epochs=5, batch_size=128, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_author_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Generating with seed: \"oes it really matter what it says this is some test text does it really matter what it says this is \"\n",
      "oes it really matter what it says this is some test text does it really matter what it says this is "
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.float64' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-af9aa51fe5bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"this is some test text does it really matter what it says \"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-16c3c1fabb4f>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(model, diversity, text)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mnext_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mnext_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mgenerated\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnext_char\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-16c3c1fabb4f>\u001b[0m in \u001b[0;36msample\u001b[0;34m(preds, temperature)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mexp_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_preds\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mprobas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.multinomial (numpy/random/mtrand/mtrand.c:37721)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'numpy.float64' has no len()"
     ]
    }
   ],
   "source": [
    "generate(model, diversity=0.7, text=\"this is some test text does it really matter what it says \" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(model, diversity=0.5, text=\"\"):\n",
    "    \"\"\"Generate text from a model\"\"\"\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    for i in range(5000):\n",
    "        x = np.zeros((1, maxlen), dtype=np.int)\n",
    "        for t, char in enumerate(sentence):\n",
    "            try:\n",
    "                x[0, t] = char_indices[char]\n",
    "            except:\n",
    "                print(sentence)\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Generating with seed: \" what it says this is some test text does it really matter what it says this is some test text does \"\n",
      " what it says this is some test text does it really matter what it says this is some test text does the chan store if the store in for Vis a singic and the places I nor for the store in find store is a gitter beat to the trist and the counders here to find. The sele the store with the storit and the chansic I was next the back the truck out. This is a git and the courder the trust and the ching store with the me to good and the next the countret to the this store is little back to the clourder the selsection for the stration's and my for probebles to have the store if friends out befould the chan net store with the park is can sele, I was not longer casteral friends. The store store is little this is this lace in findst and the inclure the selmen store in finds a befter the beet the car this store if the chan strater and closed to the counders. The strail also here in find store with the store with me and selestanding and the strorthors. So I had things a back to the mant I was not look in fried. The store in finds. The mest to the had a leng so this shore sprob food. The lent for the store for the store with the mant and the pricked out a find the chings realing and the mettreer smore with the store is later with got and the counders here for the stort and consorthing. The store store store is littely to get the onlite of the been to this shorth and I was trinks a fead a feas and my next the car the store in find store better here and class. The me of the only convenienter the the cound store. The store store is little but the only Nortar Crip and my ore also head with the things. The me so this shores and class store screet and the fings and long sometime store store is little also this is a back is this is a bittle this was small with the metr seet and my rear to was my sor the counder fead and was counders. The line for the strort and the Chrack is bak in friends and the chan in finds. The strail was come to this store is little bas in finds on the mettranter but the me store is little any the fincers and land store in findlest your for the stort office all park in things around findss. The ment store is fried was converite back the park in finds. The stores store with the counderth and check is this is a good sement and the boxt of the best thing to hand a fead and the me sell this stort of the one of the straf a feat and the chises with the only got for the stroirs and the mark is a firle a feed and the only metter slow with the stricket and my could my dere for real peating. The me something the counders to but the chan store is a Still a was a glass in finds that was conser this store shore for the strorth only regulay same but if my ere fur nite before the bands. The lane I was to gas here and the counder for the store is this is this is a bask in finds and the curont real for the store, The store in findst. The start in friends which when I gound the plation for Crome was strort and a fead all peat for your for the store and the chisies with the manity look in friends. The last in firl also but hive the was next the coure fent and the plattor for your for the store is little back to be. And the me of the stor"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-49291dfdfef3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"this is some test text does it really matter what it says \"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-9c3c76047201>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(model, diversity, text)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mnext_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mnext_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.4/site-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 902\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.4/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1583\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m         return self._predict_loop(f, ins,\n\u001b[0;32m-> 1585\u001b[0;31m                                   batch_size=batch_size, verbose=verbose)\n\u001b[0m\u001b[1;32m   1586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1587\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/.local/lib/python3.4/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1210\u001b[0m                 \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1212\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1213\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.4/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2227\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2228\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2229\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generate(model, diversity=0.5, text=\"this is some test text does it really matter what it says \" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
