{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xmltodict\n",
    "import logging\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "# Main dataset loading utility\n",
    "class PanDataLoader:\n",
    "    \n",
    "    def __init__(self, logger=None):\n",
    "        if logger is None:\n",
    "            logging.basicConfig(level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "            self.log = logging.getLogger(__name__)\n",
    "        else:\n",
    "            self.log = logger\n",
    "                               \n",
    "    def load_17(self, directory):\n",
    "            \n",
    "        \"\"\"Load and return the pan17 gender and variation twitter dataset.\n",
    "        ==============                                      ==============\n",
    "        Samples total                                                10800\n",
    "        Targets            nominal [{male, female},\n",
    "                                    {ar, pt, es, en},\n",
    "                                    {'brazil', 'australia', 'venezuela',\n",
    "                                     'portugal', 'great britain', 'chile',\n",
    "                                     'levantine', 'egypt', 'colombia',\n",
    "                                     'peru', 'ireland', 'argentina',\n",
    "                                     'maghrebi', 'mexico', 'new zealand',\n",
    "                                     'spain', 'canada', 'gulf'}]\n",
    "        ==============                                      ==============\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputdir\n",
    "        The directory containing the training data, i.e. /data/training.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        data : Pandas dataframe\n",
    "            The interesting attributes are:\n",
    "            'text', the data to learn, ['gender','lang', variety],\n",
    "            the regression targets,\n",
    "        Examples\n",
    "        --------\n",
    "        >>> from datasets import load_pan17\n",
    "        >>> df_training = load_pan17(inputdir)\n",
    "        >>> print(df_training.corpus.shape)\n",
    "        (10800, 5)\n",
    "        \"\"\"\n",
    "\n",
    "        X_docs = glob.glob(os.path.join(directory, '*.xml'), recursive=True)\n",
    "        Y_doc = os.path.join(directory, 'truth.txt')\n",
    "        # check that the dataset is loaded correctly\n",
    "\n",
    "        X_tmp = []\n",
    "        for t in X_docs:\n",
    "            with open(t) as f:\n",
    "                doc = xmltodict.parse(f.read())\n",
    "            author = os.path.splitext(os.path.basename(t))[0]\n",
    "            lang = doc['author']['@lang']\n",
    "            text = doc['author']['documents']['document']\n",
    "            X_tmp.append((author, lang, text))\n",
    "\n",
    "        text = pd.DataFrame(X_tmp, columns=[\"author\", \"lang\", \"text\"])\n",
    "\n",
    "        Y_tmp = pd.read_csv(Y_doc,\n",
    "                             sep='\\:\\:\\:',\n",
    "                             names=['author', 'gender', 'variety'],\n",
    "                             engine='python')\n",
    "\n",
    "        corpus = pd.merge(text, Y_tmp, on='author')\n",
    "        return corpus\n",
    "    \n",
    "    def load_16(self, directory):\n",
    "        return self.load_14(directory)\n",
    "    \n",
    "    def load_15(self, directory):\n",
    "        X_docs = glob.glob(os.path.join(directory, '*.xml'), recursive=True)\n",
    "        Y_doc = os.path.join(directory, 'truth.txt')\n",
    "        X_tmp = []\n",
    "        for t in X_docs:\n",
    "            with open(t) as f:\n",
    "                doc = xmltodict.parse(f.read())\n",
    "            author = os.path.splitext(os.path.basename(t))[0]\n",
    "            lang = doc['author']['@lang']\n",
    "            text = doc['author']['document']\n",
    "            # print(author, lang, text[:100])\n",
    "            X_tmp.append((author, lang, text))\n",
    "\n",
    "        text = pd.DataFrame(X_tmp, columns=[\"author\", \"lang\", \"text\"])\n",
    "\n",
    "        Y_tmp = pd.read_csv(Y_doc,\n",
    "                             sep='\\:\\:\\:',\n",
    "                             names=['author', 'gender', 'age', '1','2','3','4', '5'],\n",
    "                             engine='python') \n",
    "\n",
    "\n",
    "        corpus = pd.merge(text, Y_tmp, on='author')\n",
    "        return corpus\n",
    "    \n",
    "    \n",
    "    def load_14(self, directory):\n",
    "        errors = 0\n",
    "        X_docs = glob.glob(os.path.join(directory, '*.xml'), recursive=True)\n",
    "        Y_doc = os.path.join(directory, 'truth.txt')\n",
    "        X_tmp = []\n",
    "        for t in X_docs:\n",
    "            with open(t) as f:\n",
    "                try:\n",
    "                    doc = xmltodict.parse(f.read())\n",
    "                except Exception as e:\n",
    "                    self.log.warning(e)\n",
    "                    self.log.warning(\"Skipping: {}\".format(t))\n",
    "                    continue\n",
    "            author = os.path.splitext(os.path.basename(t))[0]\n",
    "            lang = doc['author']['@lang']\n",
    "            text = []\n",
    "            for td in doc['author']['documents']['document']:\n",
    "                try:\n",
    "                    t = BeautifulSoup(td['#text'], \"lxml\").getText()\n",
    "                    text.append(t)\n",
    "                except Exception as e:\n",
    "                    errors += 1\n",
    "                    # log.warning(e)\n",
    "                    # self.log.warning(\"skipping {}\".format(td))\n",
    "                    continue\n",
    "            X_tmp.append((author, lang, text))\n",
    "\n",
    "        text = pd.DataFrame(X_tmp, columns=[\"author\", \"lang\", \"text\"])\n",
    "\n",
    "        Y_tmp = pd.read_csv(Y_doc,\n",
    "                             sep='\\:\\:\\:',\n",
    "                             names=['author', 'gender', 'age'],\n",
    "                             engine='python') \n",
    "\n",
    "        self.log.warning(\"Skipped {}\".format(errors))\n",
    "\n",
    "        corpus = pd.merge(text, Y_tmp, on='author')\n",
    "        return corpus\n",
    "    \n",
    "    def _load_all(self, loader_func, directories):\n",
    "        \"\"\"Concatenate across languages\"\"\"\n",
    "        corpora = []\n",
    "        for dr in directories:\n",
    "            corpus = loader_func(dr)\n",
    "            corpora.append(corpus)\n",
    "        return pd.concat(corpora)\n",
    "    \n",
    "    def load_all_17(self, directories):\n",
    "        return self._load_all(self.load_17, directories)\n",
    "    \n",
    "    def load_all_16(self, directories):\n",
    "        return self._load_all(self.load_16, directories)\n",
    "    \n",
    "    def load_all_15(self, directories):\n",
    "        return self._load_all(self.load_15, directories)\n",
    "    \n",
    "    def load_all_14(self, directories):\n",
    "        return self._load_all(self.load_14, directories)\n",
    "    \n",
    "    def clean_and_normalize(self, corpus):\n",
    "        \"\"\"Standardize to lowercase for gender and langauge, m/f for gender\n",
    "           Remove personality scores\"\"\"\n",
    "        # FIXME TODO -- how do you do this in place?\n",
    "        # FIXME TODO -- normalize age ranges?\n",
    "        corpus['gender'] = corpus['gender'].apply(lambda s: s[0].lower())\n",
    "        corpus['lang'] = corpus['lang'].apply(lambda s: s.lower())\n",
    "\n",
    "        for c in ['1', '2', '3', '4', '5']:\n",
    "            if c in corpus:\n",
    "                del corpus[c]\n",
    "        return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pan17_dir = \"/root/pan17-author-profiling-training-dataset-2017-03-10/\"\n",
    "\n",
    "pdl = PanDataLoader()\n",
    "corpus = pdl.load_17(\"/root/pan17-author-profiling-training-dataset-2017-03-10/en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus['text'] = corpus['text'].apply(lambda x: \"\\n\".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>lang</th>\n",
       "      <th>text</th>\n",
       "      <th>gender</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a44dc279880378a895a6cbaabf927a5</td>\n",
       "      <td>en</td>\n",
       "      <td>#BornThisWay #cringe #notsorry #proud #LoveTru...</td>\n",
       "      <td>male</td>\n",
       "      <td>ireland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cfe998a7157a4eebf32a4f5d4f66cd0a</td>\n",
       "      <td>en</td>\n",
       "      <td>Developing movement: a march of scientists on ...</td>\n",
       "      <td>male</td>\n",
       "      <td>united states</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3f4181a9b86a6c3c7e552e8015724e36</td>\n",
       "      <td>en</td>\n",
       "      <td>Cate Blanchett, Peter Capaldi, Kit Harington, ...</td>\n",
       "      <td>male</td>\n",
       "      <td>united states</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e58540ad4a155d1fe48b87fa8cae2d9f</td>\n",
       "      <td>en</td>\n",
       "      <td>Problems You're Having With Your #SalesFunnel ...</td>\n",
       "      <td>male</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dfa2664285e6cd59926c5017ff814877</td>\n",
       "      <td>en</td>\n",
       "      <td>@thewrongdonna Yay! My kid is almost 7m and I ...</td>\n",
       "      <td>female</td>\n",
       "      <td>ireland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>d3383f12e6f5af0707e71da5eb99bc2</td>\n",
       "      <td>en</td>\n",
       "      <td>Can't wait for Belfast on Saturday 💁🏼😅 excited...</td>\n",
       "      <td>female</td>\n",
       "      <td>ireland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>d7bee78f1974ed2d72dbfc4b66d9f445</td>\n",
       "      <td>en</td>\n",
       "      <td>@RTE_GUIDE @icarustheatre @BGETheatre #win To ...</td>\n",
       "      <td>female</td>\n",
       "      <td>ireland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>57bfc594b7942f4e4e6083baacdd197b</td>\n",
       "      <td>en</td>\n",
       "      <td>@Greeneil83 decent game this!\\n@albioncoaching...</td>\n",
       "      <td>male</td>\n",
       "      <td>new zealand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>f988ad1bfcbca96341737a17c22cdba1</td>\n",
       "      <td>en</td>\n",
       "      <td>All the usual suspects! \\n#EatRealFood https:/...</td>\n",
       "      <td>male</td>\n",
       "      <td>ireland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ec07cbdbb291b13fc47376c38c1b62ac</td>\n",
       "      <td>en</td>\n",
       "      <td>@bogdanoviclab is a terrific mid career scient...</td>\n",
       "      <td>male</td>\n",
       "      <td>australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6808f5e0baab37c727ee71674ca61d60</td>\n",
       "      <td>en</td>\n",
       "      <td>Wang Qi, the Chinese man trapped in India sinc...</td>\n",
       "      <td>male</td>\n",
       "      <td>new zealand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>d3ebb8fdd5ca0c9eb184ca9f6ecf9f5f</td>\n",
       "      <td>en</td>\n",
       "      <td>Exploring compositions for my next large paint...</td>\n",
       "      <td>male</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7152adc57945262cbaa02e87ed4f2888</td>\n",
       "      <td>en</td>\n",
       "      <td>This fever dream is a doozy https://t.co/b1wil...</td>\n",
       "      <td>male</td>\n",
       "      <td>new zealand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>60705218ce29389bb7181837d701556e</td>\n",
       "      <td>en</td>\n",
       "      <td>@mtoxvaerd @sarahwillers you will be missed, b...</td>\n",
       "      <td>female</td>\n",
       "      <td>ireland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>763976b4ba8c8b11e3cb73a9bb9db680</td>\n",
       "      <td>en</td>\n",
       "      <td>JESUS! what was that! #cityvswans\\nGet in Swan...</td>\n",
       "      <td>male</td>\n",
       "      <td>ireland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6d9a873949d84fba7f89016516b23743</td>\n",
       "      <td>en</td>\n",
       "      <td>@pascalmeier74 @BMJPatientEd @bmj_latest @Rosa...</td>\n",
       "      <td>female</td>\n",
       "      <td>great britain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>c2c67d9af72b23d823387ba453bf7d09</td>\n",
       "      <td>en</td>\n",
       "      <td>“What we find changes who we become.” ― Peter ...</td>\n",
       "      <td>male</td>\n",
       "      <td>australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>48cbe1a8125d97050e3c6f7374bda9a1</td>\n",
       "      <td>en</td>\n",
       "      <td>@MoataTamaira I'm guessing I'll get very angry...</td>\n",
       "      <td>female</td>\n",
       "      <td>new zealand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1d92b74022a9780bca3eee9fd0c3e8a</td>\n",
       "      <td>en</td>\n",
       "      <td>Review of US Immigration at Irish Airports mus...</td>\n",
       "      <td>female</td>\n",
       "      <td>ireland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>759df179edd7e3f97ce2140966a850ce</td>\n",
       "      <td>en</td>\n",
       "      <td>@conorleeson more a weary self reflective comm...</td>\n",
       "      <td>male</td>\n",
       "      <td>ireland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>18f18e589212a17a9546369da71c2974</td>\n",
       "      <td>en</td>\n",
       "      <td>A Three re-brand! @daihenwood @SamHayes_ https...</td>\n",
       "      <td>female</td>\n",
       "      <td>new zealand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>c47a1dca056161ba6b53ab3ebfdc4eab</td>\n",
       "      <td>en</td>\n",
       "      <td>Your happiness will not come to you. It can on...</td>\n",
       "      <td>male</td>\n",
       "      <td>australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>352c73f4b1f550b5995811058e515f58</td>\n",
       "      <td>en</td>\n",
       "      <td>I'm a Subway pro, may as well work there but y...</td>\n",
       "      <td>male</td>\n",
       "      <td>great britain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>a0527939d43360ecab37183ce7b4b645</td>\n",
       "      <td>en</td>\n",
       "      <td>@samueltphillips what is this \"social life\" yo...</td>\n",
       "      <td>male</td>\n",
       "      <td>great britain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>16c9b93cf35fb042f18cd3deda22bcbb</td>\n",
       "      <td>en</td>\n",
       "      <td>@TinieTempah please release soon\\n@btsportfoot...</td>\n",
       "      <td>male</td>\n",
       "      <td>new zealand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>a073c80312950e1b8f363e95d3760738</td>\n",
       "      <td>en</td>\n",
       "      <td>Derby. Or, how to break 3 nails you've been gr...</td>\n",
       "      <td>female</td>\n",
       "      <td>new zealand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>44c0139190b905c64ffc581e889567e1</td>\n",
       "      <td>en</td>\n",
       "      <td>@passionpoprince @ChaseAtlantic I've had the E...</td>\n",
       "      <td>female</td>\n",
       "      <td>australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6f90c2012e153d120dbef8f38c27c436</td>\n",
       "      <td>en</td>\n",
       "      <td>@TenNewsSydney @dailybailey10 So proud of 10 a...</td>\n",
       "      <td>female</td>\n",
       "      <td>australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>273ab17e931b37415aaf18b850f34def</td>\n",
       "      <td>en</td>\n",
       "      <td>You're tempted to say exactly what's on your m...</td>\n",
       "      <td>female</td>\n",
       "      <td>australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>e03d5e6b7f64f0dd50877ad632dcbd53</td>\n",
       "      <td>en</td>\n",
       "      <td>Great idea... we'll come next time pre or post...</td>\n",
       "      <td>female</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3570</th>\n",
       "      <td>b2d6f031eb2226e08c7bb07c7b88a3f5</td>\n",
       "      <td>en</td>\n",
       "      <td>.@alessiacara needs your vote for JUNO Fan Cho...</td>\n",
       "      <td>female</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3571</th>\n",
       "      <td>b8152c6f226017decb92b69150c31187</td>\n",
       "      <td>en</td>\n",
       "      <td>@jessfraz would you like to save this file? Wo...</td>\n",
       "      <td>male</td>\n",
       "      <td>new zealand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3572</th>\n",
       "      <td>9e5e19730042c4ac15f830ae6ba7ec6</td>\n",
       "      <td>en</td>\n",
       "      <td>These ppl make my head hurt. It's one for a ne...</td>\n",
       "      <td>male</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3573</th>\n",
       "      <td>94a31515bf83090ac84393eea195e8f4</td>\n",
       "      <td>en</td>\n",
       "      <td>@malyoung @SteveKentSony @AngelicaMcD @michael...</td>\n",
       "      <td>female</td>\n",
       "      <td>united states</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3574</th>\n",
       "      <td>99201afcaf7a8152fd4649530fb4eb8b</td>\n",
       "      <td>en</td>\n",
       "      <td>That's DEAD she was the best TV\\nwhy am I not ...</td>\n",
       "      <td>female</td>\n",
       "      <td>great britain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3575</th>\n",
       "      <td>52ebcac257f9e1dfd8a2fff9aa5e1771</td>\n",
       "      <td>en</td>\n",
       "      <td>@spacetwinks I watched a documentary on skinhe...</td>\n",
       "      <td>male</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3576</th>\n",
       "      <td>8407bfb621a9314eb9dda613551d38cb</td>\n",
       "      <td>en</td>\n",
       "      <td>Only 22% of parliamentarians are women. Anna A...</td>\n",
       "      <td>female</td>\n",
       "      <td>great britain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3577</th>\n",
       "      <td>dd11b9eb3851377212f5f14b4722b3b2</td>\n",
       "      <td>en</td>\n",
       "      <td>Books, podcasts and TED Talks to ensure you st...</td>\n",
       "      <td>male</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3578</th>\n",
       "      <td>9a94002cb875e7713851fb93e69b3699</td>\n",
       "      <td>en</td>\n",
       "      <td>@meganhoang_ SO BEAUTIFUL\\n@Joe_Sugg @YouTube ...</td>\n",
       "      <td>female</td>\n",
       "      <td>australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3579</th>\n",
       "      <td>66f3f24361ba9b191ee6e369f400c594</td>\n",
       "      <td>en</td>\n",
       "      <td>@jbanal YES it is so bizarre!!!!! I can't wait...</td>\n",
       "      <td>female</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3580</th>\n",
       "      <td>7a8354100908e6f954da0295213e3283</td>\n",
       "      <td>en</td>\n",
       "      <td>The latest The Dean DeSantis Tribune! https://...</td>\n",
       "      <td>male</td>\n",
       "      <td>united states</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3581</th>\n",
       "      <td>db8efea34d6391b8596812cca2973cb0</td>\n",
       "      <td>en</td>\n",
       "      <td>Simplicity is such a complex thing. Yet always...</td>\n",
       "      <td>male</td>\n",
       "      <td>ireland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3582</th>\n",
       "      <td>b56d909afda9a926fd6e5e0323fb7821</td>\n",
       "      <td>en</td>\n",
       "      <td>@harmonycoward fuck, if they're listening to i...</td>\n",
       "      <td>male</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3583</th>\n",
       "      <td>6f0040ab9e60314189360b1ad9a2cf8e</td>\n",
       "      <td>en</td>\n",
       "      <td>There’s an adaptation of Daphne du Maurier’s M...</td>\n",
       "      <td>female</td>\n",
       "      <td>great britain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3584</th>\n",
       "      <td>a68e08c1372d1a0daf0cc5afbdf67fb8</td>\n",
       "      <td>en</td>\n",
       "      <td>'Never lamb alone': loving the new #MLA annual...</td>\n",
       "      <td>female</td>\n",
       "      <td>australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3585</th>\n",
       "      <td>671d02dc0fc3a7b4d477f49986879a00</td>\n",
       "      <td>en</td>\n",
       "      <td>The burrito. https://t.co/SLZNLhJyZs\\n@Steele1...</td>\n",
       "      <td>male</td>\n",
       "      <td>united states</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3586</th>\n",
       "      <td>f09bdd87bf3002aa3df93cdfa4b7c432</td>\n",
       "      <td>en</td>\n",
       "      <td>Bright blue sky today\\nBut still so cold outsi...</td>\n",
       "      <td>female</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3587</th>\n",
       "      <td>4d04cd62a711c5a8229659b5b505e634</td>\n",
       "      <td>en</td>\n",
       "      <td>'NEVER' - Love the description used 😂😂😂 Stay c...</td>\n",
       "      <td>female</td>\n",
       "      <td>australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3588</th>\n",
       "      <td>c1978721d144f148ac7adb59107ea0c6</td>\n",
       "      <td>en</td>\n",
       "      <td>For an overview of #pollutingPruitt's ties wit...</td>\n",
       "      <td>male</td>\n",
       "      <td>united states</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3589</th>\n",
       "      <td>d0b01a46c2384b438f1f19ee2520479a</td>\n",
       "      <td>en</td>\n",
       "      <td>It's hard to tell what's real and what's ficti...</td>\n",
       "      <td>female</td>\n",
       "      <td>australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3590</th>\n",
       "      <td>f561cce6712485d801198a32b229d0bc</td>\n",
       "      <td>en</td>\n",
       "      <td>5 key things for HR to pay attention to in 201...</td>\n",
       "      <td>male</td>\n",
       "      <td>united states</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3591</th>\n",
       "      <td>32018f86dda1145b45bf43f9ae9911a7</td>\n",
       "      <td>en</td>\n",
       "      <td>You're happy to juggle multiple things at once...</td>\n",
       "      <td>male</td>\n",
       "      <td>great britain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3592</th>\n",
       "      <td>8051eb2335045b45554bd2dc042bf03c</td>\n",
       "      <td>en</td>\n",
       "      <td>@hereinmyhead0 is what I tell myself to avoid ...</td>\n",
       "      <td>female</td>\n",
       "      <td>ireland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3593</th>\n",
       "      <td>bcdee457040d6e02a367531df619b927</td>\n",
       "      <td>en</td>\n",
       "      <td>Get me to tomorrow night 😛\\n@spac3elf always o...</td>\n",
       "      <td>female</td>\n",
       "      <td>great britain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3594</th>\n",
       "      <td>bcc0427b30ab9d5ac3bf999843f03bc</td>\n",
       "      <td>en</td>\n",
       "      <td>@Bibliocook oh! Only see this now, hope you fo...</td>\n",
       "      <td>male</td>\n",
       "      <td>ireland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595</th>\n",
       "      <td>ef26880665a889f1aa4084996ab081fd</td>\n",
       "      <td>en</td>\n",
       "      <td>Cant believe #TRUMP is on #statins! Doesn't he...</td>\n",
       "      <td>female</td>\n",
       "      <td>australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3596</th>\n",
       "      <td>13ba51ee9c97bf8f397691612334cc3d</td>\n",
       "      <td>en</td>\n",
       "      <td>Oh my goodness - just found out this gorgeous ...</td>\n",
       "      <td>female</td>\n",
       "      <td>new zealand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3597</th>\n",
       "      <td>dd62d58da4a59190a4b2af23a54b632</td>\n",
       "      <td>en</td>\n",
       "      <td>@lindabear78 I need to do this to my patio gat...</td>\n",
       "      <td>female</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3598</th>\n",
       "      <td>613911dab08662f0a4c61358e3d2bfb</td>\n",
       "      <td>en</td>\n",
       "      <td>While my friends are all hip and cool and play...</td>\n",
       "      <td>female</td>\n",
       "      <td>new zealand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599</th>\n",
       "      <td>9012e8dcff449aed59d67b5f8af566b5</td>\n",
       "      <td>en</td>\n",
       "      <td>@SkyNews is he going to talk about trump it wi...</td>\n",
       "      <td>male</td>\n",
       "      <td>great britain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3600 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                author lang  \\\n",
       "0      a44dc279880378a895a6cbaabf927a5   en   \n",
       "1     cfe998a7157a4eebf32a4f5d4f66cd0a   en   \n",
       "2     3f4181a9b86a6c3c7e552e8015724e36   en   \n",
       "3     e58540ad4a155d1fe48b87fa8cae2d9f   en   \n",
       "4     dfa2664285e6cd59926c5017ff814877   en   \n",
       "5      d3383f12e6f5af0707e71da5eb99bc2   en   \n",
       "6     d7bee78f1974ed2d72dbfc4b66d9f445   en   \n",
       "7     57bfc594b7942f4e4e6083baacdd197b   en   \n",
       "8     f988ad1bfcbca96341737a17c22cdba1   en   \n",
       "9     ec07cbdbb291b13fc47376c38c1b62ac   en   \n",
       "10    6808f5e0baab37c727ee71674ca61d60   en   \n",
       "11    d3ebb8fdd5ca0c9eb184ca9f6ecf9f5f   en   \n",
       "12    7152adc57945262cbaa02e87ed4f2888   en   \n",
       "13    60705218ce29389bb7181837d701556e   en   \n",
       "14    763976b4ba8c8b11e3cb73a9bb9db680   en   \n",
       "15    6d9a873949d84fba7f89016516b23743   en   \n",
       "16    c2c67d9af72b23d823387ba453bf7d09   en   \n",
       "17    48cbe1a8125d97050e3c6f7374bda9a1   en   \n",
       "18     1d92b74022a9780bca3eee9fd0c3e8a   en   \n",
       "19    759df179edd7e3f97ce2140966a850ce   en   \n",
       "20    18f18e589212a17a9546369da71c2974   en   \n",
       "21    c47a1dca056161ba6b53ab3ebfdc4eab   en   \n",
       "22    352c73f4b1f550b5995811058e515f58   en   \n",
       "23    a0527939d43360ecab37183ce7b4b645   en   \n",
       "24    16c9b93cf35fb042f18cd3deda22bcbb   en   \n",
       "25    a073c80312950e1b8f363e95d3760738   en   \n",
       "26    44c0139190b905c64ffc581e889567e1   en   \n",
       "27    6f90c2012e153d120dbef8f38c27c436   en   \n",
       "28    273ab17e931b37415aaf18b850f34def   en   \n",
       "29    e03d5e6b7f64f0dd50877ad632dcbd53   en   \n",
       "...                                ...  ...   \n",
       "3570  b2d6f031eb2226e08c7bb07c7b88a3f5   en   \n",
       "3571  b8152c6f226017decb92b69150c31187   en   \n",
       "3572   9e5e19730042c4ac15f830ae6ba7ec6   en   \n",
       "3573  94a31515bf83090ac84393eea195e8f4   en   \n",
       "3574  99201afcaf7a8152fd4649530fb4eb8b   en   \n",
       "3575  52ebcac257f9e1dfd8a2fff9aa5e1771   en   \n",
       "3576  8407bfb621a9314eb9dda613551d38cb   en   \n",
       "3577  dd11b9eb3851377212f5f14b4722b3b2   en   \n",
       "3578  9a94002cb875e7713851fb93e69b3699   en   \n",
       "3579  66f3f24361ba9b191ee6e369f400c594   en   \n",
       "3580  7a8354100908e6f954da0295213e3283   en   \n",
       "3581  db8efea34d6391b8596812cca2973cb0   en   \n",
       "3582  b56d909afda9a926fd6e5e0323fb7821   en   \n",
       "3583  6f0040ab9e60314189360b1ad9a2cf8e   en   \n",
       "3584  a68e08c1372d1a0daf0cc5afbdf67fb8   en   \n",
       "3585  671d02dc0fc3a7b4d477f49986879a00   en   \n",
       "3586  f09bdd87bf3002aa3df93cdfa4b7c432   en   \n",
       "3587  4d04cd62a711c5a8229659b5b505e634   en   \n",
       "3588  c1978721d144f148ac7adb59107ea0c6   en   \n",
       "3589  d0b01a46c2384b438f1f19ee2520479a   en   \n",
       "3590  f561cce6712485d801198a32b229d0bc   en   \n",
       "3591  32018f86dda1145b45bf43f9ae9911a7   en   \n",
       "3592  8051eb2335045b45554bd2dc042bf03c   en   \n",
       "3593  bcdee457040d6e02a367531df619b927   en   \n",
       "3594   bcc0427b30ab9d5ac3bf999843f03bc   en   \n",
       "3595  ef26880665a889f1aa4084996ab081fd   en   \n",
       "3596  13ba51ee9c97bf8f397691612334cc3d   en   \n",
       "3597   dd62d58da4a59190a4b2af23a54b632   en   \n",
       "3598   613911dab08662f0a4c61358e3d2bfb   en   \n",
       "3599  9012e8dcff449aed59d67b5f8af566b5   en   \n",
       "\n",
       "                                                   text  gender        variety  \n",
       "0     #BornThisWay #cringe #notsorry #proud #LoveTru...    male        ireland  \n",
       "1     Developing movement: a march of scientists on ...    male  united states  \n",
       "2     Cate Blanchett, Peter Capaldi, Kit Harington, ...    male  united states  \n",
       "3     Problems You're Having With Your #SalesFunnel ...    male         canada  \n",
       "4     @thewrongdonna Yay! My kid is almost 7m and I ...  female        ireland  \n",
       "5     Can't wait for Belfast on Saturday 💁🏼😅 excited...  female        ireland  \n",
       "6     @RTE_GUIDE @icarustheatre @BGETheatre #win To ...  female        ireland  \n",
       "7     @Greeneil83 decent game this!\\n@albioncoaching...    male    new zealand  \n",
       "8     All the usual suspects! \\n#EatRealFood https:/...    male        ireland  \n",
       "9     @bogdanoviclab is a terrific mid career scient...    male      australia  \n",
       "10    Wang Qi, the Chinese man trapped in India sinc...    male    new zealand  \n",
       "11    Exploring compositions for my next large paint...    male         canada  \n",
       "12    This fever dream is a doozy https://t.co/b1wil...    male    new zealand  \n",
       "13    @mtoxvaerd @sarahwillers you will be missed, b...  female        ireland  \n",
       "14    JESUS! what was that! #cityvswans\\nGet in Swan...    male        ireland  \n",
       "15    @pascalmeier74 @BMJPatientEd @bmj_latest @Rosa...  female  great britain  \n",
       "16    “What we find changes who we become.” ― Peter ...    male      australia  \n",
       "17    @MoataTamaira I'm guessing I'll get very angry...  female    new zealand  \n",
       "18    Review of US Immigration at Irish Airports mus...  female        ireland  \n",
       "19    @conorleeson more a weary self reflective comm...    male        ireland  \n",
       "20    A Three re-brand! @daihenwood @SamHayes_ https...  female    new zealand  \n",
       "21    Your happiness will not come to you. It can on...    male      australia  \n",
       "22    I'm a Subway pro, may as well work there but y...    male  great britain  \n",
       "23    @samueltphillips what is this \"social life\" yo...    male  great britain  \n",
       "24    @TinieTempah please release soon\\n@btsportfoot...    male    new zealand  \n",
       "25    Derby. Or, how to break 3 nails you've been gr...  female    new zealand  \n",
       "26    @passionpoprince @ChaseAtlantic I've had the E...  female      australia  \n",
       "27    @TenNewsSydney @dailybailey10 So proud of 10 a...  female      australia  \n",
       "28    You're tempted to say exactly what's on your m...  female      australia  \n",
       "29    Great idea... we'll come next time pre or post...  female         canada  \n",
       "...                                                 ...     ...            ...  \n",
       "3570  .@alessiacara needs your vote for JUNO Fan Cho...  female         canada  \n",
       "3571  @jessfraz would you like to save this file? Wo...    male    new zealand  \n",
       "3572  These ppl make my head hurt. It's one for a ne...    male         canada  \n",
       "3573  @malyoung @SteveKentSony @AngelicaMcD @michael...  female  united states  \n",
       "3574  That's DEAD she was the best TV\\nwhy am I not ...  female  great britain  \n",
       "3575  @spacetwinks I watched a documentary on skinhe...    male         canada  \n",
       "3576  Only 22% of parliamentarians are women. Anna A...  female  great britain  \n",
       "3577  Books, podcasts and TED Talks to ensure you st...    male         canada  \n",
       "3578  @meganhoang_ SO BEAUTIFUL\\n@Joe_Sugg @YouTube ...  female      australia  \n",
       "3579  @jbanal YES it is so bizarre!!!!! I can't wait...  female         canada  \n",
       "3580  The latest The Dean DeSantis Tribune! https://...    male  united states  \n",
       "3581  Simplicity is such a complex thing. Yet always...    male        ireland  \n",
       "3582  @harmonycoward fuck, if they're listening to i...    male         canada  \n",
       "3583  There’s an adaptation of Daphne du Maurier’s M...  female  great britain  \n",
       "3584  'Never lamb alone': loving the new #MLA annual...  female      australia  \n",
       "3585  The burrito. https://t.co/SLZNLhJyZs\\n@Steele1...    male  united states  \n",
       "3586  Bright blue sky today\\nBut still so cold outsi...  female         canada  \n",
       "3587  'NEVER' - Love the description used 😂😂😂 Stay c...  female      australia  \n",
       "3588  For an overview of #pollutingPruitt's ties wit...    male  united states  \n",
       "3589  It's hard to tell what's real and what's ficti...  female      australia  \n",
       "3590  5 key things for HR to pay attention to in 201...    male  united states  \n",
       "3591  You're happy to juggle multiple things at once...    male  great britain  \n",
       "3592  @hereinmyhead0 is what I tell myself to avoid ...  female        ireland  \n",
       "3593  Get me to tomorrow night 😛\\n@spac3elf always o...  female  great britain  \n",
       "3594  @Bibliocook oh! Only see this now, hope you fo...    male        ireland  \n",
       "3595  Cant believe #TRUMP is on #statins! Doesn't he...  female      australia  \n",
       "3596  Oh my goodness - just found out this gorgeous ...  female    new zealand  \n",
       "3597  @lindabear78 I need to do this to my patio gat...  female         canada  \n",
       "3598  While my friends are all hip and cool and play...  female    new zealand  \n",
       "3599  @SkyNews is he going to talk about trump it wi...    male  great britain  \n",
       "\n",
       "[3600 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-19 14:44:24,445 - pip.vcs - DEBUG - Registered VCS backend: git\n",
      "2017-04-19 14:44:24,461 - pip.vcs - DEBUG - Registered VCS backend: hg\n",
      "2017-04-19 14:44:24,490 - pip.vcs - DEBUG - Registered VCS backend: svn\n",
      "2017-04-19 14:44:24,492 - pip.vcs - DEBUG - Registered VCS backend: bzr\n"
     ]
    }
   ],
   "source": [
    "from spacy.en import English\n",
    "nlp = English()\n",
    "processed_texts = [nlp(text) for text in corpus['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 0, 0, 0, 1, 1, 1]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_labels = [0 if g == 'female' else 1 for g in corpus['gender']]\n",
    "gender_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vs = set(list(corpus['variety']))\n",
    "var_to_idx = {v: i for i,v in enumerate(vs)}\n",
    "idx_to_var = {i: v for i,v in enumerate(vs)}\n",
    "var_labels = [var_to_idx[v] for v in corpus['variety']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 5, 5, 1, 0, 0, 0, 4, 0, 2, 4]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_labels[:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3600"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "Xs = np.array([x.vector for x in processed_texts])\n",
    "ys = gender_labels\n",
    "len(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.75833333  0.775       0.825       0.80138889  0.79583333]\n",
      "0.791111111111\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from statistics import mean\n",
    "\n",
    "svm = LinearSVC()\n",
    "\n",
    "gender_score = cross_val_score(svm, Xs, gender_labels, cv=5)\n",
    "print(gender_score)\n",
    "print(mean(gender_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var_score = cross_val_score(svm, Xs, var_labels, cv=5)\n",
    "print(var_score)\n",
    "print(mean(var_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad(lst, target_length, pad_char=0):\n",
    "    if len(lst) == target_length:\n",
    "        return lst\n",
    "    if len(lst) < target_length:\n",
    "        diff = target_length - len(lst)\n",
    "        padding = [pad_char] * diff\n",
    "        return lst + padding\n",
    "    else:\n",
    "        return lst[:target_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(ngram_range=(1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_str_tags(spacy_doc):\n",
    "    return \" \".join([word.tag_ for word in spacy_doc])\n",
    "\n",
    "stags = [get_str_tags(text) for text in processed_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagXs = word_vectorizer.fit_transform(stags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.70138889  0.70694444  0.74861111  0.70138889  0.70972222]\n",
      "0.713611111111\n"
     ]
    }
   ],
   "source": [
    "tag_tfidf_score = cross_val_score(svm, tagXs, gender_labels, cv=5)\n",
    "print(tag_tfidf_score)\n",
    "print(mean(tag_tfidf_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_vectorizer = TfidfVectorizer()\n",
    "unigramXs = unigram_vectorizer.fit_transform(corpus['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3600, 378307)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigramXs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.79166667  0.78472222  0.8         0.80833333  0.80416667]\n",
      "0.797777777778\n"
     ]
    }
   ],
   "source": [
    "uni_score = cross_val_score(svm, unigramXs, gender_labels, cv=5)\n",
    "print(uni_score)\n",
    "print(mean(uni_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 0, 1, 1, 0, 1, 1, 0] [1, 1, 1, 1, 0, 0, 0, 1, 1, 1]\n",
      "[ 0.52222222  0.50138889  0.5         0.50277778  0.48472222]\n",
      "0.502222222222\n"
     ]
    }
   ],
   "source": [
    "# sanity check with shuffled labels\n",
    "from random import shuffle\n",
    "sh_gender_labels = gender_labels[:]\n",
    "shuffle(sh_gender_labels)\n",
    "print(sh_gender_labels[:10], gender_labels[:10])\n",
    "uni_score = cross_val_score(svm, unigramXs, sh_gender_labels, cv=5)\n",
    "print(uni_score)\n",
    "print(mean(uni_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.76666667  0.76805556  0.80138889  0.79722222  0.79305556]\n",
      "0.785277777778\n"
     ]
    }
   ],
   "source": [
    "char_vectorizer = TfidfVectorizer(ngram_range=(1,7), analyzer='char')\n",
    "charXs = char_vectorizer.fit_transform(corpus['text'])\n",
    "char_score = cross_val_score(svm, charXs, gender_labels, cv=5)\n",
    "print(char_score)\n",
    "print(mean(char_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "char_word_vectorizer = FeatureUnion([\n",
    "    ('word', unigram_vectorizer),\n",
    "    ('char', char_vectorizer)\n",
    "])\n",
    "\n",
    "cwXs = char_word_vectorizer.fit_transform(corpus['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.80138889  0.7875      0.80972222  0.82222222  0.81944444]\n",
      "0.808055555556\n"
     ]
    }
   ],
   "source": [
    "cw_score = cross_val_score(svm, cwXs, gender_labels, cv=5)\n",
    "print(cw_score)\n",
    "print(mean(cw_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-19 15:33:27,630 - root - INFO - Generating grammar tables from /usr/lib/python3.5/lib2to3/Grammar.txt\n",
      "2017-04-19 15:33:27,662 - root - INFO - Generating grammar tables from /usr/lib/python3.5/lib2to3/PatternGrammar.txt\n"
     ]
    }
   ],
   "source": [
    "import textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textacy.text_stats import TextStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-19 15:34:50,960 - textacy.data - INFO - Loading \"en\" language hyphenator\n"
     ]
    }
   ],
   "source": [
    "stats = TextStats(processed_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-19 15:44:28,249 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:28,386 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:28,473 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:28,629 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:28,647 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:28,671 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:28,714 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:28,719 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:28,801 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:28,828 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:28,923 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:29,080 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:29,103 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:29,127 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:29,175 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:29,277 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:29,298 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:29,358 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:29,562 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:29,582 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:29,603 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:29,705 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:29,897 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:30,028 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:30,212 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:30,263 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:30,294 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:30,375 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:30,407 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:30,641 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:30,648 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:30,720 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:30,826 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:30,881 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:30,966 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:30,977 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:31,027 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:31,061 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:31,065 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:31,142 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:31,301 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:31,312 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:31,316 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:31,319 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:31,426 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:31,594 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:31,633 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:31,682 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:31,761 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:31,765 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:31,837 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:32,220 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:32,235 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:32,252 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:32,294 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:32,316 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:32,352 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:32,394 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:32,402 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:32,500 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:32,599 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:32,685 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:32,720 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:32,726 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:32,849 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:32,863 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:32,911 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:32,925 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:32,994 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:33,092 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:33,165 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:33,263 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:33,286 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:33,298 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:33,336 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:33,350 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:33,402 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:33,466 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:33,556 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:33,579 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:33,642 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:33,714 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:33,750 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:33,805 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:33,975 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:34,018 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:34,048 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:34,079 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:34,130 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:34,147 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:34,201 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:34,277 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:34,294 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-19 15:44:34,299 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:34,312 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:34,434 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:34,502 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:34,668 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:34,729 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:35,101 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:35,106 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:35,133 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:35,302 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:35,320 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:35,476 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:35,565 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:35,621 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:35,647 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:35,696 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:35,716 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:35,778 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:35,782 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:35,995 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:36,045 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:36,317 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:36,400 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:36,407 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:36,533 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:36,635 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:36,656 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:36,684 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:36,742 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:36,778 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:36,823 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:36,827 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:36,924 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:36,983 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:37,089 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:37,098 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:37,115 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:37,178 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:37,240 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:37,270 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:37,304 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:37,341 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:37,640 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:37,661 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:37,701 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:37,793 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:37,912 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:37,942 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:37,965 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:37,994 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:38,015 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:38,047 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:38,081 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:38,101 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:38,151 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:38,426 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:38,674 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:38,781 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:38,800 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:38,815 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:38,835 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:38,917 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:38,961 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:39,004 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:39,107 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:39,140 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:39,163 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:39,793 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:39,894 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:40,497 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:40,613 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:40,707 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:40,900 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:41,069 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:41,522 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:41,529 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:41,760 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:41,900 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:42,023 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:42,248 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:42,268 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:42,570 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:42,625 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:42,681 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:42,743 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:42,748 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:43,393 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:43,825 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:44,163 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:44,625 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:44,674 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:44,701 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:44,728 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-19 15:44:44,846 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:45,285 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:45,456 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:45,464 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:45,811 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:45,834 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:45,960 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:46,177 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:46,333 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:46,396 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:46,447 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:46,650 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:46,897 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:46,917 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:47,181 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:47,529 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:47,610 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:47,663 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:47,916 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:47,922 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:48,197 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n",
      "2017-04-19 15:44:48,295 - root - WARNING - SMOG score may be unreliable for n_sents < 30\n"
     ]
    }
   ],
   "source": [
    "def get_stats(spacy_doc):\n",
    "    stats = TextStats(spacy_doc)\n",
    "    '''\n",
    "            >>> ts.basic_counts\n",
    "        {'n_chars': 685,\n",
    "         'n_long_words': 43,\n",
    "         'n_monosyllable_words': 90,\n",
    "         'n_polysyllable_words': 24,\n",
    "         'n_sents': 6,\n",
    "         'n_syllables': 214,\n",
    "         'n_unique_words': 80,\n",
    "         'n_words': 136}\n",
    "        >>> ts.readability_stats\n",
    "        {'automated_readability_index': 13.626495098039214,\n",
    "         'coleman_liau_index': 12.509300816176474,\n",
    "         'flesch_kincaid_grade_level': 11.817647058823532,\n",
    "         'flesch_readability_ease': 50.707745098039254,\n",
    "         'gulpease_index': 51.86764705882353,\n",
    "         'gunning_fog_index': 16.12549019607843,\n",
    "         'lix': 54.28431372549019,\n",
    "         'smog_index': 14.554592549557764,\n",
    "         'wiener_sachtextformel': 8.266410784313727}'''\n",
    "    return stats.n_unique_words\n",
    "\n",
    "statsXs = [get_stats(text) for text in processed_texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.49722222  0.49861111  0.49861111  0.5         0.5       ]\n",
      "0.498888888889\n"
     ]
    }
   ],
   "source": [
    "svma = LinearSVC()\n",
    "stats_score = cross_val_score(svm, statsXs, gender_labels, cv=5)\n",
    "print(stats_score)\n",
    "print(mean(stats_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mstats = [x for i,x in enumerate(statsXs) if gender_labels[i] == 1]\n",
    "fstats = [x for i,x in enumerate(statsXs) if gender_labels[i] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-146.64499999999953\t 0.018993109267635803\t 7794.280555555555\t 7647.635555555556\n",
      "-3.9849999999999994\t 0.03555011485764838\t 114.08777777777777\t 110.10277777777777\n",
      "-4.436111111111131\t 0.004502265651716365\t 987.5244444444445\t 983.0883333333334\n",
      "-21.204999999999927\t 0.014820319784829826\t 1441.4083333333333\t 1420.2033333333334\n",
      "-13.353333333333296\t 0.03711911442898234\t 366.4194444444444\t 353.0661111111111\n",
      "-40.35944444444476\t 0.01843120266343794\t 2209.9144444444446\t 2169.555\n",
      "-5.150000000000006\t 0.026919268330221333\t 193.88777777777779\t 188.73777777777778\n",
      "-27.392777777777837\t 0.040724058513733424\t 686.34\t 658.9472222222222\n",
      "0.7098574304387295\t 0.053839555769243264\t 12.829753444514338\t 13.539610874953068\n",
      "0.14774930274732956\t 0.025477536372085902\t 5.725324320988866\t 5.873073623736196\n",
      "-0.10305867392933443\t 0.0074493728034612265\t 13.886073082909284\t 13.78301440897995\n",
      "1.1122255134232617\t 0.02561896397668125\t 42.85803601353627\t 43.97026152695953\n",
      "0.12949129878162857\t 0.01146855613036804\t 11.226239659452334\t 11.355730958233963\n",
      "-1.1682387072912874\t 0.019931617568592203\t 59.19645742760968\t 58.02821872031839\n",
      "0.5804076100377706\t 0.060618724670792004\t 9.284521054170195\t 9.864928664207966\n",
      "-0.32823988189019104\t 0.005663655185261607\t 58.119604721226516\t 57.791364839336325\n",
      "0.5986977326786658\t 0.04740020379662778\t 12.331350681860906\t 12.930048414539572\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(fstats[0])):\n",
    "    avg_female = mean([x[i] for x in fstats])\n",
    "    avg_male = mean([x[i] for x in mstats])\n",
    "    diff = avg_female - avg_male\n",
    "    perc = diff/(mean([avg_female, avg_male]))\n",
    "    print(\"{}\\t {}\\t {}\\t {}\".format(diff, abs(perc), avg_male, avg_female))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(unigramXs, gender_labels, test_size=0.5)\n",
    "\n",
    "unigram_svm = LinearSVC()\n",
    "unigram_svm.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = unigram_svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78666666666666663"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_corpus = pdl.load_17(\"/root/pan17-author-profiling-training-dataset-2017-03-10/en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_corpus = new_corpus[new_corpus['lang'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweet_texts = []\n",
    "all_tweet_gender = []\n",
    "tweet_user_lookup = {}\n",
    "\n",
    "for i in range(len(new_corpus)):\n",
    "    user = new_corpus.iloc[i]\n",
    "    gender = 0 if user.gender == \"female\" else 1\n",
    "    labels = [gender] * len(user.text)\n",
    "    all_tweet_texts += user.text\n",
    "    all_tweet_gender += labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_gender_labels = [0 if g == 'female' else 1 for g in new_corpus['gender'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigramByTweetXs = unigram_vectorizer.fit_transform(all_tweet_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uniBTscore = cross_val_score(svm, unigramByTweetXs, all_tweet_gender, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "byTweetPreds = cross_val_predict(svm, unigramByTweetXs, all_tweet_gender, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(byTweetPreds[0:100]).most_common()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    # http://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_preds = list(chunks(byTweetPreds, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "maj_predictions = [Counter(c).most_common()[0][0] for c in chunked_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78027777777777774"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(maj_predictions, n_gender_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "prob_svm = SVC(kernel='linear', probability=True)\n",
    "proba = cross_val_predict(prob_svm, unigramByTweetXs, all_tweet_gender, cv=5, method='predict_proba', njobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
